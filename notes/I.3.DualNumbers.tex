
\section{Dual Numbers}
In this section we introduce a mathematically beautiful  alternative to divided differences for computing derivatives: \emph{dual numbers}. These are a commutative ring that \emph{exactly} compute derivatives, which when implemented on a computer gives very high-accuracy approximations to derivatives. They underpin forward-mode \href{https://en.wikipedia.org/wiki/Automatic_differentiation}{automatic differentation}. Automatic differentiation  is a basic tool in Machine Learning for computing gradients necessary for training neural networks.

\begin{definition}[Dual numbers] Dual numbers $\ensuremath{\bbD}$ are a commutative ring (over $\ensuremath{\bbR}$) generated by $1$ and $\ensuremath{\epsilon}$ such that $\ensuremath{\epsilon}^2 = 0$, that is,
\[
\ensuremath{\bbD} := \{a + b\ensuremath{\epsilon} \quad :\quad
 a,b \in \ensuremath{\bbR}, \quad \ensuremath{\epsilon}^2 =0 \}.
\]
\end{definition}

This is very much analoguous to complex numbers, which are a field generated by $1$ and $\I$ such that $\I^2 = -1$, that is,
\[
\ensuremath{\bbC} := \{a + b\I \quad :\quad
 a,b \in \ensuremath{\bbR}, \quad \I^2 =-1 \}.
\]
Compare multiplication of each number type which falls out of the rules of the generators:
\meeq{
(a + b \I) (c + d \I) = ac + (bc + ad) \I + bd \I^2 = ac -bd + (bc + ad) \I, \ccr
(a + b \ensuremath{\epsilon}) (c + d \ensuremath{\epsilon}) = ac + (bc + ad) \ensuremath{\epsilon} + bd \ensuremath{\epsilon}^2 = ac  + (bc + ad) \ensuremath{\epsilon}.
}
And just as we view $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbC}$ by equating $a \ensuremath{\in} \ensuremath{\bbR}$ with $a + 0\I \ensuremath{\in} \ensuremath{\bbC}$, we can view $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbD}$ by equating $a \ensuremath{\in} \ensuremath{\bbR}$ with $a + 0{\rm \ensuremath{\epsilon}} \ensuremath{\in} \ensuremath{\bbD}$.

Conceptually, dual numbers can be thought of as introducing an infinitesimally small $\ensuremath{\epsilon}$, where $\ensuremath{\epsilon}^2$ is so small it is treated as zero. This is the intuitive reason they allow for differentiation of functions. But we do not need to appeal to this calculus-like interpretation, instead, their construction and relationship to differentiation can be accomplished using purely algebraic reasoning.

\subsection{Differentiating polynomials}
Polynomials evaluated on dual numbers are well-defined as they depend only on the operations $+$ and $*$. From the formula for multiplication of dual numbers we deduce that evaluating a polynomial at a dual number $a + b \ensuremath{\epsilon}$ tells us the derivative of the polynomial at $a$:

\begin{theorem}[polynomials on dual numbers] Suppose $p$ is a polynomial. Then
\[
p(a + b \ensuremath{\epsilon}) = p(a) + b p'(a) \ensuremath{\epsilon}
\]
\end{theorem}
\textbf{Proof}

First consider $p(x) = x^n$ for $n \ensuremath{\geq} 0$.  The cases $n = 0$ and $n = 1$ are immediate. For $n > 1$ we have by induction:
\[
(a + b \ensuremath{\epsilon})^n = (a + b \ensuremath{\epsilon}) (a + b \ensuremath{\epsilon})^{n-1} = (a + b \ensuremath{\epsilon}) (a^{n-1} + (n-1) b a^{n-2} \ensuremath{\epsilon}) = a^n + b n a^{n-1} \ensuremath{\epsilon}.
\]
For a more general polynomial
\[
p(x) = \ensuremath{\sum}_{k=0}^n c_k x^k
\]
the result follows from linearity:
\[
p(a + b \ensuremath{\varepsilon}) = \ensuremath{\sum}_{k=0}^n c_k (a+b\ensuremath{\epsilon})^k = c_0 + \ensuremath{\sum}_{k=1}^n c_k (a^k +k b a^{k-1}\ensuremath{\epsilon})
= \ensuremath{\sum}_{k=0}^n c_k a^k + b \ensuremath{\sum}_{k=1}^n c_k k a^{k-1}\ensuremath{\epsilon} = p(a) + b p'(a) \ensuremath{\epsilon}.
\]
\ensuremath{\QED}

\begin{example}[differentiating polynomial] Consider computing $p'(2)$ where
\[
p(x) = (x-1)(x-2) + x^2.
\]
We can use dual numbers to differentiate, avoiding expanding in monomials or applying rules of differentiating:
\[
p(2+\ensuremath{\epsilon}) = (1+\ensuremath{\epsilon})\ensuremath{\epsilon} + (2+\ensuremath{\epsilon})^2 = \ensuremath{\epsilon} + 4 + 4\ensuremath{\epsilon} = 4 + \underbrace{5}_{p'(2)}\ensuremath{\epsilon}.
\]
\end{example}

\subsection{Differentiating other functions}
We can extend real-valued differentiable functions to dual numbers in a similar manner. First, consider a standard function with a Taylor series (e.g. ${\rm cos}$, ${\rm sin}$, ${\rm exp}$, etc.)
\[
f(x) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k x^k
\]
so that $a$ is inside the radius of convergence. This leads naturally to a definition on dual numbers:
\meeq{
f(a + b \ensuremath{\epsilon}) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k (a + b \ensuremath{\epsilon})^k = f_0 + \ensuremath{\sum}_{k=1}^\ensuremath{\infty} f_k (a^k + k a^{k-1} b \ensuremath{\epsilon}) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k a^k +  \ensuremath{\sum}_{k=1}^\ensuremath{\infty} f_k k a^{k-1} b \ensuremath{\epsilon}  \ccr
  = f(a) + b f'(a) \ensuremath{\epsilon}.
}
More generally, given a differentiable function (which may not have a Taylor series) we can extend it to dual numbers:

\begin{definition}[dual extension] Suppose a real-valued function $f : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\bbR}$ is differentiable in $\ensuremath{\Omega} \ensuremath{\subset} \ensuremath{\bbR}$.  We can construct the \emph{dual extension} $\underline{f} : \ensuremath{\Omega} + \ensuremath{\epsilon}\ensuremath{\bbR} \ensuremath{\rightarrow} \ensuremath{\bbD}$ by defining
\[
\underline{f}(a + b \ensuremath{\epsilon}) := f(a) + b f'(a) \ensuremath{\epsilon}.
\]
By viewing $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbD}$, it is natural to reuse the notation $f$ for the dual extension, hence when there's no chance of confusion we will identify $f(a + b \ensuremath{\epsilon}) \ensuremath{\equiv} \underline{f}(a+b \ensuremath{\epsilon})$. \end{definition}

Thus, for basic functions we have natural extensions:
\begin{align*}
\exp(a + b \ensuremath{\epsilon}) &:= \exp(a) + b \exp(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\sin(a + b \ensuremath{\epsilon}) &:= \sin(a) + b \cos(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\cos(a + b \ensuremath{\epsilon}) &:= \cos(a) - b \sin(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\log(a + b \ensuremath{\epsilon}) &:= \log(a) + {b \over a} \ensuremath{\epsilon} & (a \ensuremath{\in} (0,\ensuremath{\infty}), b \ensuremath{\in} \ensuremath{\bbR}) \\
\sqrt{a+b \ensuremath{\epsilon}} &:= \sqrt{a} + {b \over 2 \sqrt{a}} \ensuremath{\epsilon} & (a \ensuremath{\in} (0,\ensuremath{\infty}), b \ensuremath{\in} \ensuremath{\bbR}) \\
|a + b \ensuremath{\epsilon}| &:= |a| + b\, {\rm sign} a\, \ensuremath{\epsilon} & (a \ensuremath{\in} \ensuremath{\bbR} \backslash \{0\} , b \ensuremath{\in} \ensuremath{\bbR})
\end{align*}
provided the function is differentiable at $a$. Note the last example does not have a convergent Taylor series (at $0$) but we can still extend it where it is differentiable.

Going further, we can add, multiply, and compose such dual-extensions. And the beauty is these automatically satisfy the right properties to be dual-extensions themselves, thus allowing for differentiation of  complicated functions built from basic differentiable building blocks.

The following lemma shows that addition and multiplication in some sense \ensuremath{\ldq}commute" with the dual-extension, hence we can recover the product rule from dual number multiplication:

\begin{lemma}[addition/multiplication] Suppose $f,g : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\bbR}$ are differentiable for $\ensuremath{\Omega} \ensuremath{\subset} \ensuremath{\bbR}$ and $c \ensuremath{\in} \ensuremath{\bbR}$. Then for $a \ensuremath{\in} \ensuremath{\Omega}$ and $b \ensuremath{\in} \ensuremath{\bbR}$ we have
\meeq{
\underline{f+g}(a+b\ensuremath{\epsilon}) = \underline{f}(a+b\ensuremath{\epsilon}) + \underline{g}(a+b\ensuremath{\epsilon}) \ccr 
\underline{c f}(a+b\ensuremath{\epsilon}) = c \underline{f}(a+b \ensuremath{\epsilon}) \ccr
\underline{f g}(a+b\ensuremath{\epsilon}) = \underline{f}(a+b \ensuremath{\epsilon}) \underline{g}(a+b \ensuremath{\epsilon})
}
\end{lemma}
\textbf{Proof} The first two are immediate due to linearity:
\meeq{
\underline{(f+g)}(a+b\ensuremath{\epsilon}) = (f+g)(a) + b(f + g)'(a) \ensuremath{\epsilon} \ccr
 = (f(a)+bf'(a)\ensuremath{\epsilon}) + (g(a)+bg'(a)\ensuremath{\epsilon}) = \underline{f}(a+b\ensuremath{\epsilon}) + \underline{g}(a+b\ensuremath{\epsilon}), \ccr
\underline{cf}(a+b\ensuremath{\epsilon}) = (cf)(a) + b(cf)'(a) \ensuremath{\epsilon} = c(f(a)+bf'(a)\ensuremath{\epsilon}) = c\underline{f}(a+b\ensuremath{\epsilon}).
}
The last property essentially captures the product rule of differentiation:
\meeq{
\underline{f g}(a+b\ensuremath{\epsilon}) = f(a)g(a) + b (f(a)g'(a)+f'(a) g'(a))\ensuremath{\epsilon}  \ccr
= (f(a)+bf'(a)\ensuremath{\epsilon})(g(a)+b g'(a)\ensuremath{\epsilon}) = \underline{f}(a+b \ensuremath{\epsilon}) \underline{g}(a+b \ensuremath{\epsilon}).
}
\ensuremath{\QED}

Furthermore composition recovers the chain rule:

\begin{lemma}[composition]  Suppose $f : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\Gamma}$ and $g : \ensuremath{\Gamma} \ensuremath{\rightarrow} \ensuremath{\bbR}$ are differentiable in $\ensuremath{\Omega},\ensuremath{\Gamma} \ensuremath{\subset} \ensuremath{\bbR}$. Then
\[
\underline{(f \ensuremath{\circ} g)}(a+b \ensuremath{\epsilon}) = \underline{f}(\underline{g}(a+b\ensuremath{\epsilon}))
\]
\end{lemma}
\textbf{Proof} Again it falls out of the properties of dual numbers:
\[
\underline{(f \ensuremath{\circ} g)}(a+b \ensuremath{\epsilon}) = f(g(a)) + bg'(a) f'(g(a)) \ensuremath{\epsilon} = \underline{f}(g(a)+bg'(a)\ensuremath{\epsilon}) = \underline{f}(\underline{g}(a+b\ensuremath{\epsilon}))
\]
\ensuremath{\QED}

A simple corollary is that any function defined in terms of addition, multiplication, composition, etc. of basic functions with dual-extensions will be differentiable via dual numbers. In this following example we see a practical realisation of this, where we differentiate a function by just evaluating it on dual numbers, implicitly, using the dual-extension for the basic build blocks:

\begin{example}[differentiating non-polynomial]

Consider differentiating $f(x) =  \exp(x^2 + \cos x)$ at the point $a = 1$, where  we automatically use the dual-extension of $\exp$ and $\cos$. We can differentiate $f$ by simply evaluating on the duals:
\[
f(1 + \ensuremath{\epsilon}) = \exp(1 + 2\ensuremath{\epsilon} + \cos 1 - \sin 1 \ensuremath{\epsilon}) =  \exp(1 + \cos 1) + \exp(1 + \cos 1) (2 - \sin 1) \ensuremath{\epsilon}.
\]
Therefore we deduce that
\[
f'(1) = \exp(1 + \cos 1) (2 - \sin 1).
\]
\end{example}



