# Dual Numbers

In this section we introduce a mathematically beautiful  alternative to divided differences for computing
derivatives: _dual numbers_. These are a commutative ring that _exactly_ compute derivatives,
which when implemented on a computer gives very high-accuracy approximations to derivatives. They
underpin forward-mode [automatic differentation](https://en.wikipedia.org/wiki/Automatic_differentiation).
Automatic differentiation  is a basic tool in Machine Learning for computing gradients necessary
for training neural networks.



**Definition (Dual numbers)** Dual numbers $𝔻$ are a commutative ring (over $ℝ$)
generated by $1$ and $ϵ$ such that $ϵ^2 = 0$, that is,
$$
𝔻 := \{a + bϵ \quad :\quad
 a,b \in ℝ, \quad ϵ^2 =0 \}.
$$
∎

This is very much analoguous to complex numbers, which are a field generated by $1$ and $\I$ such that
$\I^2 = -1$, that is,
$$
ℂ := \{a + b\I \quad :\quad
 a,b \in ℝ, \quad \I^2 =-1 \}.
$$
Compare multiplication of each number type which falls out of the rules of the generators:
$$
\meeq{
(a + b \I) (c + d \I) = ac + (bc + ad) \I + bd \I^2 = ac -bd + (bc + ad) \I, \ccr
(a + b ϵ) (c + d ϵ) = ac + (bc + ad) ϵ + bd ϵ^2 = ac  + (bc + ad) ϵ.
}
$$
And just as we view $ℝ ⊂ ℂ$ by equating $a ∈ ℝ$ with $a + 0\I ∈ ℂ$,
we can view $ℝ ⊂ 𝔻$ by equating $a ∈ ℝ$ with $a + 0{\rm ϵ} ∈ 𝔻$.

Conceptually, dual numbers can be thought of as introducing an infinitesimally small $ϵ$, where
$ϵ^2$ is so small it is treated as zero. This is the intuitive reason they allow for differentiation
of functions. But we do not need to appeal to this calculus-like interpretation,
instead, their construction and relationship to differentiation can be accomplished using purely
algebraic reasoning.

## Differentiating polynomials

Polynomials evaluated on dual numbers are well-defined as
they depend only on the operations $+$ and $*$. From the formula for multiplication of dual numbers
we deduce that evaluating a polynomial at a dual number $a + b ϵ$ tells us the derivative of the polynomial at $a$:

**Theorem (polynomials on dual numbers)** Suppose $p$ is a polynomial. Then
$$
p(a + b ϵ) = p(a) + b p'(a) ϵ
$$

**Proof**

First consider $p(x) = x^n$ for $n ≥ 0$. 
The cases $n = 0$ and $n = 1$ are immediate. For $n > 1$ we have by induction:
$$
(a + b ϵ)^n = (a + b ϵ) (a + b ϵ)^{n-1} = (a + b ϵ) (a^{n-1} + (n-1) b a^{n-2} ϵ) = a^n + b n a^{n-1} ϵ.
$$
For a more general polynomial
$$
p(x) = ∑_{k=0}^n c_k x^k
$$
the result follows from linearity:
$$
p(a + b ε) = ∑_{k=0}^n c_k (a+bϵ)^k = c_0 + ∑_{k=1}^n c_k (a^k +k b a^{k-1}ϵ)
= ∑_{k=0}^n c_k a^k + b ∑_{k=1}^n c_k k a^{k-1}ϵ = p(a) + b p'(a) ϵ.
$$

∎

**Example (differentiating polynomial)** Consider computing $p'(2)$ where
$$
p(x) = (x-1)(x-2) + x^2.
$$
We can use dual numbers to differentiate, avoiding expanding in monomials
or applying rules of differentiating:
$$
p(2+ϵ) = (1+ϵ)ϵ + (2+ϵ)^2 = ϵ + 4 + 4ϵ = 4 + \underbrace{5}_{p'(2)}ϵ.
$$
∎


## Differentiating other functions


We can extend real-valued differentiable functions to dual numbers in a similar manner.
First, consider a standard function with a Taylor series (e.g. ${\rm cos}$, ${\rm sin}$, ${\rm exp}$, etc.)
$$
f(x) = ∑_{k=0}^∞ f_k x^k
$$
so that $a$ is inside the radius of convergence. This leads naturally to a definition on dual numbers:
$$
\meeq{
f(a + b ϵ) = ∑_{k=0}^∞ f_k (a + b ϵ)^k = f_0 + ∑_{k=1}^∞ f_k (a^k + k a^{k-1} b ϵ) = ∑_{k=0}^∞ f_k a^k +  ∑_{k=1}^∞ f_k k a^{k-1} b ϵ  \ccr
  = f(a) + b f'(a) ϵ.
}
$$
More generally, given a differentiable function (which may not have a Taylor series) we can extend it to dual numbers:

**Definition (dual extension)** Suppose a real-valued function $f : Ω → ℝ$ is differentiable in $Ω ⊂ ℝ$. 
We can construct the _dual extension_ $f̲ : Ω + ϵℝ → 𝔻$ by defining
$$
f̲(a + b ϵ) := f(a) + b f'(a) ϵ.
$$
By viewing $ℝ ⊂ 𝔻$, it is natural to reuse the notation $f$ for the dual extension,
hence when there's no chance of confusion we will identify $f(a + b ϵ) ≡ f̲(a+b ϵ)$.
∎

Thus, for basic functions we have natural extensions:
$$
\begin{align*}
\exp(a + b ϵ) &:= \exp(a) + b \exp(a) ϵ & (a,b ∈ ℝ) \\
\sin(a + b ϵ) &:= \sin(a) + b \cos(a) ϵ & (a,b ∈ ℝ) \\
\cos(a + b ϵ) &:= \cos(a) - b \sin(a) ϵ & (a,b ∈ ℝ) \\
\log(a + b ϵ) &:= \log(a) + {b \over a} ϵ & (a ∈ (0,∞), b ∈ ℝ) \\
\sqrt{a+b ϵ} &:= \sqrt{a} + {b \over 2 \sqrt{a}} ϵ & (a ∈ (0,∞), b ∈ ℝ) \\
|a + b ϵ| &:= |a| + b\, {\rm sign} a\, ϵ & (a ∈ ℝ \backslash \{0\} , b ∈ ℝ)
\end{align*}
$$
provided the function is differentiable at $a$. Note the last example does not have
a convergent Taylor series (at $0$) but we can still extend it where it is differentiable.

Going further, we can add, multiply, and compose such dual-extensions. And the beauty is these automatically
satisfy the right properties to be dual-extensions themselves, thus allowing for differentiation of  complicated functions
built from basic differentiable building blocks.

The following lemma shows that addition and multiplication in some sense “commute" with the dual-extension,
hence we can recover the product rule from dual number multiplication:

**Lemma (addition/multiplication)** Suppose $f,g : Ω → ℝ$ are differentiable for $Ω ⊂ ℝ$ and $c ∈ ℝ$. Then
for $a ∈ Ω$ and $b ∈ ℝ$ we have
$$
\meeq{
\underline{f+g}(a+bϵ) = f̲(a+bϵ) + g̲(a+bϵ) \ccr 
\underline{c f}(a+bϵ) = c f̲(a+b ϵ) \ccr
\underline{f g}(a+bϵ) = f̲(a+b ϵ) g̲(a+b ϵ)
}
$$

**Proof**
The first two are immediate due to linearity:
$$
\meeq{
\underline{(f+g)}(a+bϵ) = (f+g)(a) + b(f + g)'(a) ϵ \ccr
 = (f(a)+bf'(a)ϵ) + (g(a)+bg'(a)ϵ) = f̲(a+bϵ) + g̲(a+bϵ), \ccr
\underline{cf}(a+bϵ) = (cf)(a) + b(cf)'(a) ϵ = c(f(a)+bf'(a)ϵ) = cf̲(a+bϵ).
}
$$
The last property essentially captures the product rule of differentiation:
$$
\meeq{
\underline{f g}(a+bϵ) = f(a)g(a) + b (f(a)g'(a)+f'(a) g'(a))ϵ  \ccr
= (f(a)+bf'(a)ϵ)(g(a)+b g'(a)ϵ) = f̲(a+b ϵ) g̲(a+b ϵ).
}
$$
∎

Furthermore composition recovers the chain rule:

**Lemma (composition)** 
Suppose $f : Γ → ℝ$ and $g : Ω → Γ$ are differentiable in $Ω,Γ ⊂ ℝ$. Then
$$
\underline{(f ∘ g)}(a+b ϵ) = f̲(g̲(a+bϵ))
$$

**Proof**
Again it falls out of the properties of dual numbers:
$$
\underline{(f ∘ g)}(a+b ϵ) = f(g(a)) + bg'(a) f'(g(a)) ϵ = f̲(g(a)+bg'(a)ϵ) = f̲(g̲(a+bϵ))
$$
∎

A simple corollary is that any function defined in terms of addition, multiplication, composition, etc.
of basic functions with dual-extensions will be differentiable via dual numbers. In this following example we see a practical realisation
of this, where we differentiate a function by just evaluating it on dual numbers, implicitly, using the dual-extension
for the basic build blocks:

**Example (differentiating non-polynomial)**

Consider differentiating $f(x) =  \exp(x^2 + \cos x)$ at the point $a = 1$, where 
we automatically use the dual-extension of $\exp$ and $\cos$. We can differentiate $f$
by simply evaluating on the duals:
$$
f(1 + ϵ) = \exp(1 + 2ϵ + \cos 1 - \sin 1 ϵ) =  \exp(1 + \cos 1) + \exp(1 + \cos 1) (2 - \sin 1) ϵ.
$$
Therefore we deduce that
$$
f'(1) = \exp(1 + \cos 1) (2 - \sin 1).
$$


∎

