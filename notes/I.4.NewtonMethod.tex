
\section{Newton's method}
In school you may recall learning Newton's method: a way of approximating zeros/roots to a function by using a local approximation by an affine function. That is, approximate a function $f(x)$ locally around an initial guess $x_0$ by its first order Taylor series:
\[
f(x) \ensuremath{\approx} f(x_0) + f'(x_0) (x-x_0)
\]
and then find the root of the right-hand side which is
\[
 f(x_0) + f'(x_0) (x-x_0) = 0 \ensuremath{\Leftrightarrow} x = x_0 - {f(x_0) \over f'(x_0)}.
\]
We can then repeat using this root as the new initial guess. In other words we have a sequence of \emph{hopefully} more accurate approximations:
\[
x_{k+1} = x_k - {f(x_k) \over f'(x_k)}.
\]
The convergence theory of Newton's method is rich and beautiful but outside the scope of this module. But provided $f$ is smooth, if $x_0$ is sufficiently close to a root this iteration will converge. 

Thus \emph{if} we can compute derivatives, we can (sometimes) compute roots. The lab will explore using dual numbers to accomplish this task. This is in some sense a baby version of how Machine Learning algorithms train neural networks.

In terms of analysis, we can guarantee convergence provided our initial guess is accurate enough. The first step is the bound the error of an iteration in terms of the previous error:

\begin{theorem}[Newton error] Suppose $f$ is twice-differentiable in a neighbourhood $B$ of $x$  such that $f(x) = 0$, and $f'$ does not vanish in $B$. Denote the error of the $k$-th Newton iteration as $\ensuremath{\varepsilon}_k := x - x_k$. If $x_k \ensuremath{\in} B$ then
\[
|\ensuremath{\varepsilon}_{k+1}| \ensuremath{\leq} M |\ensuremath{\varepsilon}_k|^2
\]
where
\[
M := {1 \over 2} \sup_{y \ensuremath{\in} B}  | f''(y)|   \sup_{y \ensuremath{\in} B} \left| {1 \over f'(y)} \right|.
\]
\end{theorem}
\textbf{Proof} Using Taylor's theorem we find that
\[
0 = f(x) = f(x_k + \ensuremath{\varepsilon}_k) = f(x_k) + f'(x_k) \ensuremath{\varepsilon}_k + {f''(t) \over 2} \ensuremath{\varepsilon}_k^2.
\]
for some $t \ensuremath{\in} B$ between $x$ and $x_k$.  Rearranging this we get an expression for $f(x_k)$ that tells us that
\[
\ensuremath{\varepsilon}_{k+1} = x - x_{k+1} = \ensuremath{\varepsilon}_k +  {f(x_k) \over f'(x_k)} = -{f''(t) \over 2f'(x_k)} \ensuremath{\varepsilon}_k^2.
\]
Taking absolute values of each side gives the result.

\ensuremath{\QED}

Hidden in this result is a guarantee of convergence provided $x_0$ is sufficiently close to $x$.

\begin{corollary}[Newton convergence] If $x_0 \ensuremath{\in} B$ is sufficiently close to $x$ then $x_k \ensuremath{\rightarrow} x$.

\end{corollary}
\textbf{Proof}

Suppose $x_k \ensuremath{\in} B$ satisfies $|\ensuremath{\varepsilon}_k| = |x-x_k| < M^{-1}$. Then
\[
|\ensuremath{\varepsilon}_{k+1}| \ensuremath{\leq} M |\ensuremath{\varepsilon}_k|^2 < |\ensuremath{\varepsilon}_k|,
\]
hence $x_{k+1} \ensuremath{\in} B$. Thus if $x_0$ satisfies this condition we have from induction that
\[
|\ensuremath{\varepsilon}_k| \ensuremath{\leq} M^k |\ensuremath{\varepsilon}_0|^{2k}.
\]
Provided $x_0$ also satisfies $|\ensuremath{\varepsilon}_0| < M^{-1/2}$ this will go to zero as $k \ensuremath{\rightarrow} \ensuremath{\infty}$.

\ensuremath{\QED}



