# Newton's method

In school you may recall learning Newton's method: a way of approximating zeros/roots to
a function by using a local approximation by an affine function. That is, approximate a function
$f(x)$ locally around an initial guess $x_0$ by its first order Taylor series:
$$
f(x) ≈ f(x_0) + f'(x_0) (x-x_0)
$$
and then find the root of the right-hand side which is
$$
 f(x_0) + f'(x_0) (x-x_0) = 0 ⇔ x = x_0 - {f(x_0) \over f'(x_0)}.
$$
We can then repeat using this root as the new initial guess. In other words
we have a sequence of _hopefully_ more accurate approximations:
$$
x_{k+1} = x_k - {f(x_k) \over f'(x_k)}.
$$
The convergence theory of Newton's method is rich and beautiful but outside
the scope of this module. But provided $f$ is smooth, if $x_0$ is sufficiently
close to a root this iteration will converge. 

Thus _if_ we can compute derivatives, we can (sometimes) compute roots. The lab
will explore using dual numbers to accomplish this task. This is in some sense
a baby version of how Machine Learning algorithms train neural networks.

In terms of analysis, we can guarantee convergence provided our initial guess is
accurate enough. The first step is the bound the error of an iteration in terms
of the previous error:


**Theorem (Newton error)**
Suppose $f$ is twice-differentiable in a neighbourhood $B$ of $x$  such that $f(x) = 0$,
and $f'$ does not vanish in $B$. Denote the
error of the $k$-th Newton iteration as $ε_k := x - x_k$. If $x_k ∈ B$ then
$$
|ε_{k+1}| ≤ M |ε_k|^2
$$
where
$$
M := {1 \over 2} \sup_{y ∈ B}  | f''(y)|   \sup_{y ∈ B} \left| {1 \over f'(y)} \right|.
$$


**Proof**
Using Taylor's theorem we find that
$$
0 = f(x) = f(x_k + ε_k) = f(x_k) + f'(x_k) ε_k + {f''(t) \over 2} ε_k^2.
$$
for some $t ∈ B$ between $x$ and $x_k$. 
Rearranging this we get an expression for $f(x_k)$ that tells us that
$$
ε_{k+1} = x - x_{k+1} = ε_k +  {f(x_k) \over f'(x_k)} = -{f''(t) \over 2f'(x_k)} ε_k^2.
$$
Taking absolute values of each side gives the result.

∎

Hidden in this result is a guarantee of convergence provided $x_0$ is sufficiently close to $x$.

**Corollary (Newton convergence)** If $x_0 ∈ B$ is sufficiently close to $x$ then $x_k → x$.

**Proof**

Suppose $x_k ∈ B$ satisfies $|ε_k| = |x-x_k| < M^{-1}$. Then
$$
|ε_{k+1}| ≤ M |ε_k|^2 < |ε_k|,
$$
hence $x_{k+1} ∈ B$. Thus if $x_0$ satisfies this condition we have from induction
that
$$
|ε_k| ≤ M^k |ε_0|^{2k}.
$$
Provided $x_0$ also satisfies $|ε_0| < M^{-1/2}$ this will go to zero as $k → ∞$.

∎

