
\section{Orthogonal and Unitary Matrices}
PLU factorisations are an effective scheme for inverting systems, however, we saw in the lab that for very special matrices it can fail to be accurate. In the next two sections we introduce an alternative approach that is guaranteed to be stable: factorise a matrix as
\[
A = QR
\]
where $Q$ is an orthogonal/unitary matrix and $R$ is a \emph{right-triangular matrix}, which for square matrices is another name for upper-triangular.

This factorisation is valid for rectangular matrices $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$, where now \emph{right-triangular} is a rectangular version of upper-triangular. For rectangular systems we can no longer solve linear systems of the form $A\ensuremath{\bm{\x}} = \ensuremath{\bm{\b}}$ (unless $\ensuremath{\bm{\b}}$ lies in the column span of $A$) but instead we want to solve $A\ensuremath{\bm{\x}} \ensuremath{\approx} \ensuremath{\bm{\b}}$, where $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$ and $\ensuremath{\bm{\b}} \ensuremath{\in} \ensuremath{\bbC}^m$. More precisely, we can use a QR factorisation to solve \emph{least squares} problems, find $\ensuremath{\bm{\x}}$ that minimises the 2-norm:
\[
\|A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \|
\]
Before we discuss the computation of a QR factorisation and its role in solving least-squares problems, we introduce orthogonal and unitary matrices. In particular we will discuss reflections and rotations, which can be used to represent more general orthogonal matrices.

\begin{definition}[orthogonal/unitary matrix] A square real matrix is \emph{orthogonal} if its inverse is its transpose:
\[
O(n) = \{Q \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n} : Q^\ensuremath{\top}Q = I \}
\]
A square complex matrix is \emph{unitary} if its inverse is its adjoint:
\[
U(n) = \{Q \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n} : Q^\ensuremath{\star}Q = I \}.
\]
Here the adjoint is the same as the conjugate-transpose: $Q^\ensuremath{\star} := \bar Q^\ensuremath{\top}$.  \end{definition}

Note that $O(n) \ensuremath{\subset} U(n)$ as for real matrices $Q^\ensuremath{\star} = Q^\ensuremath{\top}$. Because in either case $Q^{-1} = Q^\ensuremath{\star}$ we also have $Q Q^\ensuremath{\star} = I$ (which for real matrices is $Q Q^\ensuremath{\top} = I$). These matrices are particularly important for numerical linear algebra for a number of reasons (we'll explore these properties in the problem sheets):

\begin{itemize}
\item[1. ] They are norm-preserving: for any vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$ and $Q \ensuremath{\in} U(n)$    we have $\|Q \ensuremath{\bm{\x}} \| = \| \ensuremath{\bm{\x}}\|$ where $\| \ensuremath{\bm{\x}} \|^2 := \ensuremath{\sum}_{k=1}^n x_k^2$ (i.e. the 2-norm).


\item[2. ] All eigenvalues have absolute value equal to $1$.


\item[3. ] For $Q \ensuremath{\in} O(n)$,  $\det Q = \ensuremath{\pm}1$.


\item[4. ] They are trivially invertible (just take the adjoint).


\item[5. ] They are generally \ensuremath{\ldq}stable": errors due to rounding when multiplying a vector by $Q$ are controlled.


\item[6. ] They are \emph{normal matrices}: they commute with their adjoint ($Q Q^\ensuremath{\star} = Q Q^\ensuremath{\star}$). 


\item[7. ] Both $O(n)$ and $U(n)$ are groups, in particular, they are closed under multiplication.

\end{itemize}
On a computer there are multiple ways of representing orthogonal/unitary matrices. The obvious way is to store entries  as a dense matrix, however, this is very inefficient. In the appendices we have seen permutation matrices, which are a special type of orthogonal matrices where we can store only the order the entries are permuted as a vector. 

More generally, we will use the group structure: represent general orthogonal/unitary matrices as products of simpler elements of the group. In partular we will use two building blocks:

\begin{itemize}
\item[1. ] \emph{Rotations}: Rotations are equivalent to special orthogonal matrices $SO(2)$  and correspond to rotations in 2D.


\item[2. ] \emph{Reflections}:  Reflections are elements of $U(n)$ that are defined in terms of a single unit vector $\ensuremath{\bm{\v}} \ensuremath{\in} \ensuremath{\bbC}^n$ which is reflected.

\end{itemize}
We remark a related concept to orthogonal/unitary matrices are rectangular matrices with orthonormal columns, e.g.
\[
U = [\ensuremath{\bm{\u}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\u}}_n] \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}
\]
where $m \ensuremath{\geq} n$ such that $U^\ensuremath{\star} U =  I_n$ (the $n \ensuremath{\times} n$ identity matrix). In the case where $m > n$ we must have $UU^\ensuremath{\star} \ensuremath{\neq} I_m$ as the rank of $U$ is $n < m$. 

\subsection{Rotations}
We begin with a general definition:

\begin{definition}[Special Orthogonal and Rotations] \emph{Special Orthogonal Matrices} are
\[
SO(n) := \{Q \ensuremath{\in} O(n) | \det Q = 1 \}
\]
And (simple) \emph{rotations} are $SO(2)$. \end{definition}

In what follows we use the following for writing the angle of a vector:

\begin{definition}[two-arg arctan] The two-argument arctan function gives the angle \texttt{\ensuremath{\theta}} through the point $[a,b]^\ensuremath{\top}$, i.e., 
\[
\sqrt{a^2 + b^2} \begin{bmatrix} \cos \ensuremath{\theta} \\ \sin \ensuremath{\theta} \end{bmatrix} =  \begin{bmatrix} a \\ b \end{bmatrix}.
\]
It can be defined in terms of the standard arctan as follows:
\[
{\rm atan}(b,a) := \begin{cases} {\rm atan}{b \over a} & a > 0 \\
                            {\rm atan}{b \over a} + \ensuremath{\pi} & a < 0\hbox{ and }b >0 \\
                            {\rm atan}{b \over a} - \ensuremath{\pi} & a < 0\hbox{ and }b < 0 \\
                            \ensuremath{\pi}/2 & a = 0\hbox{ and }b >0 \\
                            -\ensuremath{\pi}/2 & a = 0\hbox{ and }b < 0 
                            \end{cases}
\]
\end{definition}

We show $SO(2)$ are exactly equivalent to standard rotations:

\begin{proposition}[simple rotation] A 2\ensuremath{\times}2 \emph{rotation matrix} through angle $\ensuremath{\theta}$ is
\[
Q_\ensuremath{\theta} := \begin{bmatrix} \cos \ensuremath{\theta} & -\sin \ensuremath{\theta} \cr \sin \ensuremath{\theta} & \cos \ensuremath{\theta} \end{bmatrix}.
\]
We have $Q \ensuremath{\in} SO(2)$ if and only if $Q = Q_\ensuremath{\theta}$ for some $\ensuremath{\theta} \ensuremath{\in} \ensuremath{\bbR}$.

\end{proposition}
\textbf{Proof}

First assume $Q_\ensuremath{\theta}$ is of that form and write $c = \cos \ensuremath{\theta}$ and $s = \sin \ensuremath{\theta}$. Then we have
\[
Q_\ensuremath{\theta}^\ensuremath{\top}Q_\ensuremath{\theta} = \begin{pmatrix} c & s \\ -s & c \end{pmatrix} \begin{pmatrix} c & -s \\ s & c \end{pmatrix} = 
\begin{pmatrix} c^2 + s^2 & 0 \\ 0 & c^2 + s^2 \end{pmatrix} = I
\]
and $\det Q_\ensuremath{\theta} = c^2 + s^2 = 1$ hence $Q_\ensuremath{\theta} \ensuremath{\in} SO(2)$. 

Now suppose $Q = [\ensuremath{\bm{\q}}_1, \ensuremath{\bm{\q}}_2] \ensuremath{\in} SO(2)$ where we know its columns have norm 1, i.e. $\|\ensuremath{\bm{\q}}_k\| = 1$, and are orthogonal. Write $\ensuremath{\bm{\q}}_1 = [c,s]$ where we know $c = \cos \ensuremath{\theta}$ and $s = \sin \ensuremath{\theta}$ for $\ensuremath{\theta} = {\rm atan}(s, c)$.  Since $\ensuremath{\bm{\q}}_1\cdot \ensuremath{\bm{\q}}_2 = 0$ we can deduce $\ensuremath{\bm{\q}}_2 = \ensuremath{\pm} [-s,c]$. The sign is positive as $\det Q = \ensuremath{\pm}(c^2 + s^2) = \ensuremath{\pm}1$.

\ensuremath{\QED}

We can rotate an arbitrary vector in $\ensuremath{\bbR}^2$ to the unit axis using rotations, which are useful in linear algebra decompositions. Interestingly it only requires basic algebraic functions (no trigonometric functions):

\begin{proposition}[rotation of a vector]  The matrix
\[
Q = {1 \over \sqrt{a^2 + b^2}}
\begin{bmatrix}
 a & b \cr -b & a
\end{bmatrix}
\]
is a rotation matrix ($Q \ensuremath{\in} SO(2)$) satisfying
\[
Q \begin{bmatrix} a \\ b \end{bmatrix} = \sqrt{a^2 + b^2} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\]
\end{proposition}
\textbf{Proof} 

The last equation is trivial so the only question is that it is a rotation matrix. This follows immediately:
\[
Q^\ensuremath{\top} Q = {1 \over a^2 + b^2}  \begin{bmatrix}
 a^2 + b^2 & 0 \cr 0 & a^2 + b^2
\end{bmatrix} = I
\]
and $\det Q = 1$.

\ensuremath{\QED}

\begin{example}[rotating a vector] Consider the vector
\[
\ensuremath{\bm{\x}} = \Vectt[-1,-\sqrt{3}].
\]
We can use the proposition above to deduce the rotation matrix that rotates this vector to the positive real axis is:
\[
{1 \over \sqrt{1+3}} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix} = 
{1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
\]
Alternatively, we could determine the matrix by computing the angle of the vector via:
\[
\ensuremath{\theta} =  {\rm atan}(-\sqrt{3}, -1) = {\rm atan}(\sqrt{3}) - \ensuremath{\pi} = -{2\ensuremath{\pi} \over 3}.
\]
We thus compute:
\[
Q_{-\ensuremath{\theta}} = \begin{bmatrix}
\cos(2\ensuremath{\pi}/3) & -\sin(2\ensuremath{\pi}/3) \\
\sin(2\ensuremath{\pi}/3) & \cos(2\ensuremath{\pi}/3)
\end{bmatrix} = {1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
\]
\end{example}

More generally, we can consider rotations that operate on two entries of a vector at a time. This will be explored in the problem sheet/lab.

\subsection{Reflections}
In addition to rotations, another type of orthogonal/unitary matrix are reflections. These are specified by a single vector which is reflected, with everything orthogonal to the vector left fixed. 

\begin{definition}[reflection matrix]  Given a unit vector $\ensuremath{\bm{\v}} \ensuremath{\in} \ensuremath{\bbC}^n$ (satisfying $\|\ensuremath{\bm{\v}}\|=1$), define the corresponding \emph{reflection matrix} as:
\[
Q_{\ensuremath{\bm{\v}}} := I - 2 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star}
\]
\end{definition}

These are indeed reflections in the direction of $\ensuremath{\bm{\v}}$. We can show this as follows:

\begin{proposition}[Householder properties] $Q_{\ensuremath{\bm{\v}}}$ satisfies:

\begin{itemize}
\item[1. ] Symmetry: $Q_{\ensuremath{\bm{\v}}} = Q_{\ensuremath{\bm{\v}}}^\ensuremath{\star}$


\item[2. ] Orthogonality: $Q_{\ensuremath{\bm{\v}}} \ensuremath{\in} U(n)$


\item[3. ] The vector $\ensuremath{\bm{\v}}$ is an eigenvector of $Q_{\ensuremath{\bm{\v}}}$ with eigenvalue $-1$


\item[4. ] For the dimension $n-1$ space $W := \{\ensuremath{\bm{\w}} : \ensuremath{\bm{\w}}^\ensuremath{\star} \ensuremath{\bm{\v}} = 0 \}$, all vectors $\ensuremath{\bm{\w}} \ensuremath{\in} W$ satisfy $Q_{\ensuremath{\bm{\v}}}\ensuremath{\bm{\w}} = \ensuremath{\bm{\w}}$.


\item[5. ] Not a rotation: $\det Q_{\ensuremath{\bm{\v}}} = -1$

\end{itemize}
\end{proposition}
\textbf{Proof}

Property 1 follows immediately. Property 2 follows from
\[
Q_{\ensuremath{\bm{\v}}}^\ensuremath{\star} Q_{\ensuremath{\bm{\v}}} = Q_{\ensuremath{\bm{\v}}}^2 = I - 4 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} + 4 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} = I.
\]
Property 3 follows since
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\v}} = \ensuremath{\bm{\v}} - 2\ensuremath{\bm{\v}} (\ensuremath{\bm{\v}}^\ensuremath{\star}\ensuremath{\bm{\v}}) = -\ensuremath{\bm{\v}}.
\]
Property 4 follows from:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\w}} = \ensuremath{\bm{\w}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\w}}^\ensuremath{\star} \ensuremath{\bm{\v}}) =  \ensuremath{\bm{\w}}
\]
Property 5 then follows: Property 4 tells us that $1$ is an eigenvalue with multiplicity $n-1$. Since $-1$ is an eigenvalue with multiplicity 1,  the determinant, which is product of the eigenvalues, is $-1$.

\ensuremath{\QED}

\begin{example}[reflection through 2-vector] Consider reflection through $\ensuremath{\bm{\x}} = [1,2]^\ensuremath{\top}$.  We first need to normalise $\ensuremath{\bm{\x}}$:
\[
\ensuremath{\bm{\v}} = {\ensuremath{\bm{\x}} \over \|\ensuremath{\bm{\x}}\|} = \begin{bmatrix} {1 \over \sqrt{5}} \\ {2 \over \sqrt{5}} \end{bmatrix}
\]
The reflection matrix is:
\[
Q_{\ensuremath{\bm{\v}}} = I - 2 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\top} = \begin{bmatrix}1 \\ & 1 \end{bmatrix} - {2 \over 5} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}
 =  {1 \over 5} \begin{bmatrix} 3 & -4 \\ -4 & -3 \end{bmatrix}
\]
Indeed it is symmetric, and orthogonal. It sends $\ensuremath{\bm{\x}}$ to $-\ensuremath{\bm{\x}}$:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\x}} = {1 \over 5} \begin{bmatrix}3 - 8 \\ -4 - 6 \end{bmatrix} = -\ensuremath{\bm{\x}}
\]
Any vector orthogonal to $\ensuremath{\bm{\x}}$, like $\ensuremath{\bm{\y}} = [-2,1]^\ensuremath{\top}$, is left fixed:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\y}} = {1 \over 5} \begin{bmatrix}-6 -4 \\ 8 - 3 \end{bmatrix} = \ensuremath{\bm{\y}}
\]
\end{example}

Note that \emph{building} the matrix $Q_{\ensuremath{\bm{\v}}}$ will be expensive ($O(n^2)$ operations), but we can \emph{apply} $Q_{\ensuremath{\bm{\v}}}$ to a vector in $O(n)$ operations using the expression:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\v}}^\ensuremath{\star} \ensuremath{\bm{\x}}) = \ensuremath{\bm{\x}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\v}} \ensuremath{\cdot} \ensuremath{\bm{\x}}).
\]
\subsubsection{Householder reflections}
Just as rotations can be used to rotate vectors to be aligned with coordinate axes, so can reflections, but in this case it works for vectors in $\ensuremath{\bbC}^n$, not just $\ensuremath{\bbR}^2$. We begin with the real case:

\begin{definition}[Householder reflection, real case] For a given vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n$, define the Householder reflection
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} := Q_{\ensuremath{\bm{\w}}}
\]
for $\ensuremath{\bm{\y}} = \ensuremath{\mp} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}$ and $\ensuremath{\bm{\w}} = {\ensuremath{\bm{\y}} \over \|\ensuremath{\bm{\y}}\|}$. The default choice in sign is:
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} := Q_{\ensuremath{\bm{\x}}}^{-\hbox{sign}(x_1),\rm H}.
\]
\end{definition}

\begin{lemma}[Householder reflection maps to axis] For $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n$,
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} \ensuremath{\bm{\x}} = \ensuremath{\pm}\|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1
\]
\end{lemma}
\textbf{Proof} Note that
\begin{align*}
\| \ensuremath{\bm{\y}} \|^2 &= 2\|\ensuremath{\bm{\x}}\|^2 \ensuremath{\mp} 2 \|\ensuremath{\bm{\x}}\| x_1, \\
\ensuremath{\bm{\y}}^\ensuremath{\top} \ensuremath{\bm{\x}} &= \|\ensuremath{\bm{\x}}\|^2 \ensuremath{\mp}  \|\ensuremath{\bm{\x}}\| x_1
\end{align*}
where $x_1 = \ensuremath{\bm{\e}}_1^\ensuremath{\top} \ensuremath{\bm{\x}}$. Therefore:
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} \ensuremath{\bm{\x}}  =  (I - 2 \ensuremath{\bm{\w}} \ensuremath{\bm{\w}}^\ensuremath{\top}) \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 {\ensuremath{\bm{\y}}  \|\ensuremath{\bm{\x}}\|  \over \|\ensuremath{\bm{\y}}\|^2} (\|\ensuremath{\bm{\x}}\|\ensuremath{\mp}x_1) = \ensuremath{\bm{\x}} - \ensuremath{\bm{\y}} =  \ensuremath{\pm}\|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1.
\]
\ensuremath{\QED}

\textbf{Remark} Why do we choose the the opposite sign of $x_1$ for the default reflection? For stability, but we won't discuss this in more detail.

We can extend this definition for complex vectors. In this case the choice of the sign is delicate and so we only generalise the default choice using a complex-analogue of the sign fuunction.

\begin{definition}[Householder reflection, complex case] For a given vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$, define the Householder reflection as
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} := Q_{\ensuremath{\bm{\w}}}
\]
for $\ensuremath{\bm{\y}} = {\rm csign}(x_1) \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}$ and $\ensuremath{\bm{\w}} = {\ensuremath{\bm{\y}} \over \|\ensuremath{\bm{\y}}\|}$, for ${\rm csign}(z) = {\rm e}^{{\rm i} \arg z}$.  \end{definition}

\begin{lemma}[Householder reflection maps to axis, complex case] For $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$,
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} \ensuremath{\bm{\x}} = -{\rm csign}(x_1) \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1
\]
\end{lemma}
\textbf{Proof} Denote $\ensuremath{\alpha} := {\rm csign}(x_1)$.  Note that $\baralpha x_1 = {\rm e}^{-{\rm i} \arg x_1} x_1 = |x_1|$.  Now we have
\begin{align*}
\| \ensuremath{\bm{\y}} \|^2 &= (\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}})^\ensuremath{\star}(\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}) = |\ensuremath{\alpha}|\| \ensuremath{\bm{\x}} \|^2 + \| \ensuremath{\bm{\x}} \|  \ensuremath{\alpha} \bar x_1 + \baralpha x_1 \| \ensuremath{\bm{\x}} \| + \| \ensuremath{\bm{\x}} \|^2 \\
&= 2\| \ensuremath{\bm{\x}} \|^2 + 2|x_1| \| \ensuremath{\bm{\x}} \| \\
\ensuremath{\bm{\y}}^\ensuremath{\star} \ensuremath{\bm{\x}} &= \baralpha x_1 \| \ensuremath{\bm{\x}} \| + \|\ensuremath{\bm{\x}} \|^2 = \|\ensuremath{\bm{\x}} \|^2 + |x_1| \| \ensuremath{\bm{\x}} \|
\end{align*}
Therefore:
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} \ensuremath{\bm{\x}}  =  (I - 2 \ensuremath{\bm{\w}} \ensuremath{\bm{\w}}^\ensuremath{\star}) \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 {\ensuremath{\bm{\y}}    \over \|\ensuremath{\bm{\y}}\|^2} (\|\ensuremath{\bm{\x}} \|^2 + |x_1| \|\ensuremath{\bm{\x}} \|) = \ensuremath{\bm{\x}} - \ensuremath{\bm{\y}} =  -\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1.
\]
\ensuremath{\QED}



