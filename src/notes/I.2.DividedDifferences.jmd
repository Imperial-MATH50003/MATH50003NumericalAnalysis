# Divided Differences

Given a function, how can we approximate its derivative at a
point? We consider an intuitive approach to this problem using _(Right-sided) Divided Differences_: 
$$
f'(x) ≈ {f(x+h) - f(x) \over h}
$$
Note by the definition of the derivative we know that this approximation will converge
to the true derivative as $h → 0$. But in numerical approimxations we also
need to consider the rate of convergence. 

Now in the previous section I mentioned there are three basic tools in analysis: 
(1) integration-by-parts, (2) geometric series
or (3) Taylor series. In this case we use (3):

**Proposition (divided differences error)**
Suppose that $f$ is twice-differentiable on the interval $[x,x+h]$.
The error in approximating the derivative using divided differences is
$$
f'(x) = {f(x+h) - f(x) \over h} + δ
$$
where $|δ| ≤ Mh/2$ for  $M = \sup_{x ≤ t ≤ x+h} |f''(t)|$.

**Proof**
Follows immediately from Taylor's theorem:
$$
f(x+h) = f(x) + f'(x) h + \underbrace{{f''(t) \over 2} h^2}_{h δ}
$$
for some $x ≤ t ≤ x+h$, by bounding:
$$
|δ| ≤ \abs{{f''(t) \over 2} h} ≤ {M  h \over 2}.
$$

∎

Unlike the rectangular rule, the computational cost of computing the divided difference is
independent of $h$! We only need to evaluate a function $f$ twice and do a single division.
Here we are assuming that the computational cost of evaluating $f$ is independent of the point
of evaluation. Later we will investigate the details of how computers work with numbers via
floating point,  and confirm that this is a sensible assumption.

So why not just set $h$ ridiculously small? In the lab we explore this question and observe that
there are significant errors introduced in the numerical realisation of this algorithm.
We will return to the question of understanding these errors after learning floating point numbers. 


There are alternative versions of divided differences. Left-side divided differences evaluates to the left
of the point where we wish to know the derivative:
$$
f'(x) ≈ {f(x) - f(x-h) \over h}
$$
and central differences:
$$
f'(x) ≈ {f(x + h) - f(x - h) \over 2h}
$$
We can further arrive at an approximation to the second derivative by composing a left- and right-sided
finite difference:
$$
f''(x) ≈ {f'(x+h) - f'(x) \over h} ≈ {{f(x+h) - f(x) \over h} - {f(x) - f(x-h) \over h} \over h}
= {f(x+h) - 2f(x)  + f(x-h) \over h^2}
$$
In the lab we investigate the convergence rate of these approximations (in particular, that 
central differences is more accurate than standard divided differences) and observe that they too
suffer from unexplained (for now) loss of accuracy as $h → 0$. In the problem sheet we prove the theoretical
convergence rate, which is never realised because of these errors.