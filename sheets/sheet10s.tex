\documentclass[12pt,a4paper]{article}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}




\hypersetup
       {   pdfauthor = {  },
           pdftitle={  },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }




\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    upquote=true,
    breaklines=true,
    breakindent=0pt,
    keepspaces=true,
    showspaces=false,
    columns=fullflexible,
    showtabs=false,
    showstringspaces=false,
    escapeinside={(*@}{@*)},
    extendedchars=true,
}
\newcommand{\HLJLt}[1]{#1}
\newcommand{\HLJLw}[1]{#1}
\newcommand{\HLJLe}[1]{#1}
\newcommand{\HLJLeB}[1]{#1}
\newcommand{\HLJLo}[1]{#1}
\newcommand{\HLJLk}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkc}[1]{\textcolor[RGB]{59,151,46}{\textit{#1}}}
\newcommand{\HLJLkd}[1]{\textcolor[RGB]{214,102,97}{\textit{#1}}}
\newcommand{\HLJLkn}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkp}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkr}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkt}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLn}[1]{#1}
\newcommand{\HLJLna}[1]{#1}
\newcommand{\HLJLnb}[1]{#1}
\newcommand{\HLJLnbp}[1]{#1}
\newcommand{\HLJLnc}[1]{#1}
\newcommand{\HLJLncB}[1]{#1}
\newcommand{\HLJLnd}[1]{\textcolor[RGB]{214,102,97}{#1}}
\newcommand{\HLJLne}[1]{#1}
\newcommand{\HLJLneB}[1]{#1}
\newcommand{\HLJLnf}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnfm}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnp}[1]{#1}
\newcommand{\HLJLnl}[1]{#1}
\newcommand{\HLJLnn}[1]{#1}
\newcommand{\HLJLno}[1]{#1}
\newcommand{\HLJLnt}[1]{#1}
\newcommand{\HLJLnv}[1]{#1}
\newcommand{\HLJLnvc}[1]{#1}
\newcommand{\HLJLnvg}[1]{#1}
\newcommand{\HLJLnvi}[1]{#1}
\newcommand{\HLJLnvm}[1]{#1}
\newcommand{\HLJLl}[1]{#1}
\newcommand{\HLJLld}[1]{\textcolor[RGB]{148,91,176}{\textit{#1}}}
\newcommand{\HLJLs}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsa}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsb}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsc}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsd}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdC}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLse}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLsh}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsi}[1]{#1}
\newcommand{\HLJLso}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsr}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLss}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLssB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLnB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnbB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnfB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnh}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLni}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnil}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnoB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLoB}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLow}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLp}[1]{#1}
\newcommand{\HLJLc}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLch}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcm}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcp}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcpB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcs}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcsB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLg}[1]{#1}
\newcommand{\HLJLgd}[1]{#1}
\newcommand{\HLJLge}[1]{#1}
\newcommand{\HLJLgeB}[1]{#1}
\newcommand{\HLJLgh}[1]{#1}
\newcommand{\HLJLgi}[1]{#1}
\newcommand{\HLJLgo}[1]{#1}
\newcommand{\HLJLgp}[1]{#1}
\newcommand{\HLJLgs}[1]{#1}
\newcommand{\HLJLgsB}[1]{#1}
\newcommand{\HLJLgt}[1]{#1}


\def\endash{â€“}
\def\bbD{ {\mathbb D} }
\def\bbZ{ {\mathbb Z} }
\def\bbR{ {\mathbb R} }
\def\bbC{ {\mathbb C} }

\def\x{ {\vc x} }
\def\a{ {\vc a} }
\def\b{ {\vc b} }
\def\e{ {\vc e} }
\def\f{ {\vc f} }
\def\u{ {\vc u} }
\def\v{ {\vc v} }
\def\y{ {\vc y} }
\def\z{ {\vc z} }
\def\w{ {\vc w} }

\def\bt{ {\tilde b} }
\def\ct{ {\tilde c} }
\def\Ut{ {\tilde U} }
\def\Qt{ {\tilde Q} }
\def\Rt{ {\tilde R} }
\def\Xt{ {\tilde X} }
\def\acos{ {\rm acos}\, }

\def\red#1{ {\color{red} #1} }
\def\blue#1{ {\color{blue} #1} }
\def\green#1{ {\color{ForestGreen} #1} }
\def\magenta#1{ {\color{magenta} #1} }

\input{somacros}

\begin{document}



\textbf{Numerical Analysis MATH50003 (2023\ensuremath{\endash}24) Problem Sheet 10}

\textbf{Problem 1} What are the upper $3 \ensuremath{\times} 3$ sub-block of the multiplication matrix $X$ / Jacobi matrix $J$ for the monic and orthonormal polynomials with respect to the following weights on $[-1,1]$:
\[
1-x, \sqrt{1-x^2}, 1-x^2
\]
\textbf{SOLUTION}

\paragraph{Monic}
We know that for monic ($b_n=1$) orthogonal polynomials we can write the upper 3x3 block in the form
\[
X_3 = \begin{bmatrix} a_0 & c_0 & 0 \\ 1 & a_1 & c_1 \\ 0 & 1 & a_2 \end{bmatrix}
\]
\begin{itemize}
\item[1. ] \[
w(x) = 1-x
\]
\end{itemize}
Take $\ensuremath{\pi}_0(x) = 1$ (monic) and note
\[
\| \ensuremath{\pi}_0 \|^2 = \int_{-1}^1 (1-x) {\rm d}x = 2
\]
From
\[
x\ensuremath{\pi}_0(x) = a_0\ensuremath{\pi}_0(x) + \ensuremath{\pi}_1(x)
\]
we deduce
\[
a_0 = \ensuremath{\langle}x \ensuremath{\pi}_0, \ensuremath{\pi}_0\ensuremath{\rangle}/\|\ensuremath{\pi}_0\|^2 =  {\int_{-1}^1 (1-x) x {\rm d}x \over 2} =  -{1 \over 3}
\]
i.e.
\[
\ensuremath{\pi}_1(x) = (x-a_0) \ensuremath{\pi}_0(x) = x + 1/3.
\]
and note that
\[
\|\ensuremath{\pi}_1\|^2 = \int_{-1}^1 (1-x) (x+1/3)^2 {\rm d} x = 4/9.
\]
From
\[
x\ensuremath{\pi}_1(x) = c_0 \ensuremath{\pi}_0(x) + a_1 \ensuremath{\pi}_1(x) + \ensuremath{\pi}_2(x)
\]
we deduce
\[
c_0 = \ensuremath{\langle}x \ensuremath{\pi}_1, \ensuremath{\pi}_0\ensuremath{\rangle}/\|\ensuremath{\pi}_0\|^2 =  {\int_{-1}^1 (1-x) x (x+1/3) {\rm d}x \over 2} =  {2 \over 9}
\]
and
\[
a_1 = \ensuremath{\langle}x \ensuremath{\pi}_1, \ensuremath{\pi}_1\ensuremath{\rangle}/\|\ensuremath{\pi}_1\|^2 =  {9 \over 4} {\int_{-1}^1 (1-x) x (x+1/3)^2 {\rm d}x} =  -{1 \over 15}
\]
Thus
\[
\ensuremath{\pi}_2(x) = (x - a_1) \ensuremath{\pi}_1(x) - c_0 \ensuremath{\pi}_0(x) = (x+1/15) (x+1/3) - 2/9 = x^2 + 2x/5 -1/5.
\]
And once again as before:
\[
c_1=\frac{\ensuremath{\langle} \ensuremath{\pi}_1, x\ensuremath{\pi}_2\ensuremath{\rangle}}{\|\ensuremath{\pi}_1\|^2}= \frac{\int_{-1}^1 (x+\frac{1}{3})(x^2+\frac{2}{5}x- \frac{1}{5}) x(1-x) dx}{\int_{-1}^1 (x+\frac{1}{3})^2 (1-x) dx}= \frac{6}{25}
\]
and
\[
a_2 = \frac{\ensuremath{\langle} \ensuremath{\pi}_2, x\ensuremath{\pi}_2\ensuremath{\rangle}}{\|\ensuremath{\pi}_2\|^2} = \frac{\int_{-1}^1 (x^2+\frac{2}{5}x- \frac{1}{5})^2 x(1-x) dx}{\int_{-1}^1 (x^2+\frac{2}{5}x- \frac{1}{5})^2 (1-x) dx}= -\frac{1}{35}
\]
Thus we have
\[
X_3 = \begin{bmatrix}
-1/3 & 2/9 \\
1 & -1/15 & 6/25 \\
& 1 & -1/35
\end{bmatrix}
\]
\begin{itemize}
\item[2. ] \[
w(x)=\sqrt{1-x^2}
\]
\end{itemize}
Take $\ensuremath{\pi}_0(x) = k_0 = 1$ (monic) so that
\[
\|\ensuremath{\pi}_0\|^2 = \int_{-1}^1 \sqrt{1-x^2} = {\ensuremath{\pi} \over 2}.
\]
From PS9 we know that $a_k = 0$. Thus from the recurrence we have
\[
x\ensuremath{\pi}_0(x) =  \ensuremath{\pi}_1(x)
\]
and hence
\[
\ensuremath{\pi}_1(x) = x \ensuremath{\pi}_0(x) = x.
\]
Likewise for
\[
x\ensuremath{\pi}_1(x)= c_0\ensuremath{\pi}_0(x)+\ensuremath{\pi}_2(x)
\]
we have
\[
c_0=\frac{\ensuremath{\langle} \ensuremath{\pi}_0, x\ensuremath{\pi}_1\ensuremath{\rangle}}{\|\ensuremath{\pi}_0\|^2} \frac{\int_{-1}^1 x^2\sqrt{1-x^2} dx}{\ensuremath{\pi}/2}=\frac{\ensuremath{\pi}/8}{\ensuremath{\pi}/2}= \frac{1}{4}
\]
i.e.
\[
\ensuremath{\pi}_2(x) = x\ensuremath{\pi}_1(x) - c_0 = x^2 - {1 \over 4}.
\]
Finally:
\[
x\ensuremath{\pi}_2(x)= c_1\ensuremath{\pi}_1(x)\ensuremath{\pi}_3(x)
\]
and thus
\[
c_1=\frac{\ensuremath{\langle} \ensuremath{\pi}_1, x\ensuremath{\pi}_2\ensuremath{\rangle}}{\|\ensuremath{\pi}_1\|^2}= \frac{\int_{-1}^1 (x^2- \frac{1}{4}) x^2\sqrt{1-x^2} dx}{\int_{-1}^1 x^2 \sqrt{1-x^2} dx}= \frac{\ensuremath{\pi}/32}{\ensuremath{\pi}/8}=\frac{1}{4}
\]
Thus we have
\[
X_3 = \begin{bmatrix}
0 & 1/4 \\
1 & 0 & 1/4 \\
& 1 & 0
\end{bmatrix}
\]
\begin{itemize}
\item[3. ] \[
w(x)=1-x^2
\]
\end{itemize}
Take $\ensuremath{\pi}_0(x) = k_0 = 1$ (monic). Again due to $w(x) = w(-x)$ from recurrence we have
\[
x\ensuremath{\pi}_0(x) = \ensuremath{\pi}_1(x)
\]
Then from
\[
x\ensuremath{\pi}_1(x)= c_0\ensuremath{\pi}_0(x)+\ensuremath{\pi}_2(x)
\]
we find
\[
c_0=\frac{\ensuremath{\langle} \ensuremath{\pi}_0, x\ensuremath{\pi}_1\ensuremath{\rangle}}{\|\ensuremath{\pi}_0\|^2} \frac{\int_{-1}^1 x^2(1-x^2) dx}{4/15}=\frac{4/15}{4/3}= \frac{1}{5}
\]
Finally,
\[
x\ensuremath{\pi}_2(x)= c_1\ensuremath{\pi}_1(x)+\ensuremath{\pi}_3(x)
\]
and thus
\[
c_1=\frac{\ensuremath{\langle} \ensuremath{\pi}_1, x\ensuremath{\pi}_2\ensuremath{\rangle}}{\|\ensuremath{\pi}_1\|^2}= \frac{\int_{-1}^1 (x^2- \frac{1}{5}) x^2(1-x^2) dx}{\int_{-1}^1 x^2 (1-x^2) dx}= \frac{32/525}{4/15}=\frac{8}{35}
\]
Thus we have
\[
X_3 = \begin{bmatrix}
0 & 1/5 \\
1 & 0 & 8/35 \\
& 1 & 0
\end{bmatrix}
\]
\paragraph{Orthonormal}
The hard way to solve this problem is to compute $\|\ensuremath{\pi}_n\|$ for each case. Instead, we use a trick for computing the orthonormal  variants: III.3 Corollary 6 tells us that if we find constants $\ensuremath{\alpha}_n$ and define
\[
q_n(x) := \ensuremath{\alpha}_n \ensuremath{\pi}_n(x)
\]
so that $\|q_0\| = 1$ and the resulting Jacobi matrix is symmetric then $q_n$ must be orthonormal. Note that the three-term recurrence for $q_n$ satisfies
\begin{align*}
x q_0 = x \ensuremath{\alpha}_0 \ensuremath{\pi}_0 = \ensuremath{\alpha}_0 a_0 \ensuremath{\pi}_0 + \ensuremath{\alpha}_0 \ensuremath{\pi}_1 = a_0 q_0 + {\ensuremath{\alpha}_0 \over \ensuremath{\alpha}_1} q_1 \\
x q_m = x \ensuremath{\alpha}_n \ensuremath{\pi}_n = \ensuremath{\alpha}_n c_{n-1} \ensuremath{\pi}_{n-1} +  a_n \ensuremath{\alpha}_n \ensuremath{\pi}_n + \ensuremath{\alpha}_n \ensuremath{\pi}_{n+1} = {\ensuremath{\alpha}_n c_{n-1} \over \ensuremath{\alpha}_{n-1}}  q_{n-1} + a_n q_n + {\ensuremath{\alpha}_n \over \ensuremath{\alpha}_{n+1}} q_{n+1}
\end{align*}
This is easier to see using linear algebra:
\begin{align*}
x [q_0 | q_1 | \ensuremath{\ldots} ] &= x[\ensuremath{\pi}_0 | \ensuremath{\pi}_1 | \ensuremath{\ldots}] \begin{bmatrix} \ensuremath{\alpha}_0 \\ & \ensuremath{\alpha}_1 \\ && \ensuremath{\ddots} \end{bmatrix} = [\ensuremath{\pi}_0 | \ensuremath{\pi}_1 | \ensuremath{\ldots}] X \begin{bmatrix} \ensuremath{\alpha}_0 \\ & \ensuremath{\alpha}_1 \\ && \ensuremath{\ddots} \end{bmatrix}   \\
&=[q_0 | q_1 | \ensuremath{\ldots}] \begin{bmatrix} \ensuremath{\alpha}_0^{-1} \\ & \ensuremath{\alpha}_1^{-1} \\ && \ensuremath{\ddots} \end{bmatrix}  X \begin{bmatrix} \ensuremath{\alpha}_0 \\ & \ensuremath{\alpha}_1 \\ && \ensuremath{\ddots} \end{bmatrix} \\
&=[q_0 | q_1 | \ensuremath{\ldots}] \underbrace{\begin{bmatrix}  a_0 & c_0 \ensuremath{\alpha}_1/\ensuremath{\alpha}_0 \\
									\ensuremath{\alpha}_0/\ensuremath{\alpha}_1 & a_1 & c_1 \ensuremath{\alpha}_2/\ensuremath{\alpha}_1 \\
									& \ensuremath{\alpha}_1/\ensuremath{\alpha}_2 & a_2 & \ensuremath{\ddots} \\
									&& \ensuremath{\ddots} & \ensuremath{\ddots}
									\end{bmatrix}}_{\Xt}
\end{align*}
Thus to make this symmetric we need $\ct_n := c_n \ensuremath{\alpha}_{n+1}/\ensuremath{\alpha}_n = \ensuremath{\alpha}_n/\ensuremath{\alpha}_{n+1} =: \bt_n$, i.e., $\ensuremath{\alpha}_{n+1} = \ensuremath{\alpha}_n/\sqrt{c_n}$, in other words,
\[
\ensuremath{\alpha}_n = {\ensuremath{\alpha}_0 \over \ensuremath{\prod}_{k=0}^{n-1} \sqrt{c_k}}.
\]
Moreover, we see with this choice that $\ct_n = \sqrt{b_n} = \sqrt{c_n}$.

\begin{itemize}
\item[1. ] \[
w(x) = 1-x
\]
. We know $q_0(x) = \ensuremath{\alpha}_0 = 1/\|\ensuremath{\pi}_0\| = 1/\sqrt{2}$. Then $\ensuremath{\alpha}_1 = 1/\sqrt{2c_0} =3/2$ (hence $q_1(x) = \ensuremath{\alpha}_1 \ensuremath{\pi}_1(x) = 3x/2+ 1/2$),

\end{itemize}
which tells us
\[
\ct_0 = c_0 \ensuremath{\alpha}_1/\ensuremath{\alpha}_0 = \sqrt{2}/3 = \bt_0 (= \sqrt{c_0}.)
\]
Then $\ensuremath{\alpha}_2 = \ensuremath{\alpha}_1/\sqrt{c_1} = 15/(2\sqrt{6})$ which tells us $\ct_1 = c_1 \ensuremath{\alpha}_2/\ensuremath{\alpha}_1 = \sqrt{6}/5 = \bt_1 (= \sqrt{c_1})$. In other words we have,
\[
\Xt_3 = \begin{bmatrix}
-1/3 & \sqrt{2}/3 \\
 \sqrt{2}/3 & -1/15 & \sqrt{6}/5 \\
& \sqrt{6}/5 & -1/35
\end{bmatrix}
\]
\begin{itemize}
\item[2. ] \[
w(x) = \sqrt{1-x^2}
\]
We can just jump ahead since we know the answer is just with $\sqrt{c_n}$ in place of $b_n$ and $c_n$:

\end{itemize}
\[
\Xt_3 = \begin{bmatrix}
0 & 1/2 \\
1/2 & 0 & 1/2 \\
& 1/2 & 0
\end{bmatrix}
\]
\begin{itemize}
\item[3. ] \[
w(x) = 1-x^2
\]
:

\end{itemize}
\[
\Xt_3 = \begin{bmatrix}
0 & 1/\sqrt{5} \\
1/\sqrt{5} & 0 & \sqrt{8/35} \\
& \sqrt{8/35} & 0
\end{bmatrix}
\]
\textbf{END}

\textbf{Problem 2} Compute the roots of the Legendre polynomial $P_3(x)$, orthogonal with respect to $w(x) = 1$ on $[-1,1]$, by computing the eigenvalues of a $3 \ensuremath{\times} 3$ truncation of the Jacobi matrix.

\textbf{SOLUTION}

We have, $P_0(x) = 1$. Though recall that in order to use Lemma (zeros), the Jacobi matrix must be symmetric and hence the polynomials orthonormal. So Take $Q_0(x) = 1/||P_0(x)|| = \frac{1}{\sqrt{2}}$. Then we have, by the three term recurrence relationship,
\[
xQ_0(x) = a_0Q_0(x) + b_0Q_1(x),
\]
and taking the inner product of both sides with $Q_0(x)$ we get,
\[
a_0 = \ensuremath{\langle} xQ_0(x), Q_0(x) \ensuremath{\rangle} = \int_{-1}^1 x/2 dx = 0.
\]
Next recall that $P_1(x) =  x$ and so $Q_1(x) = x/||P_1(x)||=\sqrt{\frac{3}{2}} x$. We then have, taking the innner product of the first equation above with $Q_ 1(x)$,
\[
b_0 = \ensuremath{\langle} xQ_0(x), Q_1(x)\ensuremath{\rangle} = \int_{-1}^1 \frac{\sqrt{3}}{2}x^2 dx = \frac{1}{\sqrt{3}},
\]
and also $b_0 = c_0$ by the Corollary 8 (Jacobi matrix). We have,
\[
a_1 = \ensuremath{\langle} xQ_1(x), Q_1(x)\ensuremath{\rangle} = \int_{-1}^1 \frac{3}{2}x^3 dx = 0.
\]
Recall that $P_2(x) = \frac{1}{2}(3x^2 - 1)$, so that $Q_2(x) = P_2(x)/||P_2(x)|| = \sqrt{\frac{5}{8}}(3x^2 - 1)$, and that,
\[
xQ_1(x) = c_0Q_0(x) + a_1Q_1(x) + b_1Q_2(x).
\]
Taking inner the inner product of both sides with $Q_2(x)$, we see that,
\[
c_1 = b_1 = \ensuremath{\langle} xQ_1(x), Q_2(x)\ensuremath{\rangle} = \int_{-1}^1 \sqrt{\frac{5}{8}} \cdot \sqrt{\frac{3}{2}}(3x^2 - 1)\cdot x \cdot xdx =\frac{2}{\sqrt{15}}.
\]
Finally,
\[
a_2 = \ensuremath{\langle} Q_2(x), xQ_2(x) \ensuremath{\rangle} = \frac{5}{8}\int_{-1}^1 (3x^2 - 1)^2 x dx = 0.
\]
This gives us the truncated Jacobi matrix,
\[
X_3 = \left[\begin{matrix}
a_0 & b_0	& 0 \\
b_0 & a_1 & b_1 \\
0&b_1 & a_2
\end{matrix}
 \right] = \left[\begin{matrix}
0 & \frac{1}{\sqrt{3}}	& 0 \\
\frac{1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{15}} \\
0& \frac{2}{\sqrt{15}} & 0
\end{matrix}
 \right],
\]
whose eigenvalues are the zeros of $Q_3(x)$, and hence the zeros of $P_3(x)$ since they are the same up to a constant. To work out the eigenvalues, we have,
\begin{align*}
	|X_3 - \lambda I| = \left| \begin{matrix}
		-\lambda & \frac{1}{\sqrt{3}} & 0\\
		\frac{1}{\sqrt{3}} & -\lambda & \frac{2}{\sqrt{15}}\\
		0 & \frac{2}{\sqrt{15}} & -\lambda
	\end{matrix}\right| &= 0 \\
	\Leftrightarrow -\lambda(\lambda^2 - \frac{4}{15}) - \frac{1}{\sqrt{3}}\cdot \frac{-\lambda}{\sqrt{3}} &=0 \\
	\Leftrightarrow -\lambda^3 + \frac{3}{5}\lambda &= 0,
\end{align*}
which has solutions $\lambda = 0, \ensuremath{\pm} \sqrt{\frac{3}{5}}$

\textbf{END}

\textbf{Problem 3} Compute the 2-point interpolatory quadrature rule associated with roots of orthogonal polynomials for the weights $\sqrt{1-x^2}$, $1$, and $1-x$ on $[-1,1]$ by integrating the Lagrange bases.

\textbf{SOLUTION} For $w(x) = \sqrt{1-x^2}$ the orthogonal polynomial of degree 2 is $U_2(x) = 4x^2 -1$, with roots $\ensuremath{\bm{\x}} = \{x = \ensuremath{\pm} \frac{1}{2}\}$. The Lagrange polynomials corresponding to these roots are,
\begin{align*}
\ensuremath{\ell}_1(x) &= \frac{x - 1/2}{-1/2 - 1/2} = \frac{1}{2} - x, \\
\ensuremath{\ell}_2(x) &= \frac{x + 1/2}{1/2 + 1/2} = x + \frac{1}{2}
\end{align*}
We again work out the weights
\[
w_j = \int_{-1}^1 \ensuremath{\ell}_j(x)w(x)dx,
\]
to find,
\[
w_1 = w_2 = {\ensuremath{\pi} \over 4},
\]
and thus the interpolatory quadrature rule is,
\[
\ensuremath{\Sigma}_2^{w,\ensuremath{\bm{\x}}}(f) = \frac{\ensuremath{\pi}}{4}(f(-1/2) + f(1/2)).
\]
For $w(x) = 1$, the orthogonal polynomial of degree 2 is, using Legendre Rodriguez formula:
\[
P_2(x) = \frac{1}{(-2)^22!} \frac{d^2}{dx^2}\left(1 - x^2\right)^2 = -\frac{1}{2} + \frac{3}{2}x^2.
\]
This has roots $\ensuremath{\bm{\x}} = \left\{\ensuremath{\pm} \frac{1}{\sqrt{3}}\right\}$. We then have,
\begin{align*}
	\ensuremath{\ell}_1(x) &= -\frac{\sqrt{3}}{2}x + \frac{1}{2} \\
	\ensuremath{\ell}_2(x) &= \frac{3}{2}x + \frac{1}{2},
\end{align*}
from which we can compute the weights,
\[
w_1 = w_2 = 1,
\]
which give the quadrature rule:
\[
\ensuremath{\Sigma}_2^{w,\ensuremath{\bm{\x}}}(f) = \left[f\left(-\frac{1}{\sqrt{3}}\right) + f\left(\frac{1}{\sqrt{3}}\right)\right]
\]
Finally, with $w(x) = 1 - x$ we use the solution to PS9, which states that
\[
p_2(x) = x^2 + 2x/5 - 1/5
\]
which has roots, $\ensuremath{\bm{\x}} = \left\{-\frac{1}{5} \ensuremath{\pm} \frac{\sqrt{6}}{5} \right\}$. The Lagrange polynomials are then,
\begin{align*}
	\ensuremath{\ell}_1(x) &= \frac{x - (-\frac{1}{5} + \frac{\sqrt{6}}{5} )}{-\frac{1}{5} - \frac{\sqrt{6}}{5} - (-\frac{1}{5} + \frac{\sqrt{6}}{5}) } \\
	&= \frac{x - (-\frac{1}{5} + \frac{\sqrt{6}}{5} )}{-\frac{2\sqrt{6}}{5}} \\
	&=-\frac{5}{2\sqrt{6}}x - \frac{1}{2\sqrt{6}} + \frac{1}{2} \\
	\ensuremath{\ell}_2(x) &= \frac{x - (-\frac{1}{5} - \frac{\sqrt{6}}{5} )}{\frac{2\sqrt{6}}{5}} \\
	&= \frac{5}{2\sqrt{6}}x + \frac{1}{2\sqrt{6}} + \frac{1}{2}
\end{align*}
From which we can compute the weights,
\begin{align*}
	w_1 &= 1 + \frac{\sqrt{6}}{9}, \\
	w_2 &= 1 - \frac{\sqrt{6}}{9},
\end{align*}
giving the quadrature rule,
\[
\ensuremath{\Sigma}_2^{w,\ensuremath{\bm{\x}}}(f) = \left[\left(1 + \frac{\sqrt{6}}{9} \right)f\left(-\frac{1}{5} - \frac{\sqrt{6}}{5} \right) + \left(1 - \frac{\sqrt{6}}{9} \right)f\left(-\frac{1}{5} + \frac{\sqrt{6}}{5} \right) \right]
\]
\textbf{END}

\textbf{Problem 4(a)} For the matrix
\[
J_n = \begin{bmatrix} 0 & 1/\sqrt{2} \\
                1/\sqrt{2} & 0 & 1/2 \\
				& 1/2 & 0 & \ensuremath{\ddots}  \\
                && \ensuremath{\ddots} & \ensuremath{\ddots} & 1/2 \\
                &&& 1/2 & 0
                \end{bmatrix} \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}
\]
use the relationship with the Jacobi matrix associated with $T_n(x)$ to prove that, for $x_j = \cos \ensuremath{\theta}_j$, and $\ensuremath{\theta}_j = (n-j+1/2)\ensuremath{\pi}/n$,
\[
J_n = Q_n \begin{bmatrix} x_1 \\ & \ensuremath{\ddots} \\ && x_n \end{bmatrix} Q_n^\ensuremath{\top}
\]
where
\meeq{
\ensuremath{\bm{\e}}_1^\ensuremath{\top} Q_n \ensuremath{\bm{\e}}_j = {1 \over \sqrt{n}}, \qquad
\ensuremath{\bm{\e}}_k^\ensuremath{\top} Q_n \ensuremath{\bm{\e}}_j = \sqrt{2 \over n} \cos (k-1) \ensuremath{\theta}_j.
}
You may use without proof the sums-of-squares formula
\[
1 + 2 \ensuremath{\sum}_{k=1}^{n-1} \cos^2 k \ensuremath{\theta}_j = n.
\]
\textbf{SOLUTION}

Recall the three term recurrence for the Chebyshev Polynomials $T_n$,
\begin{align*}
	xT_0(x) &= T_1(x), \\
	xT_n(x) &= \frac{T_{n-1}(x)}{2} + \frac{T_{n+1}(x)}{2},
\end{align*}
and hence it has the multiplication matrix
\[
x [T_0 | T_1 | \ensuremath{\cdots}] = [T_0 | T_1 | \ensuremath{\cdots}] \underbrace{\begin{bmatrix} 0 & 1/2 \\
													1 & 0 & 1/2 \\
														& 1/2 & 0 & \ensuremath{\ddots} \\
														&&\ensuremath{\ddots} & \ensuremath{\ddots}
														\end{bmatrix}}_X
\]
To find the Jacobi matrix we need to symmetrise this, that is, we write
\[
[q_0(x) | q_1(x) | \ensuremath{\cdots} ] = [T_0(x) | T_1(x)| \ensuremath{\cdots}] \underbrace{\begin{bmatrix}  \ensuremath{\beta}_0 \\ & \ensuremath{\beta}_1 \\ && \ensuremath{\beta}_2 \\ &&& \ensuremath{\ddots} \end{bmatrix}}_K
\]
so that
\[
x [q_0(x) | q_1(x) | \ensuremath{\cdots} ] = [q_0(x) | q_1(x) | \ensuremath{\cdots} ] \underbrace{K^{-1} X K}_{J}
\]
where
\[
 K^{-1} X K = \begin{bmatrix} 0 & \ensuremath{\beta}_1/(2\ensuremath{\beta}_0) \\
													\ensuremath{\beta}_0/\ensuremath{\beta}_1 & 0 & \ensuremath{\beta}_2/(2\ensuremath{\beta}_1) \\
														& \ensuremath{\beta}_1/(2\ensuremath{\beta}_2) & 0 & \ensuremath{\ddots} \\
														&&\ensuremath{\ddots} & \ensuremath{\ddots}
														\end{bmatrix}
\]
First recall that the change-of-variables $x = \cos \ensuremath{\theta}$ tells us
\[
\ensuremath{\int}_{-1}^1 {{\rm d}x \over \sqrt{1-x^2}} = \ensuremath{\pi}
\]
hence $q_0(x) = \ensuremath{\beta}_0 =  1/\sqrt{\ensuremath{\pi}}$. From this we find that
\[
{\ensuremath{\beta}_0 \over \ensuremath{\beta}_1} = {\ensuremath{\beta}_1 \over 2\ensuremath{\beta}_0} \ensuremath{\Rightarrow} \ensuremath{\beta}_1 = \sqrt{2/\ensuremath{\pi}}.
\]
Other equations give us:
\[
{\ensuremath{\beta}_n \over 2 \ensuremath{\beta}_{n+1}} = {\ensuremath{\beta}_{n+1} \over 2\ensuremath{\beta}_n} \ensuremath{\Rightarrow} \ensuremath{\beta}_{n+1} = \ensuremath{\beta}_n = \sqrt{2/\ensuremath{\pi}}.
\]
Hence since $\ensuremath{\beta}_1/(2\ensuremath{\beta}_0) = 1/\sqrt{2}$ and $\ensuremath{\beta}_{n+1}/(2\ensuremath{\beta}_n) = 1/2$ we have
\[
J = \begin{bmatrix} 0 & 1/\sqrt{2} \\
					1/\sqrt{2} & 0 & 1/2 \\
					& 1/2 & 0 & 1/2 \\
					&& \ensuremath{\ddots} & \ensuremath{\ddots} & \ensuremath{\ddots}
					\end{bmatrix}
\]
The roots of $q_n(x)$ are the roots of $T_n(x) = \cos n \acos x$, i.e.,  $x_j = \cos \ensuremath{\theta}_j$ for $\ensuremath{\theta}_j = (n-j+1/2)\ensuremath{\pi}/n$. Thus we know that we can diagonalise $J_n$ as
\[
J_n  = Q_n \begin{bmatrix} x_1 \\ & \ensuremath{\ddots} \\ && x_n \end{bmatrix} Q_n^\ensuremath{\top}
\]
where
\[
Q_n = \begin{bmatrix}
q_0(x_1) & \ensuremath{\cdots} & q_0(x_n) \\
\ensuremath{\vdots}  & \ensuremath{\cdots} & \ensuremath{\vdots}  \\
q_{n-1}(x_1) & \ensuremath{\cdots} & q_{n-1}(x_n)
\end{bmatrix} \begin{bmatrix} \ensuremath{\alpha}_1^{-1} \\ & \ensuremath{\ddots} \\ && \ensuremath{\alpha}_n^{-1} \end{bmatrix}
\]
where
\[
\ensuremath{\alpha}_j = \sqrt{q_0(x_j)^2 + \ensuremath{\cdots} + q_{n-1}(x_j)^2} = {1 \over \sqrt{\ensuremath{\pi}}} \sqrt{1 +  2\ensuremath{\sum}_{k=1}^{n-1} \cos k \ensuremath{\theta}_j} = \sqrt{n \over \ensuremath{\pi}}.
\]
Thus we have
\meeq{
\ensuremath{\bm{\e}}_1^\ensuremath{\top} Q_n \ensuremath{\bm{\e}}_j = {q_0(x_j) \over  \ensuremath{\alpha}_j} = {1 \over \sqrt{n}} \ccr
\ensuremath{\bm{\e}}_k^\ensuremath{\top} Q_n \ensuremath{\bm{\e}}_j = {q_{k-1}(x_j) \over  \ensuremath{\alpha}_j} = \sqrt{2 \over n} \cos (k-1) \ensuremath{\theta}_j.
}
\textbf{END}

\textbf{Problem 4(b)} Show for $w(x) = 1/\sqrt{1-x^2}$ that the Gaussian quadrature rule is
\[
Q_n^w[f] = {\ensuremath{\pi} \over n} \sum_{j=1}^n f(x_j)
\]
where $x_j = \cos \ensuremath{\theta}_j$ for $\ensuremath{\theta}_j = (j-1/2)\ensuremath{\pi}/n$.

\textbf{SOLUTION} This follows immediately from the previous parts as $x_j$ are the eigenvalues of $J_n$ and the weights in Gauss quadrature have the form
\[
{1 \over \ensuremath{\alpha}_j^2} = {\ensuremath{\pi} \over n}.
\]
\textbf{END}

\textbf{Problem 4(c)} Give an explicit formula for the polynomial that interpolates $\exp x$ at the points $x_1,\ensuremath{\ldots},x_n$ as defined above, in terms of Chebyshev polynomials with the coefficients defined in terms of a sum involving only exponentials, cosines and $\ensuremath{\theta}_j = (n-j+1/2)\ensuremath{\pi}/n$.

\textbf{SOLUTION}

From Theorem 18 we know the interpolatory polynomial is 
\[
f_n(x) = \ensuremath{\sum}_{k=0}^{n-1} c_k^n q_k(x)
\]
where $q_0(x) = 1/\sqrt{\ensuremath{\pi}}$ and $q_n(x) = \sqrt{2/\ensuremath{\pi}} T_n(x)$ and
\[
c_k^n = \ensuremath{\Sigma}_n^w[\exp(x) q_k] = {\ensuremath{\pi} \over n} \ensuremath{\sum}_{j=1}^n \exp( \cos \ensuremath{\theta}_j) \cos(k \ensuremath{\theta}_j)
\]
for $\ensuremath{\theta}_j = (n-j+1/2)\ensuremath{\pi}/n$.

\textbf{END}



\end{document}