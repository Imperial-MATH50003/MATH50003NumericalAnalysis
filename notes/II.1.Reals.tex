
\section{Reals}
In this chapter, we introduce  the  \href{https://en.wikipedia.org/wiki/IEEE_754}{IEEE Standard for Floating-Point Arithmetic}. There are multiplies ways of representing real numbers on a computer, as well as  the precise behaviour of operations such as addition, multiplication, etc. One can use

\begin{itemize}
\item[1. ] \href{https://en.wikipedia.org/wiki/Fixed-point_arithmetic}{Fixed-point arithmetic}: essentially representing a real number as an integer where a decimal point is inserted at a fixed position. This turns out to be impractical in most applications, e.g., due to loss of relative accuracy for small numbers.


\item[2. ] \href{https://en.wikipedia.org/wiki/Floating-point_arithmetic}{Floating-point arithmetic}: essentially scientific notation where an exponent is stored alongside a fixed number of digits. This is what is used in practice.


\item[3. ] \href{https://en.wikipedia.org/wiki/Symmetric_level-index_arithmetic}{Level-index arithmetic}: stores numbers as iterated exponents. This is the most beautiful mathematically but unfortunately is not as useful for most applications and is not implemented in hardware.

\end{itemize}
Before the 1980s each processor had potentially a different representation for  floating-point numbers, as well as different behaviour for operations.  IEEE introduced in 1985 was a means to standardise this across processors so that algorithms would produce consistent and reliable results.

This chapter may seem very low level for a mathematics course but there are two important reasons to understand the behaviour of floating-point numbers in details:

\begin{itemize}
\item[1. ] Floating-point arithmetic is very precisely defined, and can even be used in rigorous computations as we shall see in the labs. But it is not exact and its important to understand how errors in computations can accumulate.


\item[2. ] Failure to understand floating-point arithmetic can cause catastrophic issues in practice, with the extreme example being the  \href{https://youtu.be/N6PWATvLQCY?t=86}{explosion of the Ariane 5 rocket}.

\end{itemize}
\subsection{Real numbers in binary}
Reals can also be presented in binary format, that is, a sequence of \texttt{0}s and \texttt{1}s alongside a decimal point:

\begin{definition}[real binary format] For $b_1,b_2,\ensuremath{\ldots}\in \{0,1\}$, Denote a non-negative real number in \emph{binary format} by:
\[
(B_p \ensuremath{\ldots}B_0.b_1b_2b_3\ensuremath{\ldots})_2 := (B_p \ensuremath{\ldots}B_0)_2 +  \sum_{k=1}^\ensuremath{\infty} {b_k \over 2^k}.
\]
\end{definition}

\begin{example}[rational in binary] Consider the number \texttt{1/3}.  In decimal recall that:
\[
1/3 = 0.3333\ensuremath{\ldots}=  \sum_{k=1}^\ensuremath{\infty} {3 \over 10^k}
\]
We will see that in binary
\[
1/3 = (0.010101\ensuremath{\ldots})_2 = \sum_{k=1}^\ensuremath{\infty} {1 \over 2^{2k}}
\]
Both results can be proven using the geometric series:
\[
\sum_{k=0}^\ensuremath{\infty} z^k = {1 \over 1 - z}
\]
provided $|z| < 1$. That is, with $z = {1 \over 4}$ we verify the binary expansion:
\[
\sum_{k=1}^\ensuremath{\infty} {1 \over 4^k} = {1 \over 1 - 1/4} - 1 = {1 \over 3}
\]
A similar argument with $z = 1/10$ shows the decimal case. \end{example}

\subsection{Floating-point numbers}
Floating-point numbers are a subset of real numbers that are representable using a fixed number of bits.

\begin{definition}[floating-point numbers] Given integers $\ensuremath{\sigma}$ (the \emph{exponential shift}), $Q$ (the number of \emph{exponent bits}) and  $S$ (the \emph{precision}), define the set of \emph{Floating-point numbers} by dividing into \emph{normal}, \emph{sub-normal}, and \emph{special number} subsets:
\[
F_{\ensuremath{\sigma},Q,S} := F^{\rm normal}_{\ensuremath{\sigma},Q,S} \cup F^{\rm sub}_{\ensuremath{\sigma},Q,S} \cup F^{\rm special}.
\]
The \emph{normal numbers} $F^{\rm normal}_{\ensuremath{\sigma},Q,S} \ensuremath{\subset} \ensuremath{\bbR}$ are
\[
F^{\rm normal}_{\ensuremath{\sigma},Q,S} := \{\red{\ensuremath{\pm}} 2^{\green{q}-\ensuremath{\sigma}} \ensuremath{\times} (1.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2 : 1 \ensuremath{\leq} q < 2^Q-1 \}.
\]
The \emph{sub-normal numbers} $F^{\rm sub}_{\ensuremath{\sigma},Q,S} \ensuremath{\subset} \ensuremath{\bbR}$ are
\[
F^{\rm sub}_{\ensuremath{\sigma},Q,S} := \{\red{\ensuremath{\pm}} 2^{\green{1}-\ensuremath{\sigma}} \ensuremath{\times} (0.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2\}.
\]
The \emph{special numbers} $F^{\rm special} \ensuremath{\nsubset} \ensuremath{\bbR}$ are 
\[
F^{\rm special} :=  \{\ensuremath{\infty}, -\ensuremath{\infty}, {\rm NaN}\}
\]
where ${\rm NaN}$ is a special symbol representing \ensuremath{\ldq}not a number", essentially an error flag. \end{definition}

Note this set of real numbers has no nice \emph{algebraic structure}: it is not closed under addition, subtraction, etc. On the other hand, we can control errors effectively hence it is extremely useful for analysis.

Floating-point numbers are stored in $1 + Q + S$ total number of bits, in the format
\[
\red{s}\ \green{q_{Q-1}\ensuremath{\ldots}q_0}\ \blue{b_1\ensuremath{\ldots}b_S}
\]
The first bit ($s$) is the \emph{sign bit}: 0 means positive and 1 means negative. The bits $q_{Q-1}\ensuremath{\ldots}q_0$ are the \emph{exponent bits}: they are the binary digits of the unsigned integer $q$: 
\[
q = (\green{q_{Q-1}\ensuremath{\ldots}q_0})_2.
\]
Finally, the bits $b_1\ensuremath{\ldots}b_S$ are the \emph{significand bits}. If $1 \ensuremath{\leq} q < 2^Q-1$ then the bits represent the normal number
\[
x = \red{\ensuremath{\pm}} 2^{\green{q}-\ensuremath{\sigma}} \ensuremath{\times} (1.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2.
\]
If $q = 0$ (i.e. all bits are 0) then the bits represent the sub-normal number
\[
x = \red{\ensuremath{\pm}} 2^{\green{1}-\ensuremath{\sigma}} \ensuremath{\times} (0.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2.
\]
If $q = 2^Q-1$  (i.e. all bits are 1) then the bits represent a special number, discussed later.

\subsection{IEEE floating-point numbers}
\begin{definition}[IEEE floating-point numbers]  IEEE has 3 standard floating-point formats: 16-bit (half precision), 32-bit (single precision) and 64-bit (double precision) defined by (you \emph{do not} need to memorise these):
\begin{align*}
F_{16} &:= F_{15,5,10} \\
F_{32} &:= F_{127,8,23} \\
F_{64} &:= F_{1023,11,52}
\end{align*}
\end{definition}

\begin{example}[interpreting 16-bits as a float] Consider the number with bits
\[
\red{0}\ \green{10000}\ \blue{1010000000}
\]
assuming it is a half-prevision float ($F_{16}$).  Since the sign bit is \texttt{0} it is positive. The exponent is $2^4 - \ensuremath{\sigma} = 16 - 15 = 1$ Hence this number is:
\[
2^1 (1.1010000000)_2 = 2 (1 + 1/2 + 1/8) = 3+1/4 = 3.25.
\]
\end{example}

\begin{example}[rational to 16-bits] How is the number $1/3$ stored in $F_{16}$? Recall that
\[
1/3 = (0.010101\ensuremath{\ldots})_2 = 2^{-2} (1.0101\ensuremath{\ldots})_2 = 2^{13-15} (1.0101\ensuremath{\ldots})_2
\]
and since $13 = (1101)_2$  the exponent bits are \texttt{01101}. For the significand we round the last bit to the nearest element of $F_{16}$,  (the exact rule for rounding is explained in detail later), so we have
\[
1.010101010101010101010101\ensuremath{\ldots}\approx 1.0101010101 \in F_{16} 
\]
and the significand bits are \texttt{0101010101}. Thus the stored bits for $1/3$ are:
\[
\red{0}\ \green{01101}\ \blue{0101010101}
\]
\end{example}

\subsection{Sub-normal and special numbers}
For sub-normal numbers, the simplest example is zero, which has $q=0$ and all significand bits zero: \texttt{0 00000 0000000000}. Unlike integers, we also have a negative zero, which has bits: \texttt{1 00000 0000000000}. This is treated as identical to positive \texttt{0} (except for degenerate operations as explained in the lab).

\begin{example}[subnormal in 16-bits] Consider the number with bits
\[
\red{1}\ \green{00000}\ \blue{1100000000}
\]
assuming it is a half-prevision float ($F_{16}$).  Since all exponent bits are zero it is sub-normal. Since the sign bit is \texttt{1} it is negative.  Hence this number is:
\[
-2^{1-\ensuremath{\sigma}} (0.1100000000)_2 = -2^{-14} (2^{-1} + 2^{-2}) = -3 \ensuremath{\times} 2^{-16}
\]
\end{example}

The special numbers extend the real line by adding $\ensuremath{\pm}\ensuremath{\infty}$ but also a notion of ``not-a-number" ${\rm NaN}$. Whenever the bits of $q$ of a floating-point number are all 1 then they represent an element of $F^{\rm special}$. If all $b_k=0$, then the number represents either $\ensuremath{\pm}\ensuremath{\infty}$. All other special floating-point numbers represent ${\rm NaN}$. 

\begin{example}[special in 16-bits] The number with bits
\[
\red{1}\ \green{11111}\ \blue{0000000000}
\]
has all exponent bits equal to $1$, and significand bits $0$ and sign bit $1$, hence represents $-\ensuremath{\infty}$. On the other hand, the number with bits
\[
\red{1}\ \green{11111}\ \blue{0000000001}
\]
has all exponent bits equal to $1$ but does not have all significand bits equal to $0$, hence is one of many representations for  ${\rm NaN}$. \end{example}



