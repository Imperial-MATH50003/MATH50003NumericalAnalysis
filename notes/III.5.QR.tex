
\section{QR Factorisation}
Let $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ be a rectangular or square matrix such that $m \ensuremath{\geq} n$ (i.e. more rows then columns). In this section we consider two closely related factorisations, which can be viewed in some sense as a competitor to the PLU factorisation, but one that extends to rectangular matrices and allows for the solution of least squares problems.

The QR factorisation consists of writing $A$ as a product of a (square) \emph{orthogonal} and a (rectangular) \emph{right triangular} matrix:

\begin{definition}[QR factorisation] The \emph{QR factorisation} is
\[
A = Q R = \underbrace{\begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_m \end{bmatrix}}_{Q \ensuremath{\in} U(m)} \underbrace{\begin{bmatrix} \ensuremath{\times} & \ensuremath{\cdots} & \ensuremath{\times} \\ & \ensuremath{\ddots} & \ensuremath{\vdots} \\ && \ensuremath{\times} \\ &&0 \\ &&\ensuremath{\vdots} \\ && 0 \end{bmatrix}}_{R \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}}
\]
where $Q$ is unitary (i.e., $Q \ensuremath{\in} U(m)$, satisfying $Q^\ensuremath{\star}Q = I$, with columns $\ensuremath{\bm{\q}}_j \ensuremath{\in} \ensuremath{\bbC}^m$) and $R$ is \emph{right triangular}, which means it  is only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$). \end{definition}

It is often more convenient to work with an alternative version known as the reduced QR factorisation, which is a product of a (rectangular) matrix with orthonormal columns and a (square) upper triangular matrix:

\begin{definition}[Reduced QR factorisation] The \emph{reduced QR factorisation}
\[
A = \hat Q \hat R = \underbrace{\begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_n \end{bmatrix}}_{ \hat Q \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}} \underbrace{\begin{bmatrix} \ensuremath{\times} & \ensuremath{\cdots} & \ensuremath{\times} \\ & \ensuremath{\ddots} & \ensuremath{\vdots} \\ && \ensuremath{\times}  \end{bmatrix}}_{\hat R \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}}
\]
where $\hat Q$ has orthonormal columns ($\hat Q^\ensuremath{\star} \hat Q = I$, $\ensuremath{\bm{\q}}_j \ensuremath{\in} \ensuremath{\bbC}^m$) and $\hat R$ is upper triangular. \end{definition}

Note for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is \emph{upper triangular}. The importance of these factorisation for square matrices is that their component pieces are easy to invert:
\[
A = QR \qquad \ensuremath{\Rightarrow} \qquad A^{-1}\ensuremath{\bm{\b}} = R^{-1} Q^\ensuremath{\top} \ensuremath{\bm{\b}}
\]
and we saw previously that triangular and orthogonal matrices are easy to invert when applied to a vector $\ensuremath{\bm{\b}}$. On the other hand, in the rectangular case the QR factorisation contains within it the reduced QR factorisation:
\[
A = QR = \begin{bmatrix} \hat Q | \ensuremath{\bm{\q}}_{n+1} | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_m \end{bmatrix} \begin{bmatrix} \hat R \\  \ensuremath{\bm{\zero}}_{m-n \ensuremath{\times} n} \end{bmatrix} = \hat Q \hat R.
\]
For rectangular matrices we will see that the QR factorisation leads to efficient solutions to the \emph{least squares problem}: find $\ensuremath{\bm{\x}}$ that minimizes the 2-norm $\| A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \|.$

In this section we discuss the following:

\begin{itemize}
\item[1. ] Reduced QR and Gram\ensuremath{\endash}Schmidt: The Reduced QR factorisation is equivalent to the Gram\ensuremath{\endash}Schmidt procedure, which you may have seen in 1st year or in the 1st half of the module.


\item[2. ] Householder reflections and QR: Alternatively, the  QR factorisation can be computed using a sequence of Householder reflections. This process mimics that of the LU factorisation, but using orthogonal matrices in-place of lower triangular ones to introduces zeros in column-by-column.  This is a more accurate approach for computing QR factorisations than Gram\ensuremath{\endash}Schmidt as applying orthogonal matrices is stable.


\item[3. ] QR and least squares: We discuss the QR factorisation and its usage in solving least squares problems.

\end{itemize}
\subsection{Reduced QR and Gram\ensuremath{\endash}Schmidt}
How do we compute the QR factorisation? We begin with a method you may have seen before in another guise. Write
\[
A = \begin{bmatrix} \ensuremath{\bm{\a}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\a}}_n \end{bmatrix}
\]
where $\ensuremath{\bm{\a}}_k \ensuremath{\in}  \ensuremath{\bbC}^m$ and assume they are linearly independent ($A$ has full column rank).

\begin{proposition}[Column spaces match] Suppose $A = \hat Q  \hat R$ where $\hat Q = [\ensuremath{\bm{\q}}_1|\ensuremath{\ldots}|\ensuremath{\bm{\q}}_n]$ has orthonormal columns and $\hat R$ is upper-triangular, and $A$ has full rank. Then the first $j$ columns of $\hat Q$ span the same space as the first $j$ columns of $A$:
\[
\hbox{span}(\ensuremath{\bm{\a}}_1,\ensuremath{\ldots},\ensuremath{\bm{\a}}_j) = \hbox{span}(\ensuremath{\bm{\q}}_1,\ensuremath{\ldots},\ensuremath{\bm{\q}}_j).
\]
\end{proposition}
\textbf{Proof}

Because $A$ has full rank we know $\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} \ensuremath{\neq}Â 0$. If $\ensuremath{\bm{\v}} \ensuremath{\in} \hbox{span}(\ensuremath{\bm{\a}}_1,\ensuremath{\ldots},\ensuremath{\bm{\a}}_j)$ we have for $\ensuremath{\bm{\c}} \ensuremath{\in} \ensuremath{\bbC}^j$
\[
\ensuremath{\bm{\v}} = \begin{bmatrix} \ensuremath{\bm{\a}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\a}}_j \end{bmatrix} \ensuremath{\bm{\c}} = 
\begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_j \end{bmatrix}  \hat R[1:j,1:j] \ensuremath{\bm{\c}} \ensuremath{\in} \hbox{span}(\ensuremath{\bm{\q}}_1,\ensuremath{\ldots},\ensuremath{\bm{\q}}_j)
\]
while if $\ensuremath{\bm{\w}} \ensuremath{\in} \hbox{span}(\ensuremath{\bm{\q}}_1,\ensuremath{\ldots},\ensuremath{\bm{\q}}_j)$ we have for $\vc d \ensuremath{\in} \ensuremath{\bbR}^j$
\[
\ensuremath{\bm{\w}} = \begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_j \end{bmatrix} \vc d  =  \begin{bmatrix} \ensuremath{\bm{\a}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\a}}_j \end{bmatrix} \hat R[1:j,1:j]^{-1} \vc d \ensuremath{\in}  \hbox{span}(\ensuremath{\bm{\a}}_1,\ensuremath{\ldots},\ensuremath{\bm{\a}}_j).
\]
\ensuremath{\QED}

It is possible to find $\hat Q$ and $\hat R$   using the \emph{Gram\ensuremath{\endash}Schmidt algorithm}. We construct it column-by-column. For $j = 1, 2, \ensuremath{\ldots}, n$ define
\begin{align*}
\ensuremath{\bm{\v}}_j &:= \ensuremath{\bm{\a}}_j - \ensuremath{\sum}_{k=1}^{j-1} \underbrace{\ensuremath{\bm{\q}}_k^\ensuremath{\star} \ensuremath{\bm{\a}}_j}_{r_{kj}} \ensuremath{\bm{\q}}_k, \\
r_{jj} &:= {\|\ensuremath{\bm{\v}}_j\|}, \\
\ensuremath{\bm{\q}}_j &:= {\ensuremath{\bm{\v}}_j \over r_{jj}}.
\end{align*}
\textbf{Theorem (Gram\ensuremath{\endash}Schmidt and reduced QR)} Define $\ensuremath{\bm{\q}}_j$ and $r_{kj}$ as above (with $r_{kj} = 0$ if $k > j$). Then a reduced QR factorisation is given by:
\[
A = \underbrace{\begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_n \end{bmatrix}}_{ \hat Q \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}} \underbrace{\begin{bmatrix} r_{11} & \ensuremath{\cdots} & r_{1n} \\ & \ensuremath{\ddots} & \ensuremath{\vdots} \\ && r_{nn}  \end{bmatrix}}_{\hat R \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}}
\]
\textbf{Proof}

We first show that $\hat Q$ has orthonormal columns. Assume that $\ensuremath{\bm{\q}}_\ensuremath{\ell}^\ensuremath{\star} \ensuremath{\bm{\q}}_k = \ensuremath{\delta}_{\ensuremath{\ell}k}$ for $k,\ensuremath{\ell} < j$.  For $\ensuremath{\ell} < j$ we then have
\[
\ensuremath{\bm{\q}}_\ensuremath{\ell}^\ensuremath{\star} \ensuremath{\bm{\v}}_j = \ensuremath{\bm{\q}}_\ensuremath{\ell}^\ensuremath{\star} \ensuremath{\bm{\a}}_j - \ensuremath{\sum}_{k=1}^{j-1}  \ensuremath{\bm{\q}}_\ensuremath{\ell}^\ensuremath{\star}\ensuremath{\bm{\q}}_k \ensuremath{\bm{\q}}_k^\ensuremath{\star} \ensuremath{\bm{\a}}_j = 0
\]
hence $\ensuremath{\bm{\q}}_\ensuremath{\ell}^\ensuremath{\star} \ensuremath{\bm{\q}}_j = 0$ and indeed $\hat Q$ has orthonormal columns. Further: from the definition of $\ensuremath{\bm{\v}}_j$ we find
\[
\ensuremath{\bm{\a}}_j = \ensuremath{\bm{\v}}_j + \ensuremath{\sum}_{k=1}^{j-1} r_{kj} \ensuremath{\bm{\q}}_k = \ensuremath{\sum}_{k=1}^j r_{kj} \ensuremath{\bm{\q}}_k  = \hat Q \hat R \ensuremath{\bm{\e}}_j
\]
\ensuremath{\QED}

\subsection{Householder reflections and QR}
As an alternative, we will consider using Householder reflections to introduce zeros below the diagonal. Thus, if Gram\ensuremath{\endash}Schmidt is a process of \emph{triangular orthogonalisation} (using triangular matrices to orthogonalise), Householder reflections is a process of \emph{orthogonal triangularisation}  (using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column, that is, for
\[
Q_1 := Q_{\ensuremath{\bm{\a}}_1}^{\rm H},
\]
consider
\[
Q_1 A = \begin{bmatrix} \ensuremath{\times} & \ensuremath{\times} & \ensuremath{\cdots} & \ensuremath{\times} \\
& \ensuremath{\times} & \ensuremath{\cdots} & \ensuremath{\times} \\
                    & \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                    & \ensuremath{\times} & \ensuremath{\cdots} & \ensuremath{\times} \end{bmatrix} = 
\begin{bmatrix}  \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\ 
& A_2   \end{bmatrix}
\]
where 
\[
\ensuremath{\alpha}_1 := -{\rm csign}(a_{11})  \|\ensuremath{\bm{\a}}_1\|, \ensuremath{\bm{\w}}_1 = (Q_1 A)[1, 2:n]  \qquad \hbox{and} \qquad A_2 = (Q_1 A)[2:m, 2:n],
\]
where as before ${\rm csign}(z) :=  {\rm e}^{{\rm i} \arg z}$. That is, we have made the first column triangular. In terms of an algorithm, we then introduce zeros into the first column of $A_2$, leaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple proof by induction, reminisicent of our proofs for the PLU and Cholesky factorisations:

\begin{theorem}[QR]  Every matrix $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ has a QR factorisation:
\[
A = QR
\]
where $Q \ensuremath{\in} U(m)$ and $R \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ is right triangular.

\end{theorem}
\textbf{Proof}

First assume $m \ensuremath{\geq} n$. If $A = [\ensuremath{\bm{\a}}_1] \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} 1}$ then we have for the Householder reflection $Q_1 = Q_{\ensuremath{\bm{\a}}_1}^{\rm H}$
\[
Q_1 A = \ensuremath{\alpha} \ensuremath{\bm{\e}}_1
\]
which is right triangular, where $\ensuremath{\alpha} = -{\rm csign}(a_{11}) \|\ensuremath{\bm{\a}}_1\|$.  In other words 
\[
A = \underbrace{Q_1}_Q \underbrace{\ensuremath{\alpha} \ensuremath{\bm{\e}}_1}_R.
\]
For $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation. For $A = [\ensuremath{\bm{\a}}_1|\ensuremath{\ldots}|\ensuremath{\bm{\a}}_n] \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$, let $Q_1 = Q_{\ensuremath{\bm{\a}}_1}^{\rm H}$ so that
\[
Q_1 A =  \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\ & A_2 \end{bmatrix}.
\]
By assumption $A_2 = \tilde Q \tilde R$. Thus we have (recalling that $Q_1^{-1} = Q_1^\ensuremath{\star} = Q_1$):
\begin{align*}
A = Q_1 \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\ & \tilde Q \tilde R \end{bmatrix} \\
=\underbrace{Q_1 \begin{bmatrix} 1 \\ & \tilde Q \end{bmatrix}}_Q  \underbrace{\begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\ &  \tilde R \end{bmatrix}}_R.
\end{align*}
If $m < n$, i.e., $A$ has more columns then rows, write 
\[
A = \begin{bmatrix} \At & B \end{bmatrix}
\]
where $\At \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} m}$. From above we know we can write $\At = Q \Rt$. We thus have
\[
A = Q \underbrace{\begin{bmatrix} \Rt & Q^\ensuremath{\star} B \end{bmatrix}}_R
\]
where $R$ is right triangular.

\ensuremath{\QED}

\begin{example}[QR by hand] We will now do an example by hand. Consider finding the QR factorisation where the diagonal of $R$ is positive for the $4 \ensuremath{\times} 3$ matrix
\[
A = \begin{bmatrix} 
1 & 1 & 1 \\ 
-1 & 0 & 1 \\
-1 & -1 & 0 \\
-1 & 0 & 0
\end{bmatrix}
\]
For the first column, since the entry $a_{11} > 0$ on a computer  we would want to choose the Householder reflection that makes this negative, but in this case we want $R$ to have a positive diagonal (partly because the numbers involved become very complicated otherwise!). So instead we choose the "wrong" sign and leave it positive. Since $\| \ensuremath{\bm{\a}}_1 \|$ = 2 we have
\[
\ensuremath{\bm{\y}}_1 = \Vectt[1,-1,-1,-1]  -\Vectt[2,0,0,0] = \Vectt[-1,-1,-1,-1] \ensuremath{\Rightarrow} \ensuremath{\bm{\w}}_1 = {\ensuremath{\bm{\y}}_1 \over \| \ensuremath{\bm{\y}}_1 \|} = {1 \over 2} \Vectt[-1,-1,-1,-1].
\]
Hence
\[
Q_1 :=  I - {1 \over 2} \begin{bmatrix} -1 \\ -1 \\ -1 \\ -1 \end{bmatrix} \begin{bmatrix} -1 & -1 & -1 & -1 \end{bmatrix} =
 {1 \over 2} \begin{bmatrix}
1 & -1 & -1 & -1 \\
-1 & 1 & -1 & -1 \\
-1 & -1 & 1 & -1 \\
-1 & -1 & -1 &  1
\end{bmatrix}
\]
so that
\[
Q_1 A = \begin{bmatrix} 2 &  1 & 0 \\
 & 0 & 0 \\
  & -1 & -1 \\
& 0 & -1
\end{bmatrix}
\]
For the second column we have a zero entry so on a computer we can either send it to positive or negative sign,  but in this case we are told to make it positive. Thus we have
\[
\ensuremath{\bm{\y}}_2 :=   [0,-1,0] - \Vectt[1,0,0] = \Vectt[-1,-1,0]  \ensuremath{\Rightarrow} \ensuremath{\bm{\w}}_2 = {\ensuremath{\bm{\y}}_2 \over \| \ensuremath{\bm{\y}}_2 \|} = {1 \over \sqrt{2}} \Vectt[-1,-1,0]
\]
Thus we have
\[
Q_2 := I - 
 \begin{bmatrix} -1 \\ -1 \\ 0
\end{bmatrix} \begin{bmatrix} -1 & -1 & 0 \end{bmatrix}
= \begin{bmatrix}
0 & -1 & 0 \\
-1& 0 & 0 \\
0 & 0 & 1
\end{bmatrix}
\]
so that
\[
\tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 0 \\
&  & -1
\end{bmatrix}
\]
The final vector is 
\[
\ensuremath{\bm{\y}}_3 := \Vectt[0,-1] - \Vectt[1,0] = \Vectt[-1,-1] \ensuremath{\Rightarrow} \ensuremath{\bm{\w}}_3 = -{1 \over \sqrt{2}} \Vectt[1,1].
\]
Hence
\[
Q_3 := I - \Vectt[1,1] \begin{bmatrix} 1 & 1 \end{bmatrix} = \sopmatrix{0 & - 1 \\ -1 & 0}
\]
so that 
\[
\tilde Q_3 \tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 1 \\
&  & 0
\end{bmatrix} =: R
\]
and
\[
Q := Q_1 \tilde Q_2 \tilde Q_3 = {1 \over 2}  \begin{bmatrix}
1 & 1 & 1 & -1 \\
-1 & 1 & 1 & 1 \\
-1 & -1 & 1 & -1 \\
-1 & 1 & -1 & -1
\end{bmatrix}.
\]
\subsection{QR and least squares}
We consider rectangular matrices with more rows than columns. Given $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ and $\ensuremath{\bm{\b}} \ensuremath{\in} \ensuremath{\bbC}^m$, a least squares problem consists of finding a vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$ that minimises the 2-norm: $\| A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \|$. There is a lot of theory around least squares, however, we focus on a simple computational aspect: we can solve least squares problems using the QR factorisation.

\begin{theorem}[least squares via QR] Suppose $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ with $m \ensuremath{\geq} n$ has full rank and a QR factorisation $A = Q R$ (which includes within it a reduced QR factorisation $A = \hat Q \hat R$). The vector
\[
\ensuremath{\bm{\x}} = \hat R^{-1} \hat Q^\ensuremath{\star} \ensuremath{\bm{\b}}
\]
minimises $\| A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \|$. 

\end{theorem}
\textbf{Proof}

The norm-preserving property ($\|Q\ensuremath{\bm{\x}}\| = \|\ensuremath{\bm{\x}}\|$) of unitary matrices tells us
\[
\| A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \| = \| Q R \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \| = \| Q (R \ensuremath{\bm{\x}} - Q^\ensuremath{\star} \ensuremath{\bm{\b}}) \| = \| R \ensuremath{\bm{\x}} - Q^\ensuremath{\star} \ensuremath{\bm{\b}} \| = \left \| 
\begin{bmatrix} \hat R \\ \ensuremath{\bm{\zero}}_{m-n \ensuremath{\times} n} \end{bmatrix} \ensuremath{\bm{\x}} - \begin{bmatrix} \hat Q^\ensuremath{\star} \\ \ensuremath{\bm{\q}}_{n+1}^\ensuremath{\star} \\ \ensuremath{\vdots} \\ \ensuremath{\bm{\q}}_m^\ensuremath{\star} \end{bmatrix}     \ensuremath{\bm{\b}} \right \|
\]
Now note that the rows $k > n$ are independent of $\ensuremath{\bm{\x}}$ and are a fixed contribution. Thus to minimise this norm it suffices to drop them and minimise:
\[
\| \hat R \ensuremath{\bm{\x}} - \hat Q^\ensuremath{\star} \ensuremath{\bm{\b}} \|.
\]
This norm is minimised if it is zero. Provided the column rank of $A$ is full, $\hat R$ will be invertible.

\end{example}

\subsection{Lab and Problem Sheet}
In the lab we see how Householder reflection can be implemented by an iterative algorithm. To achieve optimal complexity it is important to take advantage of the structure of the Householder reflections, which is explored in the problems. For Tridiagonal matrices it is possible to use rotations in-place of reflections to upper-triangularise a matrix, in which case the resulting matrix is upper-tridiagonal. We investigate implementing this, giving an $O(n)$ algorithm for computing the QR factorisation of a tridiagonal matrix. (This can also be done with sparse Householder reflections, and thereby extended to general banded matrices). Finally, we look at the relationship between least squares and the QR factorisation.

In the problem sheet we see a simple example of a computing the QR factorisation using Householder reflections by-hand. (It's actually very hard to come up with examples where it's reasonable to do it by hand: it's more of a computer-based algorithm than one meant for pen-and-pencil calculations. Therefore this is not examinable but is there to help facilitate understanding.) We also explore some basic properties of QR factorisation such as uniqueness.



