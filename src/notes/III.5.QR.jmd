# QR Factorisation

Let $A âˆˆ â„‚^{m Ã— n}$ be a rectangular or square matrix such that $m â‰¥ n$ (i.e. more rows then columns).
In this section we consider two closely related factorisations:


**Definition (QR factorisation)** The _QR factorisation_ is
$$
A = Q R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_m \end{bmatrix}}_{Q âˆˆ U(m)} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã— \\ &&0 \\ &&â‹® \\ && 0 \end{bmatrix}}_{R âˆˆ â„‚^{m Ã— n}}
$$
where $Q$ is unitary (i.e., $Q âˆˆ U(m)$, satisfying $Q^â‹†Q = I$, with columns $ğª_j âˆˆ â„‚^m$) and $R$ is _right triangular_, which means it 
is only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$).
âˆ

**Definition (Reduced QR factorisation)** The _reduced QR factorisation_
$$
A = \hat Q \hat R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã—  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$
where $\hat Q$ has orthonormal columns ($\hat Q^â‹† \hat Q = I$, $ğª_j âˆˆ â„‚^m$) and $\hat R$ is upper triangular.
âˆ

Note for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is _upper triangular_.
The importance of these factorisation for square matrices is that their component pieces are easy to invert:
$$
A = QR \qquad â‡’ \qquad A^{-1}ğ› = R^{-1} Q^âŠ¤ ğ›
$$
and we saw previously that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$.

For rectangular matrices we will see that the QR factorisation leads to efficient solutions to the _least squares problem_: find
$ğ±$ that minimizes the 2-norm $\| A ğ± - ğ› \|.$
Note in the rectangular case the QR factorisation contains within it the reduced QR factorisation:
$$
A = QR = \begin{bmatrix} \hat Q | ğª_{n+1} | â‹¯ | ğª_m \end{bmatrix} \begin{bmatrix} \hat R \\  ğŸ_{m-n Ã— n} \end{bmatrix} = \hat Q \hat R.
$$




In this section we discuss the following:

1. Reduced QR and Gramâ€“Schmidt: We discuss computation of the Reduced QR factorisation using Gramâ€“Schmidt.
2. Householder reflections and QR: We discuss computing the  QR factorisation using Householder reflections. This is a more accurate approach for computing QR factorisations.
3. QR and least squares: We discuss the QR factorisation and its usage in solving least squares problems.


## Reduced QR and Gramâ€“Schmidt


How do we compute the QR factorisation? We begin with a method
you may have seen before in another guise. Write
$$
A = \begin{bmatrix} ğš_1 | â‹¯ | ğš_n \end{bmatrix}
$$
where $ğš_k âˆˆ  â„‚^m$ and assume they are linearly independent ($A$ has full column rank).


**Proposition (Column spaces match)** Suppose $A = \hat Q  \hat R$ where $\hat Q = [ğª_1|â€¦|ğª_n]$
has orthonormal columns and $\hat R$ is upper-triangular, and $A$ has full rank.
Then the first $j$ columns of
$\hat Q$ span the same space as the first $j$ columns of $A$:
$$
\hbox{span}(ğš_1,â€¦,ğš_j) = \hbox{span}(ğª_1,â€¦,ğª_j).
$$

**Proof**

Because $A$ has full rank we know $\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} â‰ Â 0$.
If $ğ¯ âˆˆ \hbox{span}(ğš_1,â€¦,ğš_j)$ we have for $ğœ âˆˆ â„‚^j$
$$
ğ¯ = \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} ğœ = 
\begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix}  \hat R[1:j,1:j] ğœ âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)
$$
 while if $ğ° âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)$ we have for $\vc d âˆˆ â„^j$
$$
ğ° = \begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix} \vc d  =  \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} \hat R[1:j,1:j]^{-1} \vc d âˆˆ  \hbox{span}(ğš_1,â€¦,ğš_j).
$$

âˆ

 
It is possible to find $\hat Q$ and $\hat R$   using the _Gramâ€“Schmidt algorithm_.
We construct it column-by-column. For $j = 1, 2, â€¦, n$ define
$$
\begin{align*}
ğ¯_j &:= ğš_j - âˆ‘_{k=1}^{j-1} \underbrace{ğª_k^â‹† ğš_j}_{r_{kj}} ğª_k, \\
r_{jj} &:= {\|ğ¯_j\|}, \\
ğª_j &:= {ğ¯_j \over r_{jj}}.
\end{align*}
$$

**Theorem (Gramâ€“Schmidt and reduced QR)** Define $ğª_j$ and $r_{kj}$ as above
(with $r_{kj} = 0$ if $k > j$). Then a reduced QR factorisation is given by:
$$
A = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} r_{11} & â‹¯ & r_{1n} \\ & â‹± & â‹® \\ && r_{nn}  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$

**Proof**

We first show that $\hat Q$ has orthonormal columns. Assume that $ğª_â„“^â‹† ğª_k = Î´_{â„“k}$ for $k,â„“ < j$. 
For $â„“ < j$ we then have
$$
ğª_â„“^â‹† ğ¯_j = ğª_â„“^â‹† ğš_j - âˆ‘_{k=1}^{j-1}  ğª_â„“^â‹†ğª_k ğª_k^â‹† ğš_j = 0
$$
hence $ğª_â„“^â‹† ğª_j = 0$ and indeed $\hat Q$ has orthonormal columns. Further: from the definition of $ğ¯_j$ we find
$$
ğš_j = ğ¯_j + âˆ‘_{k=1}^{j-1} r_{kj} ğª_k = âˆ‘_{k=1}^j r_{kj} ğª_k  = \hat Q \hat R ğ_j
$$

âˆ

## Householder reflections and QR

As an alternative, we will consider using Householder reflections to introduce zeros below
the diagonal.
Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices
to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ 
(using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column,
that is, for
$$
Q_1 := Q_{ğš_1}^{\rm H},
$$
consider
$$
Q_1 A = \begin{bmatrix} Ã— & Ã— & â‹¯ & Ã— \\
& Ã— & â‹¯ & Ã— \\
                    & â‹® & â‹± & â‹® \\
                    & Ã— & â‹¯ & Ã— \end{bmatrix} = 
\begin{bmatrix}  Î±_1 & ğ°_1^âŠ¤ \\ 
& A_2   \end{bmatrix}
$$
where 
$$
Î±_1 := -{\rm csign}(a_{11})  \|ğš_1\|, ğ°_1 = (Q_1 A)[1, 2:n]  \qquad \hbox{and} \qquad A_2 = (Q_1 A)[2:m, 2:n],
$$
where as before ${\rm csign}(z) :=  {\rm e}^{{\rm i} \arg z}$.
That is, we have made the first column triangular.
In terms of an algorithm, we then introduce zeros into the first column of $A_2$,
leaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple
proof by induction, reminisicent of our proofs for the PLU and Cholesky factorisations:

**Theorem (QR)** 
Every matrix $A âˆˆ â„‚^{m Ã— n}$ has a QR factorisation:
$$
A = QR
$$
where $Q âˆˆ U(m)$ and $R âˆˆ â„‚^{m Ã— n}$ is right triangular.

**Proof**

First assume $m â‰¥ n$. If $A = [ğš_1] âˆˆ â„‚^{m Ã— 1}$ then we have for the Householder
reflection $Q_1 = Q_{ğš_1}^{\rm H}$
$$
Q_1 A = Î± ğ_1
$$
which is right triangular, where $Î± = -{\rm csign}(a_{11}) \|ğš_1\|$. 
In other words 
$$
A = \underbrace{Q_1}_Q \underbrace{Î± ğ_1}_R.
$$

For $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation.
For $A = [ğš_1|â€¦|ğš_n] âˆˆ â„‚^{m Ã— n}$, let $Q_1 = Q_{ğš_1}^{\rm H}$ so that
$$
Q_1 A =  \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & A_2 \end{bmatrix}.
$$
By assumption $A_2 = Q_2 R_2$. Thus we have (recalling that $Q_1^{-1} = Q_1^â‹† = Q_1$):
$$
\begin{align*}
A = Q_1 \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & Q_2 R_2 \end{bmatrix} \\
=\underbrace{Q_1 \begin{bmatrix} 1 \\ & Q_2 \end{bmatrix}}_Q  \underbrace{\begin{bmatrix} Î± & ğ°^âŠ¤ \\ &  R_2 \end{bmatrix}}_R.
\end{align*}
$$

If $m < n$, i.e., $A$ has more columns then rows, write 
$$
A = \begin{bmatrix} \At & B \end{bmatrix}
$$
where $\At âˆˆ â„‚^{m Ã— m}$. From above we know we can write $\At = Q \Rt$. We thus have
$$
A = Q \underbrace{\begin{bmatrix} \Rt & Q^â‹† B \end{bmatrix}}_R
$$
where $R$ is right triangular.

âˆ



**Example (QR by hand)** We will now do an example by hand. Consider finding the QR factorisation
where the diagonal of $R$ is positive for the $4 Ã— 3$ matrix
$$
A = \begin{bmatrix} 
1 & 1 & 1 \\ 
-1 & 0 & 1 \\
-1 & -1 & 0 \\
-1 & 0 & 0
\end{bmatrix}
$$
For the first column, since the entry $a_{11} > 0$ on a computer 
we would want to choose the Householder reflection that
makes this negative, but in this case we want $R$ to have a positive diagonal (partly because the numbers involved become very complicated
otherwise!). So instead we choose the "wrong" sign and leave it positive. Since $\| ğš_1 \|$ = 2 we have
$$
ğ²_1 = \Vectt[1,-1,-1,-1]  -\Vectt[2,0,0,0] = \Vectt[-1,-1,-1,-1] â‡’ ğ°_1 = {ğ²_1 \over \| ğ²_1 \|} = {1 \over 2} \Vectt[-1,-1,-1,-1].
$$
 Hence
$$
Q_1 :=  I - {1 \over 2} \begin{bmatrix} -1 \\ -1 \\ -1 \\ -1 \end{bmatrix} \begin{bmatrix} -1 & -1 & -1 & -1 \end{bmatrix} =
 {1 \over 2} \begin{bmatrix}
1 & -1 & -1 & -1 \\
-1 & 1 & -1 & -1 \\
-1 & -1 & 1 & -1 \\
-1 & -1 & -1 &  1
\end{bmatrix}
$$
so that
$$
Q_1 A = \begin{bmatrix} 2 &  1 & 0 \\
 & 0 & 0 \\
  & -1 & -1 \\
& 0 & -1
\end{bmatrix}
$$
For the second column we have a zero entry so on a computer we can either send it to positive or negative sign, 
but in this case we are told to make it positive. Thus we have
$$
ğ²_2 :=   [0,-1,0] - \Vectt[1,0,0] = \Vectt[-1,-1,0]  â‡’ ğ°_2 = {ğ²_2 \over \| ğ²_2 \|} = {1 \over \sqrt{2}} \Vectt[-1,-1,0]
$$
Thus we have
$$
Q_2 := I - 
 \begin{bmatrix} -1 \\ -1 \\ 0
\end{bmatrix} \begin{bmatrix} -1 & -1 & 0 \end{bmatrix}
= \begin{bmatrix}
0 & -1 & 0 \\
-1& 0 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$
so that
$$
\tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 0 \\
&  & -1
\end{bmatrix}
$$
The final vector is 
$$
ğ²_3 := \Vectt[0,-1] - \Vectt[1,0] = \Vectt[-1,-1] â‡’ ğ°_3 = -{1 \over \sqrt{2}} \Vectt[1,1].
$$
Hence
$$
Q_3 := I - \Vectt[1,1] \begin{bmatrix} 1 & 1 \end{bmatrix} = \sopmatrix{0 & - 1 \\ -1 & 0}
$$
so that 
$$
\tilde Q_3 \tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 1 \\
&  & 0
\end{bmatrix} =: R
$$
and
$$
Q := Q_1 \tilde Q_2 \tilde Q_3 = {1 \over 2}  \begin{bmatrix}
1 & 1 & 1 & -1 \\
-1 & 1 & 1 & 1 \\
-1 & -1 & 1 & -1 \\
-1 & 1 & -1 & -1
\end{bmatrix}.
$$


## QR and least squares

We consider rectangular matrices with more rows than columns. Given $A âˆˆ â„‚^{m Ã— n}$ and $ğ› âˆˆ â„‚^m$,
a least squares problem consists of finding a vector $ğ± âˆˆ â„‚^n$ that minimises the 2-norm: $\| A ğ± - ğ› \|$.
There is a lot of theory around least squares, however, we focus on a simple computational aspect: we can
solve least squares problems using the QR factorisation.


**Theorem (least squares via QR)** Suppose $A âˆˆ â„‚^{m Ã— n}$ with $m â‰¥ n$ has full rank and a QR factorisation $A = Q R$
(which includes within it a reduced QR factorisation $A = \hat Q \hat R$). The vector
$$
ğ± = \hat R^{-1} \hat Q^â‹† ğ›
$$
minimises $\| A ğ± - ğ› \|$. 

**Proof**

The norm-preserving property ($\|Qğ±\| = \|ğ±\|$) of unitary matrices tells us
$$
\| A ğ± - ğ› \| = \| Q R ğ± - ğ› \| = \| Q (R ğ± - Q^â‹† ğ›) \| = \| R ğ± - Q^â‹† ğ› \| = \left \| 
\begin{bmatrix} \hat R \\ ğŸ_{m-n Ã— n} \end{bmatrix} ğ± - \begin{bmatrix} \hat Q^â‹† \\ ğª_{n+1}^â‹† \\ â‹® \\ ğª_m^â‹† \end{bmatrix}     ğ› \right \|
$$
Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| \hat R ğ± - \hat Q^â‹† ğ› \|
$$
This norm is minimised if it is zero. Provided the column rank of $A$ is full, $\hat R$ will be invertible.

âˆ



