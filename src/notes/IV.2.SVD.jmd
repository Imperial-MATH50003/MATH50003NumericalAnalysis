# Singular Value Decomposition and Matrix Compression

In the previous section we saw an application of least squares to regression, where the best 2-norm fit to a vector of function
samples was computed. But what if the data is a matrix, eg. corresponding to a 2D function? Here we consider finding the best
approximation to a matrix in the 2-norm sense by by a matrix with a  lower rank.  This concept has numerous applications
in compressing matrices, including  Principle Component Analysis (PCA) and Machine Learning (where one might want to compress the ``weight'' matrices to minimise
degrees of freedom). 


We will use induced matrix norms (see appendix), in particular the 2-norm of a matrix $A âˆˆ â„‚^{m Ã— n}$ is defined via
$$
\| A \| :=  \sup_{ğ¯ âˆˆ â„‚^m : \|ğ¯\|_X=1} \|A ğ¯\| =  \sup_{ğ± âˆˆ â„‚^m} {\|A ğ±\| \over \| ğ±\|}
$$
The matrix 1- and $âˆ$-norms have simple definitions in terms of column/row sums. On the other hand,
the 2-norm has no simple formula. 


In order to define the 2-norm we will introduce the  _Singular Value Decomposition (SVD)_: a matrix factorisation
that encodes how much a matrix ``stretches''  vectors:

**Definition (singular value decomposition)** For $A âˆˆ â„‚^{m Ã— n}$ with rank $r > 0$, 
the _(reduced) singular value decomposition (SVD)_ is
$$
A = U Î£ V^â‹†
$$
where $U âˆˆ â„‚^{m Ã— r}$ and $V âˆˆ  â„‚^{n Ã— r}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose
diagonal entries, which which we call _singular values_, are all positive and non-increasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_r > 0$.
The _full singular value decomposition (SVD)_ is
$$
A = UÌƒ Î£Ìƒ VÌƒ^â‹†
$$
where $UÌƒ âˆˆ U(m)$ and $VÌƒ âˆˆ  U(n)$ are unitary matrices and $Î£Ìƒ âˆˆ â„^{m Ã— n}$ has only
diagonal non-zero entries, i.e., if $m > n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_n \\ && 0 \\ && â‹® \\ && 0 \end{bmatrix}
$$
and if $m < n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_m & 0 & â‹¯ & 0\end{bmatrix}
$$
where $Ïƒ_k = 0$ if $k > r$.
âˆ

In particular, we discuss:

1. Existence of the SVD: we show that an SVD exists by relating it to the eigenvalue Decomposition of $A^â‹†A$ and $AA^â‹†$.
2. 2-norm and SVD: the 2-norm of a matrix is equal to  the largest singular value $Ïƒ_1$.
3. Best rank-$k$ approximation and compression: the best approximation of a matrix by a smaller rank matrix can be constructed using the SVD, which gives an effective way to compress matrices. 

What we do not discuss is computation of the SVD. There are reliable and efficient iterative algorithms for computing the SVD, similar to computing
eigen-decompositions, but this is beyond the scope of this module.


## Existence


To show the SVD exists we first establish some properties of a _Gram matrix_ ($A^â‹† A$), which
has within its eigendecomposition part of the SVD. The Gram matrix is best viewed as the matrix of inner products of the columns of a matrix:
if $A = \begin{bmatrix} ğš_1 | â‹¯ | ğš_n \end{bmatrix} âˆˆ â„‚^{m Ã— n}$ then the Gram matrix is the Hermitian matrix
$$
A^â‹† A = \begin{bmatrix} ğš_1^â‹† ğš_1 & â‹¯ &ğš_1^â‹† ğš_n \\
                        â‹® &  â‹± & â‹® \\
                        ğš_n^â‹† ğš_1 & â‹¯ &ğš_n^â‹† ğš_n \end{bmatrix} âˆˆ â„‚^{n Ã— n}.
$$

We first establish that the kernels match:

**Proposition (Gram matrix kernel)** The kernel of $A$ equals the kernel of $A^â‹† A$. 

**Proof**
If $ğ± âˆˆ \hbox{ker}(A)$, i.e., $A ğ± = 0$, then clearly $A^â‹†A ğ± = 0 = A^â‹† 0 = 0$.
On the other hand, if $ğ± âˆˆ \hbox{ker}(A^â‹†A)$ so  that $A^â‹† A ğ± = 0$ then we have
$$
0 = ğ±^â‹† A^â‹† A ğ± = \| A ğ± \|^2
$$
which means $A ğ± = 0$ and $ğ± âˆˆ \hbox{ker}(A)$.
âˆ

As mentioned in PS6, the spectral theorem states that any normal matrix is unitarily diagonalisable: if $A$ is normal then $A = Q Î› Q^â‹†$
where $Q âˆˆ U(n)$ and $Î›$ is diagonal. In the special case where $A$ is symmetric/Hermitian you would have seen a proof of this in first year.
We can use this to ensure that the Gram matrix is diagonalisable:

**Proposition (Gram matrix diagonalisation)** The Gram matrix
satisfies
$$
A^â‹† A = Q Î› Q^â‹† âˆˆ â„‚^{n Ã— n}
$$
is a Hermitian matrix where $Q âˆˆ U(n)$ and the eigenvalues $Î»_k$ are real and non-negative.
If $A âˆˆ â„^{m Ã— n}$ then we can choose $Q âˆˆ O(n)$.

**Proof**
$A^â‹† A$ is Hermitian so we appeal to the spectral theorem for the
existence of the decomposition. 
To see that the eigenvalues are real and positive  note for the corresponding (orthonormal) eigenvector $ğª_k$ we have
$$
Î»_k = Î»_k ğª_k^â‹† ğª_k = ğª_k^â‹† A^â‹† A ğª_k = \| A ğª_k \|^2 â‰¥ 0.
$$
The fact that real $A$  implies $Q âˆˆ O(n)$ (i.e. has real entries) follows since if $ğª_k âˆˆ â„‚^n$ is an eigenvector than
so is $â„œ ğª_k$:
$$
A^âŠ¤ A â„œ ğª_k = â„œ A^âŠ¤ A  ğª_k  = â„œ Î»_k ğª_k = Î»_k â„œ ğª_k.
$$

âˆ


This connection allows us to prove existence:

**Theorem (SVD existence)** Every $A âˆˆ â„‚^{m Ã— n}$ has an SVD.

**Proof**
Consider
$$
A^â‹† A = Q Î› Q^â‹†.
$$
Assume (as usual) that the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$
are an enumeration of the non-zero eigenvalues and
$$
V := \begin{bmatrix} ğª_1 | â‹¯ | ğª_r \end{bmatrix}
$$
the corresponding (orthonormal) eigenvectors, with the columns
$$
K = \begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \end{bmatrix}
$$
spanning the kernel of $A^â‹† A$ (and hence $A$). 
Define
$$
Î£ :=  \begin{bmatrix} \sqrt{Î»_1} \\ & â‹± \\ && \sqrt{Î»_r} \end{bmatrix}
$$
Now define $U := AV Î£^{-1}$.
Since $A^â‹† A V = V Î£^2$ we can verify that $U$ has orthonormal columns:
$$
U^â‹† U = Î£^{-1} V^â‹† A^â‹† A V Î£^{-1} = I.
$$
Thus we have
$$
U Î£ V^â‹† = A V V^â‹† = A \underbrace{\begin{bmatrix} V | K \end{bmatrix}}_Q\underbrace{\begin{bmatrix} V^â‹† \\ K^â‹† \end{bmatrix}}_{Q^â‹†} = A
$$
since $Q Q^â‹† = I$, and
where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.

âˆ

## The matrix 2-norm


Somewhat surprisingly, the 2-norm for matrices does not have a simple formula but instead, as we shall show, can
be defined in terms of the singular values. We begin with two cases where we do have simple formulÃ¦:


**Proposition (diagonal/orthogonal column 2-norms)** If $Î›$ is diagonal with entries $Î»_k$ then
$\|Î›\| = \max_k |Î»_k|$. If $U$ has orthonormal columns then $\|U\| = \|U^â‹†\| = 1$.

**Proof**

The first property follows from
$$
\| Î› ğ±\| = \sqrt{âˆ‘_{k=1}^n |Î»_k|^2 |x_k|^2} â‰¤  \max_k |Î»_k| \| ğ± \|
$$
hence $\|Î›\| â‰¤  \max_k |Î»_k|$. If $k$ is an index where this maximum is achieved we see this upper bound is achieved by $\| Î›ğ_k\| = |Î»_k|$.

 $U$ with orthonormal columns has unit norm because:
$$
 \|U ğ±\|^2 = ğ±^â‹† \underbrace{U^â‹†U}_I ğ± =  \|ğ±\|^2.
$$

To show $U^â‹†$ also has unit norm is more challenging. We first note from the spectral theorem we know we can diagonalise
$$
U U^â‹† = Q Î› Q^â‹†
$$
and therefore 
$$
\| U U^â‹† \| â‰¤ \|Q \| \| Î› \| \| Q^â‹† \| =\| Î› \|.
$$
We further note this is a projection, that is, it equals its square:
$$
(U U^â‹†)^2 = U \underbrace{U^â‹† U}_{I} U^â‹† = U U^â‹†.
$$
Thus the eigenvalues are either 0 or 1 since:
$$
Î»_j = Î»_j ğª_j^â‹† ğª_j = ğª_j^â‹† U U^â‹† ğª_j = ğª_j^â‹† (U U^â‹†)^2 ğª_j = Î»_j^2
$$
and thus $\| U U^â‹† \| â‰¤ \| Î›\| â‰¤ 1$. We thus have  (using the fact that multiplication by $U$ preserves norm)
$$
\|U^â‹† ğ± \| = \|U U^ â‹† ğ± \| â‰¤ \| U U^â‹†\| \| ğ±\| â‰¤ \| ğ±\|
$$
hence $\| U^â‹†\| â‰¤ 1$. But this bound is achieved using any column of $U$ since, eg. for $ğ®_1 = Uğ_1$ we have
$$
\sup_{\|ğ¯\|=1} \| U^â‹† ğ¯\| â‰¥\| U^â‹† ğ®_1 \| = \| ğ_1 \| = 1.
$$




âˆ


These two facts allow us to deduce the 2-norm from the SVD of a matrix:

**Corollary (singular values and norm)**
$$
\|A \| = Ïƒ_1
$$
and if $A âˆˆ â„‚^{n Ã— n}$ is invertible, then
$$
\|A^{-1} \| = Ïƒ_n^{-1}
$$

**Proof**

First we establish the upper-bound using the fact for induced norms that $\|A B\| â‰¤ \|A\| \|B\|$ (see appendix):
$$
\|A \| â‰¤  \|U \| \| Î£ \| \| V^â‹†\| = \| Î£ \|  = Ïƒ_1
$$
This is attained using the first right singular vector: for $ğ¯_1 = V ğ_1$ we have
$$
\|A ğ¯_1\| = \|Î£ V^â‹† ğ¯_1\| = \|Î£  ğ_1\| = Ïƒ_1
$$
The inverse result follows since the inverse has SVD
$$
A^{-1} = V Î£^{-1} U^â‹† = (V W) (W Î£^{-1} W) (W U)^â‹†
$$
is the SVD of $A^{-1}$, i.e. $VW âˆˆ U(n)$ are the left singular vectors and
$W U$ are the right singular vectors, where
$$
W := P_Ïƒ = \begin{bmatrix} && 1 \\ & â‹° \\ 1 \end{bmatrix}
$$
is the permutation that reverses the entries of a vector, that is, $Ïƒ$ has Cauchy notation
$$
\begin{pmatrix}
1 & 2 & â‹¯ & n \\
n & n-1 & â‹¯ & 1
\end{pmatrix}.
$$

âˆ


## Best rank-$k$ approximation and compression

One of the main usages for SVDs is low-rank compression: approximating a (possibly full rank) matrix $A âˆˆ â„‚^{m Ã— n}$ by a matrix with a much smaller rank $k â‰ª m,n$.

**Theorem (best low rank approximation)** The  matrix
$$
A_k := \underbrace{\begin{bmatrix} ğ®_1 | â‹¯ | ğ®_k \end{bmatrix}}_{=: U_k âˆˆ â„‚^{m Ã— k}} \underbrace{\begin{bmatrix}
Ïƒ_1 \\
& â‹± \\
&& Ïƒ_k\end{bmatrix}}_{=: Î£_k âˆˆ â„‚^{k Ã— k}} \underbrace{\begin{bmatrix} ğ¯_1 | â‹¯ | ğ¯_k \end{bmatrix}^â‹†}_{=: V_k^â‹† âˆˆ â„‚^{k Ã— n}}
$$
is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have 
$$
Ïƒ_{k+1} = \|A - A_k\| â‰¤ \|A -B \|.
$$


**Proof**
We have
$$
A - A_k = U \begin{bmatrix} 0  \cr &\ddots \cr && 0 \cr &&& Ïƒ_{k+1} \cr &&&& \ddots \cr &&&&& Ïƒ_r\end{bmatrix} V^â‹†
= \begin{bmatrix} ğ®_{k+1} | â‹¯ | ğ®_r \end{bmatrix}
\begin{bmatrix}
Ïƒ_{k+1} \\
& â‹± \\
&& Ïƒ_r \end{bmatrix}\begin{bmatrix} ğ¯_{k+1} | â‹¯ | ğ¯_r \end{bmatrix}^â‹†
$$
hence $\| A - A_k \| = Ïƒ_{k+1}$.


Suppose a rank-$k$ matrix $B$ has 
$$
\|A-B\|  < \|A-A_k\| = Ïƒ_{k+1}.
$$
For all $ğ° âˆˆ \ker(B)$ we have 
$$
\|A ğ°\| = \|(A-B) ğ°\| â‰¤ \|A-B\|\|ğ°\|  < Ïƒ_{k+1} \|ğ°\|
$$

But for all $ğ± âˆˆ {\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$, that is, $ğ± = V[:,1:k+1]ğœ$ for some $ğœ âˆˆ â„‚^{k+1}$  we have 
$$
\|A ğ±\|^2 = \|U Î£_k ğœ\|^2 = \|Î£_k ğœ\|^2 =
\sum_{j=1}^{k+1} (Ïƒ_j c_j)^2 â‰¥ Ïƒ_{k+1}^2 \| ğœ  \|^2,
$$
i.e., $\|A ğ±\| â‰¥ Ïƒ_{k+1} \|ğ±\|$, where we use the fact that
$$
\|ğ±\|^2 = \|V[:,1:k+1]ğœ\|^2 = ğœ^â‹† V[:,1:k+1]^â‹† V[:,1:k+1]ğœ = ğœ^â‹†ğœ = \|ğœ\|^2.
$$
Thus $ğ°$ cannot be in this span.


The dimension of the span of $\ker(B)$ is at least $n-k$, but the dimension of ${\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$ is at least $k+1$.
Since these two spaces cannot intersect (apart from at 0) we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  âˆ



## Lab and Problem Sheet

In the lab we explore the relationship between smoothness of a function and the decay in the singular values.
We also implement low rank compression and see for samples of some smooth functions that we can massively reduce the data
needed to represent the matrix. We also look at the case of the Hilbert matrix
$$
H_n := \begin{bmatrix} 1 & 1/2 & 1/3 & â‹¯ & 1/n \\
                      1/2 & 1/3 & 1/4 & â‹¯ & 1/(n+1) \\
                       1/3 & 1/4 & 1/5 & â‹¯ & 1/(n+2) \\
                       â‹® & â‹® & â‹® & â‹± & â‹® \\
                       1/n & 1/(n+1) & 1/(n+2) & â‹¯ & 1/(2n-1)
                       \end{bmatrix},
$$
a famous matrix introduced by Hilbert which has a number of applications in approximation theory.


In the problem sheet we see that the SVD can be used to define the _Mooreâ€“Penrose pseudoinverse_, a way of 
constructing a notion if an inverse for a rectangular or non-invertible matrix that is consistent with least sqaures.