
We introduce Big-O, little-o and asymptotic notation and see how they can be used to describe computational cost. 

\section{Asymptotics as $n \ensuremath{\rightarrow} \ensuremath{\infty}$}
Big-O, little-o, and \ensuremath{\ldq}asymptotic to" are used to describe behaviour of functions at infinity. 

\begin{definition}[Big-O] 
\[
f(n) = O(\ensuremath{\phi}(n)) \qquad \hbox{(as $n \ensuremath{\rightarrow} \ensuremath{\infty}$)}
\]
means $\left|{f(n) \over \ensuremath{\phi}(n)}\right|$ is bounded for sufficiently large $n$. That is, there exist constants $C$ and $N_0$ such  that, for all $n \geq N_0$, $|{f(n) \over \ensuremath{\phi}(n)}| \leq C$. \end{definition}

\begin{definition}[little-O] 
\[
f(n) = o(\ensuremath{\phi}(n)) \qquad \hbox{(as $n \ensuremath{\rightarrow} \ensuremath{\infty}$)}
\]
means $\lim_{n \ensuremath{\rightarrow} \ensuremath{\infty}} {f(n) \over \ensuremath{\phi}(n)} = 0.$ \end{definition}

\begin{definition}[asymptotic to] 
\[
f(n) \ensuremath{\sim} \ensuremath{\phi}(n) \qquad \hbox{(as $n \ensuremath{\rightarrow} \ensuremath{\infty}$)}
\]
means $\lim_{n \ensuremath{\rightarrow} \ensuremath{\infty}} {f(n) \over \ensuremath{\phi}(n)} = 1.$ \end{definition}

\begin{example}[asymptotics with $n$]

\begin{itemize}
\item[1. ] \[
{\cos n \over n^2 -1} = O(n^{-2})
\]
as

\end{itemize}
\[
\left|{{\cos n \over n^2 -1} \over n^{-2}} \right| \leq \left| n^2 \over n^2 -1 \right|  \leq 2
\]
for $n \geq N_0 = 2$.

\begin{itemize}
\item[2. ] \[
\log n = o(n)
\]
as $\lim_{n \ensuremath{\rightarrow} \ensuremath{\infty}} {\log n \over n} = 0.$


\item[3. ] \[
n^2 + 1 \ensuremath{\sim} n^2
\]
as ${n^2 +1 \over n^2} \ensuremath{\rightarrow} 1.$

\end{itemize}
\end{example}

Note we sometimes write $f(O(\ensuremath{\phi}(n)))$ for a function of the form $f(g(n))$ such that $g(n) = O(\ensuremath{\phi}(n))$.

We have some simple algebraic rules:

\begin{proposition}[Big-O rules]
\begin{align*}
O(\ensuremath{\phi}(n))O(\ensuremath{\psi}(n)) = O(\ensuremath{\phi}(n)\ensuremath{\psi}(n))  \qquad \hbox{(as $n \ensuremath{\rightarrow} \ensuremath{\infty}$)} \\
O(\ensuremath{\phi}(n)) + O(\ensuremath{\psi}(n)) = O(|\ensuremath{\phi}(n)| + |\ensuremath{\psi}(n)|)  \qquad \hbox{(as $n \ensuremath{\rightarrow} \ensuremath{\infty}$)}.
\end{align*}
\end{proposition}
\textbf{Proof} See any standard book on asymptotics, eg \href{https://www.taylorfrancis.com/books/mono/10.1201/9781439864548/asymptotics-special-functions-frank-olver}{F.W.J. Olver, Asymptotics and Special Functions}. \ensuremath{\QED}

\section{Asymptotics as $x \ensuremath{\rightarrow} x_0$}
We also have Big-O, little-o and "asymptotic to" at a point:

\begin{definition}[Big-O] 
\[
f(x) = O(\ensuremath{\phi}(x)) \qquad \hbox{(as $x \ensuremath{\rightarrow} x_0$)}
\]
means $|{f(x) \over \ensuremath{\phi}(x)}|$ is bounded in a neighbourhood of $x_0$. That is, there exist constants $C$ and $r$ such  that, for all $0 \leq |x - x_0| \leq r$, $|{f(x) \over \ensuremath{\phi}(x)}| \leq C$. \end{definition}

\begin{definition}[little-O] 
\[
f(x) = o(\ensuremath{\phi}(x)) \qquad \hbox{(as $x \ensuremath{\rightarrow} x_0$)}
\]
means $\lim_{x \ensuremath{\rightarrow} x_0} {f(x) \over \ensuremath{\phi}(x)} = 0.$ \end{definition}

\begin{definition}[asymptotic to] 
\[
f(x) \ensuremath{\sim} \ensuremath{\phi}(x) \qquad \hbox{(as $x \ensuremath{\rightarrow} x_0$)}
\]
means $\lim_{x \ensuremath{\rightarrow} x_0} {f(x) \over \ensuremath{\phi}(x)} = 1.$ \end{definition}

\begin{example}[asymptotics with $x$]
\[
\exp x = 1 + x + O(x^2) \qquad \hbox{as $x \ensuremath{\rightarrow} 0$}
\]
since $\exp x = 1 + x + {\exp t \over 2} x^2$ for some $t \in [0,x]$ and
\[
\left|{{\exp t \over 2} x^2 \over x^2}\right| \leq {3 \over 2}
\]
provided $x \leq 1$. \end{example}

\section{Computational cost}
We will use Big-O notation to describe the computational cost of algorithms. Consider the following simple sum
\[
\sum_{k=1}^n x_k^2
\]
which we might implement as:


\begin{lstlisting}
(*@\HLJLk{function}@*) (*@\HLJLnf{sumsq}@*)(*@\HLJLp{(}@*)(*@\HLJLn{x}@*)(*@\HLJLp{)}@*)
    (*@\HLJLn{n}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{length}@*)(*@\HLJLp{(}@*)(*@\HLJLn{x}@*)(*@\HLJLp{)}@*)
    (*@\HLJLn{ret}@*) (*@\HLJLoB{=}@*) (*@\HLJLnfB{0.0}@*)
    (*@\HLJLk{for}@*) (*@\HLJLn{k}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLn{n}@*)
        (*@\HLJLn{ret}@*) (*@\HLJLoB{=}@*) (*@\HLJLn{ret}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{x}@*)(*@\HLJLp{[}@*)(*@\HLJLn{k}@*)(*@\HLJLp{]}@*)(*@\HLJLoB{{\textasciicircum}}@*)(*@\HLJLni{2}@*)
    (*@\HLJLk{end}@*)
    (*@\HLJLn{ret}@*)
(*@\HLJLk{end}@*)
\end{lstlisting}

\begin{lstlisting}
sumsq (generic function with 1 method)
\end{lstlisting}


Each step of this algorithm consists of one memory look-up (\texttt{z = x[k]}), one multiplication (\texttt{w = z*z}) and one addition (\texttt{ret = ret + w}). We will ignore the memory look-up in the following discussion. The number of CPU operations per step is therefore 2 (the addition and multiplication). Thus the total number of CPU operations is $2n$. But the constant $2$ here is misleading: we didn't count the memory look-up, thus it is more sensible to just talk about the asymptotic complexity, that is, the \emph{computational cost} is $O(n)$.

Now consider a double sum like:
\[
\sum_{k=1}^n \sum_{j=1}^k x_j^2
\]
which we might implement as:


\begin{lstlisting}
(*@\HLJLk{function}@*) (*@\HLJLnf{sumsq2}@*)(*@\HLJLp{(}@*)(*@\HLJLn{x}@*)(*@\HLJLp{)}@*)
    (*@\HLJLn{n}@*) (*@\HLJLoB{=}@*) (*@\HLJLnf{length}@*)(*@\HLJLp{(}@*)(*@\HLJLn{x}@*)(*@\HLJLp{)}@*)
    (*@\HLJLn{ret}@*) (*@\HLJLoB{=}@*) (*@\HLJLnfB{0.0}@*)
    (*@\HLJLk{for}@*) (*@\HLJLn{k}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLn{n}@*)
        (*@\HLJLk{for}@*) (*@\HLJLn{j}@*) (*@\HLJLoB{=}@*) (*@\HLJLni{1}@*)(*@\HLJLoB{:}@*)(*@\HLJLn{k}@*)
            (*@\HLJLn{ret}@*) (*@\HLJLoB{=}@*) (*@\HLJLn{ret}@*) (*@\HLJLoB{+}@*) (*@\HLJLn{x}@*)(*@\HLJLp{[}@*)(*@\HLJLn{j}@*)(*@\HLJLp{]}@*)(*@\HLJLoB{{\textasciicircum}}@*)(*@\HLJLni{2}@*)
        (*@\HLJLk{end}@*)
    (*@\HLJLk{end}@*)
    (*@\HLJLn{ret}@*)
(*@\HLJLk{end}@*)
\end{lstlisting}

\begin{lstlisting}
sumsq2 (generic function with 1 method)
\end{lstlisting}


Now the inner loop is $O(1)$ operations (we don't try to count the precise number), which we do $k$ times for $O(k)$ operations as $k \ensuremath{\rightarrow} \ensuremath{\infty}$. The outer loop therefore takes
\[
\ensuremath{\sum}_{k = 1}^n O(k) = O\left(\ensuremath{\sum}_{k = 1}^n k\right) = O\left( {n (n+1) \over 2} \right) = O(n^2)
\]
operations.



