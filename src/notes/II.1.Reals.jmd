# Reals


In this chapter, we introduce  the 
[IEEE Standard for Floating-Point Arithmetic](https://en.wikipedia.org/wiki/IEEE_754).
There are multiplies ways of representing real numbers on a computer, as well as 
the precise behaviour of operations such as addition, multiplication, etc. One can use

1. [Fixed-point arithmetic](https://en.wikipedia.org/wiki/Fixed-point_arithmetic): essentially representing a real number as an integer where a decimal point is inserted at a fixed position. This turns out to be impractical in most applications, e.g., due to loss of relative accuracy for small numbers.
2. [Floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic): essentially scientific notation where an exponent is stored alongside a fixed number of digits. This is what is used in practice.
3. [Level-index arithmetic](https://en.wikipedia.org/wiki/Symmetric_level-index_arithmetic): stores numbers as iterated exponents. This is the most beautiful mathematically but unfortunately is not as useful for most applications and is not implemented in hardware.

Before the 1980s each processor had potentially a different representation for 
floating-point numbers, as well as different behaviour for operations. 
IEEE introduced in 1985  standardised this across
processors so that algorithms would produce consistent and reliable results.

This chapter may seem very low level for a mathematics course but there are
two important reasons to understand the behaviour of floating-point numbers in details:
1. Floating-point arithmetic is precisely defined, and can even be used in rigorous computations as we shall see in the labs. But it is not exact and its important to understand how errors in computations can accumulate.
2. Failure to understand floating-point arithmetic can cause catastrophic issues in practice, with the extreme example being the  [explosion of the Ariane 5 rocket](https://youtu.be/N6PWATvLQCY?t=86).


## Real numbers in binary

We begin by describing how both integers and real numbers can be written in binary, that is, base-2.
In this case the digits are either 0 or 1, which matches how a computer stores data. 

Integers can be written in binary as follows:

**Definition (binary format)**
For $B_0,\ldots,B_p \in \{0,1\}$ denote an integer in _binary format_ by:
$$
±(B_p\ldots B_1B_0)_2 := ±\sum_{k=0}^p B_k 2^k
$$
∎


Reals can also be presented in binary format, that is, a sequence of `0`s and `1`s alongside a decimal point:

**Definition (real binary format)**
For $b_1,b_2,…\in \{0,1\}$, Denote a non-negative real number in _binary format_ by:
$$
(B_p …B_0.b_1b_2b_3…)_2 := (B_p …B_0)_2 +  \sum_{k=1}^∞ {b_k \over 2^k}.
$$
∎

**Example (rational in binary)**
Consider the number `1/3`.  In decimal recall that:
$$
1/3 = 0.3333…=  \sum_{k=1}^∞ {3 \over 10^k}
$$
We will see that in binary
$$
1/3 = (0.010101…)_2 = \sum_{k=1}^∞ {1 \over 2^{2k}}
$$
Both results can be proven using the geometric series:
$$
\sum_{k=0}^∞ z^k = {1 \over 1 - z}
$$
provided $|z| < 1$. That is, with $z = {1 \over 4}$ we verify the binary expansion:
$$
\sum_{k=1}^∞ {1 \over 4^k} = {1 \over 1 - 1/4} - 1 = {1 \over 3}
$$
A similar argument with $z = 1/10$ shows the decimal case.
∎


## Floating-point numbers

Floating-point numbers are a subset of real numbers that are representable using
a fixed number of bits.

**Definition (floating-point numbers)**
Given integers $σ$ (the _exponent shift_), $Q$ (the number of _exponent bits_) and 
$S$ (the _precision_), define the set of
_Floating-point numbers_ by as the union of _normal_, _sub-normal_, and _special_ floating
point numbers:
$$
F_{σ,Q,S} := F^{\rm normal}_{σ,Q,S} \cup F^{\rm sub}_{σ,Q,S} \cup F^{\rm special}.
$$
The _normal numbers_
$F^{\rm normal}_{σ,Q,S} ⊂ ℝ$ are
$$
F^{\rm normal}_{σ,Q,S} := \{\red{±} 2^{\green{q}-σ} × (1.\blue{b_1b_2b_3…b_S})_2 : 1 ≤ q < 2^Q-1 \}.
$$
The _sub-normal numbers_ $F^{\rm sub}_{σ,Q,S} ⊂ ℝ$ are
$$
F^{\rm sub}_{σ,Q,S} := \{\red{±} 2^{\green{1}-σ} × (0.\blue{b_1b_2b_3…b_S})_2\}.
$$
The _special numbers_ $F^{\rm special} ⊄ ℝ$ are 
$$
F^{\rm special} :=  \{∞, -∞, {\rm NaN}\}
$$
where ${\rm NaN}$ is a special symbol representing “not a number", essentially an error flag.
∎

Note this set of real numbers has no nice _algebraic structure_: it is not closed under addition, subtraction, etc.
On the other hand, we can control errors effectively hence it is extremely useful for analysis.

Floating-point numbers are stored in $1 + Q + S$ total number of bits, in the format
$$
\red{s}\ \green{q_{Q-1}…q_0}\ \blue{b_1…b_S}
$$
The first bit ($s$) is the _sign bit_: 0 means positive and 1 means
negative. The bits $q_{Q-1}…q_0$ are the _exponent bits_:
they are the binary digits of the unsigned integer $q$: 
$$
q = (\green{q_{Q-1}…q_0})_2.
$$
Finally, the bits $b_1…b_S$ are the _significand bits_.
If $1 ≤ q < 2^Q-1$ then the bits represent the normal number
$$
x = \red{±} 2^{\green{q}-σ} × (1.\blue{b_1b_2b_3…b_S})_2.
$$
If $q = 0$ (i.e. all bits are 0) then the bits represent the sub-normal number
$$
x = \red{±} 2^{\green{1}-σ} × (0.\blue{b_1b_2b_3…b_S})_2.
$$
If $q = 2^Q-1$  (i.e. all bits are 1) then the bits represent a special number.
If all sigificand bits are $0$ then it represents $±∞$. Otherwise if any significand bit is
$1$ then it represents ${\tt NaN}$.

**Remark** A common point of confusion is the difference between a _number_ whose digits are 0 and 1
but may have a sign (±) and a decimal point and a _sequence of bits_, which is how a number is stored in memory in a computer.



## IEEE floating-point numbers

**Definition (IEEE floating-point numbers)** 
IEEE has 3 standard floating-point formats: 16-bit (half precision), 32-bit (single precision) and
64-bit (double precision) defined by (you _do not_ need to memorise these):
$$
\begin{align*}
F_{16} &:= F_{15,5,10} \\
F_{32} &:= F_{127,8,23} \\
F_{64} &:= F_{1023,11,52}
\end{align*}
$$
∎

We now see a simple example of relating the bits of a floating point number with the number
it represents:

**Example (interpreting 16-bits as a float)** Consider the number with bits
$$
\red{0}\ \green{10000}\ \blue{1010000000}
$$
assuming it is a half-precision float ($F_{16}$). 
Since the sign bit is `0` it is positive. The exponent bits encode 
$$
q = (10000)_2 = 2^4
$$
hence the exponent is
$$
q - σ = 2^4 - 15 = 1
$$
and the number is:
$$
2^1 (1.1010000000)_2 = 2 (1 + 1/2 + 1/8) = 3+1/4 = 3.25.
$$
∎

**Example (rational to 16-bits)** How is the number $1/3$ stored in $F_{16}$?
Recall that
$$
1/3 = (0.010101…)_2 = 2^{-2} (1.0101…)_2 = 2^{13-15} (1.0101…)_2
$$
and since $13 = (1101)_2$  the exponent bits are `01101`.
For the significand we round the last bit to the nearest element of $F_{16}$, 
(the exact rule for rounding is explained in detail later), so we have
$$
1.010101010101010101010101…\approx 1.0101010101 \in F_{16} 
$$
and the significand bits are `0101010101`.
Thus the stored bits for $1/3$ are:
$$
\red{0}\ \green{01101}\ \blue{0101010101}
$$
∎


## Sub-normal and special numbers

For sub-normal numbers, the simplest example is zero, which has $q=0$ and all significand bits zero:
`0 00000 0000000000`.
Unlike integers, we also have a negative zero, which has bits:
`1 00000 0000000000`.
This is treated as identical to positive `0` (except for degenerate operations as explained in the lab).


**Example (subnormal in 16-bits)** Consider the number with bits
$$
\red{1}\ \green{00000}\ \blue{1100000000}
$$
assuming it is a half-precision float ($F_{16}$). 
Since all exponent bits are zero it is sub-normal. Since the sign bit is `1` it is negative. 
Hence this number is:
$$
-2^{1-σ} (0.1100000000)_2 = -2^{-14} (2^{-1} + 2^{-2}) = -3 × 2^{-16}
$$
∎

The special numbers extend the real line by adding $±∞$ but also a notion of ``not-a-number" ${\rm NaN}$.
Whenever the bits of $q$ of a floating-point number are all 1 then they represent an element of $F^{\rm special}$.
If all $b_k=0$, then the number represents either $±∞$.
All other special floating-point numbers represent ${\rm NaN}$. 

**Example (special in 16-bits)** The number with bits
$$
\red{1}\ \green{11111}\ \blue{0000000000}
$$
has all exponent bits equal to $1$, and significand bits $0$ and sign bit $1$, hence represents $-∞$. On the other hand,
the number with bits
$$
\red{1}\ \green{11111}\ \blue{0000000001}
$$
has all exponent bits equal to $1$ but does not have all significand bits equal to $0$, hence is one of many representations
for  ${\rm NaN}$.
∎

## Lab and problem sheet

In the lab we explore how integers and floating point numbers are stored in a computer
via different _type_s.  We begin with a description both signed and unsigned integers, whose mathematical behaviour are detailed in the
(non-examinable) appendix. We see how different sequences of bits can be reinterpreted as different numbers, and
explore using string manipulation to construct different types of numbers. We also see how integers can sometimes be output in hexadecimal (base-16) format,
which aligns better with the underlying binary storage. We then explore floating point numbers including construction 
by specifying the bits directly. We finally investigate some of the degenerate behaviour such as arithmetic with special numbers.
In the problem sheet we investigate some simple examples of representing numbers in floating point format.
