**Numerical Analysis MATH50003 (2024‚Äì25) Problem Sheet 7**

This problem sheet explores approximation of functions by polynomials using interpolation and regression via least squares.
Part of the nuance is understanding the relationship between Lagrange interpolation and interpolation via the Vandermonde matrices.
We also discuss the singular value decomposition, in particular, we explore how itr can be used to construct the _(Moore‚ÄìPenrose) pseudoinverse_:
a way of making sense of an ``inverse'' for rectangular or non-invertible matrices in a way that is consistent with least squares.


We begin with simple examples of constructing interpolants (using Lagrange interpolation) and linea regressions (using QR factorisations):


-----

**Problem 1(a)** Use Lagrange interpolation to
interpolate the function $\cos x$ by a polynomial at the points
$[0,2,3,4]$ and evaluate at $x = 1$.

**SOLUTION**

- $‚Ñì_1(x)=\frac{(x-2)(x-3)(x-4)}{(0-2)(0-3)(0-4)}=-\frac{1}{24}(x-2)(x-3)(x-4)$
- $‚Ñì_2(x)=\frac{(x-0)(x-3)(x-4)}{(2-0)(2-3)(2-4)}=\frac{1}{4}x(x-3)(x-4)$
- $‚Ñì_3(x)=\frac{(x-0)(x-2)(x-4)}{(3-0)(3-2)(3-4)}=-\frac{1}{3}x(x-2)(x-4)$
- $‚Ñì_4(x)=\frac{(x-0)(x-2)(x-3)}{(4-0)(4-2)(4-3)}=\frac{1}{8}x(x-2)(x-3)$
So that $p(x)=\cos(0)‚Ñì_1(x)+\cos(2)‚Ñì_2(x)+\cos(3)‚Ñì_3(x)+\cos(4)‚Ñì_4(x)$. Note that $‚Ñì_0(1)=1/4$, $‚Ñì_2(1)=3/2$, $‚Ñì_3(1)=-1$, $‚Ñì_4(1)=1/4$, so $p(1)=1/4\cos(0)+3/2\cos(2)-\cos(3)+1/4\cos(4)$.


**END**

**Problem 1(b)** Find a least squares fit to $\cos x$ by a polynomial of degree $1$ (linear regression) at the points $[0,2,3,4]$ using QR.
Hint: Choose the (non-default) sign so that $R[1,1]$ is positive to arrive at simpler computations. Since we are doing arithmetic by-hand without
rounding the benefits of the default sign no longer apply.

**SOLUTION**

The Vandermonde matrix is
$$
V = \begin{pmatrix}
1 & 0 \\ 
1 & 2 \\
1 & 3 \\
1 & 4
\end{pmatrix}
$$
We construct the Householder reflection using
$$
ùê± = \Vectt[1,1,1,1] ‚áí \| ùê± \| = 2 ‚áí ùê≤ := ùê± - \| ùê± \| ùêû_1 = \Vectt[-1,1,1,1] ‚áí \|ùê≤\| = 2 ‚áí ùê∞ := ùê≤/\|ùê≤\| = {1 \over 2}\Vectt[-1,1,1,1].
$$
Thus we have
$$
Q_1 V = \begin{bmatrix} 2 & 9/2 \\ & -5/2 \\ & -3/2 \\ & -1/2 \end{bmatrix}.
$$



**END**

-----


The Vandermonde matrix used in interpolation is a fundamental mathematical object, in particular, its determinant comes up
in differential equations, representation theory, and theoretical physics. The following problems investigate the relationship between
the LU factorisation of a (transposed) Vandermonde matrix and its determinant.

---


**Problem 2(a)** Compute the LU factorisation of the following transposed Vandermonde matrices:
$$
\begin{bmatrix}
1 & 1 \\
x & y
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 \\
x & y & z \\
x^2 & y^2 & z^2
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 & 1 \\
x & y & z & t \\
x^2 & y^2 & z^2 & t^2 \\
x^3 & y^3 & z^3 & t^3
\end{bmatrix}
$$
Can you spot a pattern? Test your conjecture with a $5 √ó 5$ Vandermonde matrix.

**SOLUTION**
(1)
$$
\begin{bmatrix}
1 & 1 \\
x & y
\end{bmatrix} =  \begin{bmatrix}
1 &  \\
x & 1
\end{bmatrix} \begin{bmatrix}
1 & 1 \\
 & y-x
\end{bmatrix}
$$

(2)
$$
V := \begin{bmatrix}
1 & 1 & 1 \\
x & y & z \\
x^2 & y^2 & z^2
\end{bmatrix} =  \begin{bmatrix}
1 &  \\
x & 1 \\
x^2 && 1
\end{bmatrix} \begin{bmatrix}
1 & 1 & 1\\
 & y-x & z-x \\
 & y^2-x^2 & z^2 - x^2
\end{bmatrix}
$$
We then have
$$
\begin{bmatrix}
 y-x & z-x \\
 y^2-x^2 & z^2 - x^2
\end{bmatrix} = \begin{bmatrix}
 1 &  \\
 y+x & 1
\end{bmatrix} \begin{bmatrix}
y-x & z-x \\
& (z-y)(z-x)
\end{bmatrix}
$$
since $z^2 - x^2 - (z-x) (y+x) = z^2 + xy  - zy = (z-y)(z-x)$. Thus we have
$$
V = \begin{bmatrix}
1 &  \\
x & 1 \\
x^2 & x+y& 1
\end{bmatrix}  \begin{bmatrix}
1 & 1 & 1\\
 & y-x & z-x \\
 &  &  (z-y)(z-x)
\end{bmatrix}
$$

(3)
$$
V := \begin{bmatrix}
1 & 1 & 1 & 1 \\
x & y & z & t \\
x^2 & y^2 & z^2 & t^2 \\
x^3 & y^3 & z^3 & t^3
\end{bmatrix} =
\begin{bmatrix}
1 &  \\
x & 1 \\
x^2 && 1 \\
x^3 &&& 1
\end{bmatrix} \begin{bmatrix}
1 & 1 & 1 & 1\\
 & y-x & z-x & t-x \\
 & y^2-x^2 & z^2 - x^2 & t^2 - x^2 \\
 & y^3-x^3 & z^3 - x^3 & t^3 - x^3
\end{bmatrix}
$$
We then have
$$
\meeq{
\begin{bmatrix}
y-x & z-x & t-x \\
y^2-x^2 & z^2 - x^2 & t^2 - x^2 \\
y^3-x^3 & z^3 - x^3 & t^3 - x^3
\end{bmatrix} = \begin{bmatrix}
1 &  &  \\
y + x & 1 &  \\
y^2 + xy + x^2 &  & 1
\end{bmatrix} \\
& \qquad √ó \begin{bmatrix}
y-x & z-x & t-x \\
 & (z-y)(z-x) & (t-y)(t-x) \\
 & (z-x)(z-y) (x+y+z) & (t-x)(t-y) (x+y+t)
\end{bmatrix}
}
$$
since
$$
z^3 - x^3 - (z-x) (y^2 + x y + x^2) = z^3 - z y^2 - x y z  - z x^2 + x y^2  + x^2 y
= (x-z)(y-z) (x+y+z).
$$
Finally we have
$$
\begin{align*}
&\begin{bmatrix}
 (z-y)(z-x) & (t-y)(t-x) \\
 (z-x)(z-y) (x+y+z) & (t-x)(t-y) (x+y+t)
\end{bmatrix}\\
&\qquad = \begin{bmatrix}
 1 & \\
 x+y+z & 1
\end{bmatrix}
 \begin{bmatrix}
 (z-y)(z-x) & (t-y)(t-x) \\
  & (t-x)(t-y) (t-z)
\end{bmatrix}
\end{align*}
$$
since
$$
(t-x)(t-y) (x+y+t) - (x+y+z) (t-y)(t-x) = (t-y)(t-x)(t-z).
$$
Putting everything together we have
$$
V = \begin{bmatrix}
1 &  \\
x & 1 \\
x^2 & x+y  & 1 \\
x^3 &y^2 + xy + x^2  & x+ y + z & 1
\end{bmatrix} \begin{bmatrix}
1 & 1 & 1 & 1\\
 & y-x & z-x & t-x \\
 &  & (z-y)(z-x) & (t-y)(t-x) \\
 &  &  & (t-y)(t-x)(t-z)
\end{bmatrix}
$$


We conjecture that $L[k,j]$ for $k > j$ contains a sum of all monomials of degree $k$ of $x_1,‚Ä¶,x_j$, and
$$
U[k,j] = ‚àè_{s = 1}^{k-1} (x_j-x_s)
$$
for $1 < k ‚â§ j$. We can confirm that
$$
\begin{align*}
&\begin{bmatrix}
1 & 1 & 1 & 1 & 1 \\
x & y & z & t & s\\
x^2 & y^2 & z^2 & t^2 & s^2 \\
x^3 & y^3 & z^3 & t^3 & s^3 \\
x^4 & y^4 & z^4 & t^4 & s^4
\end{bmatrix} \\
&\qquad =
\begin{bmatrix}
1 &  \\
x & 1 \\
x^2 & x+y  & 1 \\
x^3 &y^2 + xy + x^2  & x+ y + z & 1 \\
x^4 & x^3 + x^2 y + x y^2 + y^3 & x^2 + y^2 + z^2 + xy + xz + yz  & x + y + z + t & 1
\end{bmatrix} \\
&\qquad √ó
\begin{bmatrix}
1 & 1 & 1 & 1 & 1\\
 & y-x & z-x & t-x & s-x  \\
 &  & (z-y)(z-x) & (t-y)(t-x) & (s-x) (s-y) \\
 &  &  & (t-y)(t-x)(t-z) &   (s-y)(s-x)(s-z)  \\
 &  &  &  &   (s-y)(s-x)(s-z)(s-t)
\end{bmatrix}
\end{align*}
$$
Multiplying it out we confirm that our conjecture is correct in this case.

**END**

**Problem 2(b)** Use the LU factorisation to deduce the determinant of the above Vandermonde matrices. What
happens to the determinants when you swap two points (i.e., swapping $x$ and $y$)?

**SOLUTION**

The determinant can be deduced from an LU factorisation by multiplying the diagonal of $U$.
The  2 √ó 2 case gives $y-x$. The 3 √ó 3 case gives $(y-x) (z-x)(z-y)$. The 4 √ó 4 case gives $(y-x)(z-x)(z-y)(w-x)(w-y)(w-z)$.
In all cases swapping $x$ and $y$ (or any variables) flips the sign.

**END**

--------


The following problem is a simple example of an interpolatory quadrature rule, which can be constructed
using the Lagrange basis.


------


**Problem 3** Compute the interpolatory quadrature rule
$$
‚à´_{-1}^1 f(x) w(x) \dx ‚âà ‚àë_{j=1}^n w_j f(x_j)
$$
for the points $[x_1,x_2,x_3] = [-1,1/2,1]$, for the weights $w(x) = 1$ and $w(x) = \sqrt{1-x^2}$.

**SOLUTION**

‚Ä¢ $w(x) = 1$

‚Ä¢ $w(x) = \sqrt{1-x^2}$

For the points $ùê± = \{-1, 1/2, 1\}$ we have the Lagrange polynomials:
$$
‚Ñì_1(x) = \left(\frac{x - 1/2}{-1 - 1/2}\right)\cdot\left(\frac{x - 1}{-1 - 1}\right) = \frac{1}{3}\left(x^2 - \frac{3}{2}x + \frac{1}{2}\right),
$$
and
$$
‚Ñì_2(x) = -\frac{4}{3}x^2 + \frac{4}{3}, ‚Ñì_3(x) =x^2 + \frac{1}{2}x - \frac{1}{2},
$$
similarly. We can then compute the weights,
$$
w_j = \int_{-1}^1 ‚Ñì_j(x)w(x)dx,
$$
using,
$$
\int_{-1}^1 x^k \sqrt{1-x^2}dx = \begin{cases}
 \frac{œÄ}{2} &	k=0 \\
 0 & k=1 \\
\frac{œÄ}{8} & k=2
 \end{cases}
$$
to find,
$$
w_j = \begin{cases}
 	\frac{œÄ}{8} & j = 1 \\
 	\frac{œÄ}{2} & j = 2 \\
 	-\frac{œÄ}{8} & j = 3,
 \end{cases}
$$
so that the interpolatory quadrature rule is:
$$
Œ£_3^{w,ùê±}(f) = \frac{œÄ}{2}\left(\frac{1}{4}f(-1) + f(1/2) -\frac{1}{4}f(1) \right)
$$

**END**


-----

We now turn to the (real-valued) SVD $A = U Œ£ V^‚ä§ ‚àà ‚Ñù^{m √ó n}$, where $U ‚àà ‚Ñù^{m √ó r}$ and $V ‚àà ‚Ñù^{n √ó r}$ have orthonormal columns
and $Œ£$ is a diagonal matrix with non-increasing positive entries. We can use these to define a generalisation of the inverse of a matrix,
known as a  _pseudo-inverse_:
$$
A^+ := V Œ£^{-1} U^‚ä§.
$$
The pseudo-inverse has a number of properties similar to inverses:

-----

**Problem 4**  Show that $A^+$ satisfies the _Moore-Penrose conditions_: (1) $A A^+ A = A$,
(2) $A^+ A A^+ = A^+$, and (3) $(A A^+)^‚ä§ = A A^+$ and $(A^+ A)^‚ä§ = A^+ A$

**SOLUTION**

Let $A=UŒ£ V^‚ä§$ and $A^+ := V Œ£^{-1} U^‚ä§$, where $U ‚àà ‚Ñù^{m √ó r}$ and $V ‚àà ‚Ñù^{n √ó r}$. Note that $U^‚ä§U = I_m$ and $V^‚ä§V = I_r$. 

1. We have
$$
A A^+ A = U Œ£ V^‚ä§ V Œ£^{-1} U^‚ä§ U Œ£ V^‚ä§ = U Œ£ Œ£^{-1} Œ£ V^‚ä§ = UŒ£ V^‚ä§ = A
$$
2. Moreover,
$$
A^+ A A^+ = V Œ£^{-1}U^‚ä§ U Œ£ V^‚ä§ V Œ£^{-1} U^‚ä§ = V Œ£^{-1}Œ£ Œ£^{-1} U^‚ä§ = V Œ£^{-1} U^‚ä§ = A^+
$$
3. 
$$
\begin{align*}
(A A^+)^‚ä§ = (A^+)^‚ä§ A^‚ä§ = U Œ£^{-1} V^‚ä§ V Œ£ U^‚ä§ = U U^‚ä§ = U Œ£ V^‚ä§ V Œ£^{-1} U^‚ä§ = A A^+ \\
(A^+ A)^‚ä§ = A^‚ä§ (A^+)^‚ä§ =  V Œ£ U^‚ä§ U Œ£^{-1} V^‚ä§ = V V^‚ä§ = V Œ£^{-1} U^‚ä§ U Œ£ V^‚ä§  = A^+ A
\end{align*}
$$


**END**

----

An important feature of the pseudo-inverse is that it can be used to solve least squares problems.
That is, the SVD provides an alternative approach to QR for solving least squares, albeit one that
is computationally less efficient.
One advantage of the SVD for least squares is that it also allows for the case where the
matrix involved is not of full column rank.




----

**Problem 5(a)** Show for $A ‚àà ‚Ñù^{m √ó n}$ with $m ‚â• n$ and full rank
that $ùê± =  A^+ ùêõ$ is the least squares solution, i.e., minimises $\| A ùê± - ùêõ \|$.
Hint: extend $U$ in the SVD to be a square orthogonal matrix.

**SOLUTION**

The proof mimics that of the QR factorisation. Write $A =  U Œ£ V^‚ä§$ and let
$$
UÃÉ = \begin{bmatrix}U & K \end{bmatrix}
$$
so that $UÃÉ$ is orthogonal. We use the fact orthogonal matrices do not change norms:
$$
\begin{align*}
\|A ùê± - ùêõ \|^2 &= \|U Œ£ V^‚ä§ ùê± - ùêõ \|^2 = \|UÃÉ^‚ä§ U Œ£ V^‚ä§ ùê± - UÃÉ^‚ä§ ùêõ \|^2 = 
\|\underbrace{\begin{bmatrix}I_n \\ O \end{bmatrix}}_{‚àà ‚Ñù^{m √ó n}} Œ£ V^‚ä§ ùê± - \begin{bmatrix} U^‚ä§ \\ K^‚ä§ \end{bmatrix} ùêõ \|^2 \\
&= \|Œ£ V^‚ä§ ùê± - U^‚ä§ ùêõ \|^2 + \|K^‚ä§ ùêõ\|^2
\end{align*}
$$
The second term is independent of $ùê±$. The first term is minimised when zero:
$$
 \|Œ£ V^‚ä§ ùê± - U^‚ä§ ùêõ \| =\|Œ£ V^‚ä§ V Œ£^{-1} U^‚ä§ ùêõ  - U^‚ä§ ùêõ \| = 0
$$

**END**





**Problem 5(b)**
If $A ‚àà ‚Ñù^{m √ó n}$ has a non-empty kernel there are multiple solutions to the least
squares problem as 
we can add any element of the kernel. Show that $ùê± = A^+ ùêõ$ gives the least squares solution
such that $\| ùê± \|$ is minimised.

**SOLUTION**

Let $ùê±     =A^+b$ and let $ùê± + ùê§$ to be another solution i.e.
$$
\|Aùê± - b \| = \|A (ùê± +ùê§) - b \|
$$
Following the previous part we deduce:
$$
Œ£ V^‚ä§ (ùê± +ùê§) = U^‚ä§ ùêõ ‚áí V^‚ä§ ùê§ = 0
$$
As $ùê± = V ùêú$ lies in the span of the columns of $V$ we have
$ùê±^‚ä§ ùê§ = 0$. Thus
$$
\| ùê± + ùê§ \|^2 = \| ùê± \|^2 + \| ùê§ \|^2
$$
which is minimised when $ùê§ = 0$.

**END**


------
