# Dual Numbers

In this section we introduce a mathematically beautiful  alternative to divided differences for computing
derivatives: _dual numbers_. These are a commutative ring that _exactly_ compute derivatives,
which when implemented on a computer gives very high-accuracy approximations to derivatives. They
underpin forward-mode [automatic differentation](https://en.wikipedia.org/wiki/Automatic_differentiation).
Automatic differentiation  is a basic tool in Machine Learning for computing gradients necessary
for training neural networks.



**Definition (Dual numbers)** Dual numbers $ùîª$ are a commutative ring (over $‚Ñù$)
generated by $1$ and $œµ$ such that $œµ^2 = 0$, that is,
$$
ùîª := \{a + bœµ \quad :\quad
 a,b \in ‚Ñù, \quad œµ^2 =0 \}.
$$
‚àé

This is very much analoguous to complex numbers, which are a field generated by $1$ and $\I$ such that
$\I^2 = -1$, that is,
$$
‚ÑÇ := \{a + b\I \quad :\quad
 a,b \in ‚Ñù, \quad \I^2 =-1 \}.
$$
Compare multiplication of each number type which falls out of the rules of the generators:
$$
\meeq{
(a + b \I) (c + d \I) = ac + (bc + ad) \I + bd \I^2 = ac -bd + (bc + ad) \I, \ccr
(a + b œµ) (c + d œµ) = ac + (bc + ad) œµ + bd œµ^2 = ac  + (bc + ad) œµ.
}
$$
And just as we view $‚Ñù ‚äÇ ‚ÑÇ$ by equating $a ‚àà ‚Ñù$ with $a + 0\I ‚àà ‚ÑÇ$,
we can view $‚Ñù ‚äÇ ùîª$ by equating $a ‚àà ‚Ñù$ with $a + 0{\rm œµ} ‚àà ùîª$.

Conceptually, dual numbers can be thought of as introducing an infinitesimally small $œµ$, where
$œµ^2$ is so small it is treated as zero. This is the intuitive reason they allow for differentiation
of functions. But we do not need to appeal to this calculus-like interpretation,
instead, their construction and relationship to differentiation can be accomplished using purely
algebraic reasoning.

## Differentiating polynomials

Polynomials evaluated on dual numbers are well-defined as
they depend only on the operations $+$ and $*$. From the formula for multiplication of dual numbers
we deduce that evaluating a polynomial at a dual number $a + b œµ$ tells us the derivative of the polynomial at $a$:

**Theorem (polynomials on dual numbers)** Suppose $p$ is a polynomial. Then
$$
p(a + b œµ) = p(a) + b p'(a) œµ
$$

**Proof**

First consider $p(x) = x^n$ for $n ‚â• 0$. 
The cases $n = 0$ and $n = 1$ are immediate. For $n > 1$ we have by induction:
$$
(a + b œµ)^n = (a + b œµ) (a + b œµ)^{n-1} = (a + b œµ) (a^{n-1} + (n-1) b a^{n-2} œµ) = a^n + b n a^{n-1} œµ.
$$
For a more general polynomial
$$
p(x) = ‚àë_{k=0}^n c_k x^k
$$
the result follows from linearity:
$$
p(a + b Œµ) = ‚àë_{k=0}^n c_k (a+bœµ)^k = c_0 + ‚àë_{k=1}^n c_k (a^k +k b a^{k-1}œµ)
= ‚àë_{k=0}^n c_k a^k + b ‚àë_{k=1}^n c_k k a^{k-1}œµ = p(a) + b p'(a) œµ.
$$

‚àé

**Example (differentiating polynomial)** Consider computing $p'(2)$ where
$$
p(x) = (x-1)(x-2) + x^2.
$$
We can use dual numbers to differentiate, avoiding expanding in monomials
or applying rules of differentiating:
$$
p(2+œµ) = (1+œµ)œµ + (2+œµ)^2 = œµ + 4 + 4œµ = 4 + \underbrace{5}_{p'(2)}œµ.
$$
‚àé


## Differentiating other functions


We can extend real-valued differentiable functions to dual numbers in a similar manner.
First, consider a standard function with a Taylor series (e.g. ${\rm cos}$, ${\rm sin}$, ${\rm exp}$, etc.)
$$
f(x) = ‚àë_{k=0}^‚àû f_k x^k
$$
so that $a$ is inside the radius of convergence. This leads naturally to a definition on dual numbers:
$$
\meeq{
f(a + b œµ) = ‚àë_{k=0}^‚àû f_k (a + b œµ)^k = f_0 + ‚àë_{k=1}^‚àû f_k (a^k + k a^{k-1} b œµ) = ‚àë_{k=0}^‚àû f_k a^k +  ‚àë_{k=1}^‚àû f_k k a^{k-1} b œµ  \ccr
  = f(a) + b f'(a) œµ.
}
$$
More generally, given a differentiable function (which may not have a Taylor series) we can extend it to dual numbers:

**Definition (dual extension)** Suppose a real-valued function $f : Œ© ‚Üí ‚Ñù$ is differentiable in $Œ© ‚äÇ ‚Ñù$. 
We can construct the _dual extension_ $fÃ≤ : Œ© + œµ‚Ñù ‚Üí ùîª$ by defining
$$
fÃ≤(a + b œµ) := f(a) + b f'(a) œµ.
$$
By viewing $‚Ñù ‚äÇ ùîª$, it is natural to reuse the notation $f$ for the dual extension,
hence when there's no chance of confusion we will identify $f(a + b œµ) ‚â° fÃ≤(a+b œµ)$.
‚àé

Thus, for basic functions we have natural extensions:
$$
\begin{align*}
\exp(a + b œµ) &:= \exp(a) + b \exp(a) œµ & (a,b ‚àà ‚Ñù) \\
\sin(a + b œµ) &:= \sin(a) + b \cos(a) œµ & (a,b ‚àà ‚Ñù) \\
\cos(a + b œµ) &:= \cos(a) - b \sin(a) œµ & (a,b ‚àà ‚Ñù) \\
\log(a + b œµ) &:= \log(a) + {b \over a} œµ & (a ‚àà (0,‚àû), b ‚àà ‚Ñù) \\
\sqrt{a+b œµ} &:= \sqrt{a} + {b \over 2 \sqrt{a}} œµ & (a ‚àà (0,‚àû), b ‚àà ‚Ñù) \\
|a + b œµ| &:= |a| + b\, {\rm sign} a\, œµ & (a ‚àà ‚Ñù \backslash \{0\} , b ‚àà ‚Ñù)
\end{align*}
$$
provided the function is differentiable at $a$. Note the last example does not have
a convergent Taylor series (at $0$) but we can still extend it where it is differentiable.

Going further, we can add, multiply, and compose such dual-extensions. And the beauty is these automatically
satisfy the right properties to be dual-extensions themselves, thus allowing for differentiation of  complicated functions
built from basic differentiable building blocks.

The following lemma shows that addition and multiplication in some sense ‚Äúcommute" with the dual-extension,
hence we can recover the product rule from dual number multiplication:

**Lemma (addition/multiplication)** Suppose $f,g : Œ© ‚Üí ‚Ñù$ are differentiable for $Œ© ‚äÇ ‚Ñù$ and $c ‚àà ‚Ñù$. Then
for $a ‚àà Œ©$ and $b ‚àà ‚Ñù$ we have
$$
\meeq{
\underline{f+g}(a+bœµ) = fÃ≤(a+bœµ) + gÃ≤(a+bœµ) \ccr 
\underline{c f}(a+bœµ) = c fÃ≤(a+b œµ) \ccr
\underline{f g}(a+bœµ) = fÃ≤(a+b œµ) gÃ≤(a+b œµ)
}
$$

**Proof**
The first two are immediate due to linearity:
$$
\meeq{
\underline{(f+g)}(a+bœµ) = (f+g)(a) + b(f + g)'(a) œµ \ccr
 = (f(a)+bf'(a)œµ) + (g(a)+bg'(a)œµ) = fÃ≤(a+bœµ) + gÃ≤(a+bœµ), \ccr
\underline{cf}(a+bœµ) = (cf)(a) + b(cf)'(a) œµ = c(f(a)+bf'(a)œµ) = cfÃ≤(a+bœµ).
}
$$
The last property essentially captures the product rule of differentiation:
$$
\meeq{
\underline{f g}(a+bœµ) = f(a)g(a) + b (f(a)g'(a)+f'(a) g'(a))œµ  \ccr
= (f(a)+bf'(a)œµ)(g(a)+b g'(a)œµ) = fÃ≤(a+b œµ) gÃ≤(a+b œµ).
}
$$
‚àé

Furthermore composition recovers the chain rule:

**Lemma (composition)** 
Suppose $f : Œì ‚Üí ‚Ñù$ and $g : Œ© ‚Üí Œì$ are differentiable in $Œ©,Œì ‚äÇ ‚Ñù$. Then
$$
\underline{(f ‚àò g)}(a+b œµ) = fÃ≤(gÃ≤(a+bœµ))
$$

**Proof**
Again it falls out of the properties of dual numbers:
$$
\underline{(f ‚àò g)}(a+b œµ) = f(g(a)) + bg'(a) f'(g(a)) œµ = fÃ≤(g(a)+bg'(a)œµ) = fÃ≤(gÃ≤(a+bœµ))
$$
‚àé

A simple corollary is that any function defined in terms of addition, multiplication, composition, etc.
of basic functions with dual-extensions will be differentiable via dual numbers. In this following example we see a practical realisation
of this, where we differentiate a function by just evaluating it on dual numbers, implicitly, using the dual-extension
for the basic build blocks:

**Example (differentiating non-polynomial)**

Consider differentiating $f(x) =  \exp(x^2 + \cos x)$ at the point $a = 1$, where 
we automatically use the dual-extension of $\exp$ and $\cos$. We can differentiate $f$
by simply evaluating on the duals:
$$
f(1 + œµ) = \exp(1 + 2œµ + \cos 1 - \sin 1 œµ) =  \exp(1 + \cos 1) + \exp(1 + \cos 1) (2 - \sin 1) œµ.
$$
Therefore we deduce that
$$
f'(1) = \exp(1 + \cos 1) (2 - \sin 1).
$$


‚àé

## Lab and problem sheet

In the lab we explore how one can turn this mathematical idea into a practical 
implementation on a computer, giving a basic version of _forward-mode automatic differentiation_.
This is a concept that underpins machine learning, which uses _reverse-mode automatic differentiation_
to compute gradients when performing stochastic gradient descent.
 In order to implement dual numbers, we will introduce the concept of a
_type_: a data structure with fields.  For example, we will implement a type `Rat` for representing
rationals $p/q$, where the type has two fields (`p` and `q`). Basic arithmetic operations like `+` and `*` can be implemented
to correctly do rational arithmetic. 
We will then create a new type that can
 represent a dual number $a + b œµ$, where the the type has two fields (`a` and `b`). By implementing basic arithmetic operations
as well as more complicated functions like `exp` we can efficiently, and extremely accurately, compute derivatives of quite
general functions.

In the problem sheet, we explore how dual numbers can also be used for pen-and-paper calculations of derivatives. This gives an alternative
to traditional differentiation rules like chain and product rule, that while it is mathematically equivalent feels very different in practice. (I prefer it because it is much more algorithmic!)
Make sure when doing the problem sheet to only use dual numbers and not fall back to the more traditional rules. 
We also see that one can extend the concept to a 2D-analogue of dual numbers, which allows for computation of gradients.