

# Rectangular rule

One possible definition for an integral is the limit of a Riemann sum, for example:
$$
  ∫_a^b f(x) {\rm d}x = \lim_{n → ∞} h ∑_{j=1}^n f(x_j)
$$
where $x_j = a+jh$ are evenly spaced points dividing up the interval $[a,b]$, that
is  with the _step size_ $h = (b-a)/n$.
This suggests an algorithm known as the _(right-sided) rectangular rule_
for approximating an integral: choose $n$ large so that
$$
  ∫_a^b f(x) {\rm d}x ≈ h ∑_{j=1}^n f(x_j).
$$
We will show that
the error in approximation is bounded by $C/n$ for some constant $C$. This can be expressed using
“Big-O" notation:
$$
∫_a^b f(x) {\rm d}x = h ∑_{j=1}^n f(x_j) + O(1/n).
$$
In these notes we consider the “Analysis" part of “Numerical Analysis": we want to _prove_ the
convergence rate of the approximation, including finding an explicit expression for the constant $C$.

To tackle this question we consider the error incurred on a single panel $(x_{j-1},x_j)$, then sum up the errors on rectangles.

Now for a secret. There are only so many tools available in analysis (especially at this stage of your career), and
one can make a safe bet that the right tool in any analysis proof is either (1) integration-by-parts, (2) geometric series
or (3) Taylor series. In this case we use (1):

**Lemma ((Right-sided) Rectangular Rule error on one panel)**
Assuming $f$ is differentiable on $[a,b]$ and its derivative is integrable we have
$$
∫_a^b f(x) {\rm d}x = (b-a) f(b) + δ
$$
where $|δ| ≤ M (b-a)^2$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$.

**Proof**
We write
$$
\meeq{
∫_a^b f(x) {\rm d}x = ∫_a^b (x-a)' f(x)  {\rm d}x = [(x-a) f(x)]_a^b - ∫_a^b (x-a) f'(x) {\rm d} x \ccr
= (b-a) f(b) + \underbrace{\left(-∫_a^b (x-a) f'(x) {\rm d} x \right)}_δ.
}
$$
Recall that we can bound the absolute value of an integral by the supremum of the integrand
times the width of the integration interval:
$$
\abs{∫_a^b g(x) {\rm d} x} ≤ (b-a) \sup_{a ≤ x ≤ b}|g(x)|.
$$
The lemma thus follows since
$$
\begin{align*}
\abs{∫_a^b (x-a) f'(x) {\rm d} x} &≤ (b-a) \sup_{a ≤ x ≤ b}|(x-a) f'(x)|  \\
&≤ (b-a) \sup_{a ≤ x ≤ b}|x-a| \sup_{a ≤ x ≤ b}|f'(x)|\\
&≤ M (b-a)^2.
\end{align*}
$$
∎

Now summing up the errors in each panel gives us the error of using the Rectangular rule:

**Theorem (Rectangular Rule error)**
Assuming $f$ is differentiable on $[a,b]$ and its derivative is integrable we have
$$
∫_a^b f(x) {\rm d}x =  h ∑_{j=1}^n f(x_j) +  δ
$$
where $|δ| ≤ M (b-a) h$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$, $h = (b-a)/n$ and $x_j = a + jh$.

**Proof**
We split the integral into a sum of smaller integrals:
$$
∫_a^b f(x) {\rm d}x = ∑_{j=1}^n  ∫_{x_{j-1}}^{x_j} f(x) {\rm d}x =
∑_{j=1}^n  \br[(x_j - x_{j-1}) f(x_j) + δ_j] =  h ∑_{j=1}^n f(x_j) +  \underbrace{∑_{j=1}^n δ_j}_δ
$$
where $δ_j$, the error on each panel as in the preceding lemma, satisfies
$$
|δ_j| ≤ (x_j-x_{j-1})^2 \sup_{x_{j-1} ≤ x ≤ x_j}|f'(x)| ≤ M h^2.
$$
Thus using the triangular inequality we have
$$
|δ| = \abs{ ∑_{j=1}^n δ_j} ≤ ∑_{j=1}^n |δ_j| ≤ M n h^2 = M(b-a)h.
$$
∎

Note a consequence of this lemma is that the approximation converges as $n → ∞$ (i.e. $h → 0$). In the labs and problem
sheets we will consider the left-sided rule:
$$
∫_a^b f(x) {\rm d}x ≈  h ∑_{j=0}^{n-1} f(x_j).
$$
We also consider the _Trapezium rule_. Here we approximate an integral  by an affine function:
$$
∫_a^b f(x) {\rm d} x ≈ ∫_a^b {(b-x)f(a) + (x-a)f(b) \over b-a} \dx
= {b-a \over 2} \br[f(a) + f(b)].
$$
Subdividing an interval $a = x_0 < x_1 < … < x_n = b$ and applying this approximation separately on
each subinterval $[x_{j-1},x_j]$, where $h = (b-a)/n$ and $x_j = a + jh$, leads to the approximation
$$
∫_a^b f(x) {\rm d}x ≈  {h \over 2} f(a) + h ∑_{j=1}^{n-1} f(x_j) + {h \over 2} f(b)
$$
We shall see both experimentally and provably that this approximation converges faster than the
rectangular rule.






## Lab and problem sheet

In the lab, we explore the practical implementation of the right-sided rectangular rule and extensions to other rules like
the left-sided rectangular rule and trapezium rule. We also see how linear convergence ($O(h) = O(1/n)$) can be deduced _experimentally_: by applying an implementation of the rule to specific integrals with known formula
we can see the error rate visually by plotting it. In particular, we deduce that the Trapezium rule converges much faster to the true value of the integral than the other rules.  We also observe that in practice there are some subtleties: the error can bounce around at an extremely small, but not zero, level  on the order of $10^{-15}$.  In the problem sheet we explore the _analysis_ of these other rules, proving that the Trapezium rule converges to the
true integral at a faster quadratic ($O(h^2)$) error rate. This is a guarantee that the integral can be computed much more accurately for the same amount of work by taking into account the analysis, highlighting the important contribution of analysis in the construction of algorithms.