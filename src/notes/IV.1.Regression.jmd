# Polynomial Interpolation and Regression

_Polynomial interpolation_ is the process of finding a polynomial that equals data at a precise set of points. In this section we see how an interpolant can be constructed
by either solving a linear system involving the Vandermonde matrix, or directly in terms of the Lagrange basis for polynomials. We also investigate an application of polynomial
interpolation to computing integrals, giving an alternative to the rectangular and triangular rules from the first chapter. In the lab we see that this leads to much more accurate
computation. We also see in the lab that polynomial interpolation has issues, particular with an evenly spaced grid or with a monomial basis. Overcoming this will motivate orthogonal
polynomials later in the module.

A more robust scheme that overcomes some of the issues with naive polynomial interpolation is _polynomial regression_,
where we use more data than the degrees of freedom in the polynomial. We can determine such a polynomial by solving a _least squares peroblem_: instead of insisting that
the polynomial matches data exactly, we find the polynomial whose samples at the points are as close as possible to the data, as measured in the $2$-norm. 


## Polynomial interpolation

Our prelimary goal is given a set of points and data at those points, usually samples of a function
$f_j = f(x_j)$, find a polynomial that interpolates the data at the points:

**Definition (interpolatory polynomial)** Given _distinct_ points $𝐱 = \vectt[x_1,…,x_n] ∈ ℂ ^n$
and _data_ $𝐟 = \vectt[f_1,…,f_n] ∈ ℂ^n$, a degree $n-1$
_interpolatory polynomial_ $p(x)$ satisfies
$$
p(x_j) = f_j
$$
∎

The easiest way to solve this problem is to invert the Vandermonde system:

**Definition (Vandermonde)** The _Vandermonde matrix_ associated with $𝐱 ∈ ℂ^m$
is the matrix
$$
V_{𝐱,n} := \begin{bmatrix} 1 & x_1 & ⋯ & x_1^{n-1} \\
                    ⋮ & ⋮ & ⋱ & ⋮ \\
                    1 & x_m & ⋯ & x_m^{n-1}
                    \end{bmatrix} ∈ ℂ^{m × n}.
$$
When it is clear from context we omit the subscripts $𝐱,n$.
∎

Writing the coefficients of a polynomial
$$
p(x) = ∑_{k=0}^{n-1} c_k x^k
$$
as a vector
 $𝐜 = \vectt[c_0,…,c_{n-1}] ∈ ℂ^n$, we note that $V$ encodes the linear map from coefficients to values at a grid, that is,
$$
V𝐜 = \Vectt[c_0 + c_1 x_1 + ⋯ + c_{n-1} x_1^{n-1}, ⋮, c_0 + c_1 x_m + ⋯ + c_{n-1} x_m^{n-1}] = \Vectt[p(x_1),⋮,p(x_m)].
$$
In the square case (where $m=n$), the coefficients of an interpolatory polynomial are given by $𝐜 = V^{-1} 𝐟$, so that
$$
\Vectt[p(x_1),⋮,p(x_n)] = V 𝐜 = V V^{-1} 𝐟 = \Vectt[f_1,⋮,f_n].
$$
This inversion is justified by the following:

**Proposition (interpolatory polynomial uniqueness)**
Interpolatory polynomials are unique and therefore square Vandermonde matrices are invertible.

**Proof**
Suppose $p$ and $\pt$ are both interpolatory polynomials of the same function. Then $p(x) - \pt(x)$ vanishes at $n$ distinct points $x_j$. By the fundamental theorem of
algebra it must be zero, i.e., $p = \pt$.

For the second part, if $V 𝐜 = 0$ for $𝐜 = \vectt[c_0,…,c_{n-1}] ∈ ℂ^n$ then for $q(x) = c_0 + ⋯ + c_{n-1} x^{n-1}$ we have
$$
q(x_j) = 𝐞_j^⊤ V 𝐜 = 0
$$
hence $q$ vanishes at $n$ distinct points and is therefore 0, i.e., $𝐜 = 0$.

∎


We can invert square Vandermonde matrix numerically in $O(n^3)$ operations using the PLU factorisation.
But it turns out we can also construct the interpolatory polynomial directly, and evaluate the polynomial in only $O(n^2)$ operations.
We will use the following polynomials which equal $1$ at one grid point and zero at the others:

**Definition (Lagrange basis polynomial)** The _Lagrange basis polynomial_ is defined as
$$
ℓ_k(x) := ∏_{j ≠ k} {x-x_j \over x_k - x_j} =  {(x-x_1) ⋯(x-x_{k-1})(x-x_{k+1}) ⋯ (x-x_n) \over (x_k - x_1) ⋯ (x_k - x_{k-1}) (x_k - x_{k+1}) ⋯ (x_k - x_n)}
$$
∎

Plugging in the grid points verifies that $ℓ_k(x_j) = δ_{kj}$.

We can use the Lagrange basis to directly construct the interpolatory polynomial:

**Theorem (Lagrange interpolation)**
The unique interpolation polynomial is:
$$
p(x) = f_1 ℓ_1(x) + ⋯ + f_n ℓ_n(x)
$$

**Proof**
It follows from inspection:
$$
p(x_j) = ∑_{k=1}^n f_k ℓ_k(x_j) = f_j.
$$
∎

**Example (interpolating an exponential)** We can interpolate $\exp(x)$ at the points $0,1,2$.
That is, our data is $𝐟 = \vectt[1, {\rm e},{\rm e}^2]$ and the interpolatory polynomial is
$$
\begin{align*}
p(x) &= ℓ_1(x) + {\rm e} ℓ_2(x) + {\rm e}^2 ℓ_3(x) =
{(x - 1) (x-2) \over (-1)(-2)} + {\rm e} {x (x-2) \over (-1)} +
{\rm e}^2 {x (x-1) \over 2} \\
&= (1/2 - {\rm e} +{\rm e}^2/2)x^2 + (-3/2 + 2 {\rm e}  - {\rm e}^2 /2) x + 1
\end{align*}
$$
∎


**Remark** Interpolating at evenly spaced points is a really _bad_ idea as it  is inherently ill-conditioned.
The lab  explores this issue experimentally. Another
serious issue is that monomials are a horrible basis for interpolation. 
This is intuitive: when $n$ is large $x^n$ is basically zero near the origin and hence $x_j^n$ numerically lose linear independence, that is, on a computer they appear to be linearly dependent (up to rounding errors). 
Use alternative sets of points and bases entirely overcomes this issue.

## Interpolatory quadrature rules

Interpolation leads naturally to quadrature rules where one integrates the interpolatory
polynomial exactly. This can be viewed as an extension of one-panel Rectangular Rules
(which are degree 0 interpolants at a single point) and Trapezium Rules (which are degree 1 interpolants
at two points). 
Using the Lagrange basis for interpolation we can write general interpolatory quadrature rules as a simple
weighted sum:

**Definition (interpolatory quadrature rule)** Given a set of points $𝐱 = [x_1,…,x_n]^⊤$
the interpolatory quadrature rule is:
$$
Σ_n^{w,𝐱}[f] := ∑_{j=1}^n w_j f(x_j)
$$
where
$$
w_j := ∫_a^b ℓ_j(x) w(x) {\rm d} x.
$$
∎

The convergence of such a scheme is explored in the lab. But an important feature is that it is
exact for all low degree polynomials:

**Proposition (interpolatory quadrature is exact for polynomials)** 
Interpolatory quadrature is exact for all degree $n-1$ polynomials $p$:
$$
∫_a^b p(x) w(x) {\rm d}x = Σ_n^{w,𝐱}[p]
$$

**Proof**
The result follows since, by uniqueness of interpolatory polynomial, if $p$ is a polynomial then
$$
p(x) = ∑_{j=1}^n p(x_j) ℓ_j(x)
$$
Hence
$$
∫_a^b p(x) w(x) {\rm d}x = ∑_{j=1}^n p(x_j) \int_a^b ℓ_j(x) w(x) {\rm d}x = Σ_n^{w,𝐱}[p].
$$

∎

**Example (3-point interpolatory quadrature)** We find the interpolatory quadrature rule for $w(x) = 1$ on $[0,1]$ with  points $[x_1,x_2,x_3] = [0,1/4,1]$.
We have:
$$
\begin{align*}
w_1 = \int_0^1 w(x) ℓ_1(x) {\rm d}x  = \int_0^1 {(x-1/4)(x-1) \over (-1/4)(-1)}{\rm d}x = -1/6 \\
w_2 = \int_0^1 w(x) ℓ_2(x) {\rm d}x  = \int_0^1 {x(x-1) \over (1/4)(-3/4)}{\rm d}x = 8/9 \\
w_3 = \int_0^1 w(x) ℓ_3(x) {\rm d}x  = \int_0^1 {x(x-1/4) \over 3/4}{\rm d}x = 5/18
\end{align*}
$$
That is we have
$$
Σ_n^{w,𝐱}[f]  = -{f(0) \over 6} + {8f(1/4) \over 9} + {5 f(1) \over 18}.
$$
This is indeed exact for polynomials up to degree $2$ (and no more):
$$
Σ_n^{w,𝐱}[1] = 1, Σ_n^{w,𝐱}[x] = 1/2, Σ_n^{w,𝐱}[x^2] = 1/3, Σ_n^{w,𝐱}[x^3] = 7/24 ≠ 1/4.
$$
∎


## Polynomial regression



In many settings interpolation is not an accurate or appropriate tool. Data is often on an evenly spaced grid in which case (as seen in the lab)
interpolation breaks down catastrophically. Or the data is noisy and one ends up over resolving: approximating the
noise rather than the signal. A simple solution is _polynomial regression_: use more sample points  than the degrees of freedom in the polynomial. The special case of an affine polynomial is called _linear regression_.

More precisely, for $𝐱 ∈ ℂ^m$ and for $n < m$ we want to find a degree $n-1$ polynomial
$$
p(x) = ∑_{k=0}^{n-1} c_k x^k
$$
such that
$$
\Vectt[p(x_1), ⋮, p(x_m)] ≈ \underbrace{\Vectt[f_1,⋮,f_m]}_{𝐟}.
$$
Mapping between coefficients $𝐜 ∈ ℂ^n$ to polynomial values on a grid can be accomplished 
via rectangular Vandermonde matrices. In particular, our goal is to choose $𝐜 ∈ ℂ^n$ so that
$$
V 𝐜  = \Vectt[p(x_1), ⋮, p(x_m)] ≈ 𝐟.
$$
We do so by solving the _least squares_ system: given $V ∈ ℂ^{m × n}$ and $𝐟 ∈ ℂ^m$ we want to find $𝐜 ∈ ℂ^n$ such that
$$
\| V 𝐜 - 𝐟 \|
$$
is minimal. Note interpolation is a special case where this norm is precisely zero (which is indeed minimal), but in general this norm may be rather large.   We will discuss the numerical solution
of least squares problems in the next few sections.


**Remark** Using regression instead of interpolation can overcome the issues with evenly spaced grids. However, the monomial basis is still very problematic.