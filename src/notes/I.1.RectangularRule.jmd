

# Rectangular rule

One possible definition for an integral is the limit of a Riemann sum, for example:
$$
  ∫_a^b f(x) {\rm d}x = \lim_{n → ∞} h ∑_{j=1}^n f(x_j)
$$
where $x_j = a+jh$ are evenly spaced points dividing up the interval $[a,b]$, that
is  with the _step size_ $h = (b-a)/n$.
This suggests an algorithm known as the _(right-sided) rectangular rule_
for approximating an integral: choose $n$ large so that
$$
  ∫_a^b f(x) {\rm d}x ≈ h ∑_{j=1}^n f(x_j).
$$
In the lab we explore practical implementation of this approximation, and observe that
the error in approximation is bounded by $C/n$ for some constant $C$. This can be expressed using
“Big-O" notation:
$$
∫_a^b f(x) {\rm d}x = h ∑_{j=1}^n f(x_j) + O(1/n).
$$

In these notes we consider the “Analysis" part of “Numerical Analysis": we want to _prove_ the
convergence rate of the approximation, including finding an explicit expression for the constant $C$.

To tackle this question we consider the error incurred on a single panel $(x_{j-1},x_j)$, then sum up the errors on rectangles.

Now for a secret. There are only so many tools available in analysis (especially at this stage of your career), and
one can make a safe bet that the right tool in any analysis proof is either (1) integration-by-parts, (2) geometric series
or (3) Taylor series. In this case we use (1):

**Lemma ((Right-sided) Rectangular Rule error on one panel)**
Assuming $f$ is differentiable we have
$$
∫_a^b f(x) {\rm d}x = (b-a) f(b) + δ
$$
where $|δ| ≤ M (b-a)^2$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$.

**Proof**
We write
$$
\meeq{
∫_a^b f(x) {\rm d}x = ∫_a^b (x-a)' f(x)  {\rm d}x = [(x-a) f(x)]_a^b - ∫_a^b (x-a) f'(x) {\rm d} x \ccr
= (b-a) f(b) + \underbrace{\left(-∫_a^b (x-a) f'(x) {\rm d} x \right)}_δ.
}
$$
Recall that we can bound the absolute value of an integral by the supremum of the integrand
times the width of the integration interval:
$$
\abs{∫_a^b g(x) {\rm d} x} ≤ (b-a) \sup_{a ≤ x ≤ b}|g(x)|.
$$
The lemma thus follows since
$$
\begin{align*}
\abs{∫_a^b (x-a) f'(x) {\rm d} x} &≤ (b-a) \sup_{a ≤ x ≤ b}|(x-a) f'(x)|  \\
&≤ (b-a) \sup_{a ≤ x ≤ b}|x-a| \sup_{a ≤ x ≤ b}|f'(x)|\\
&≤ M (b-a)^2.
\end{align*}
$$
∎

Now summing up the errors in each panel gives us the error of using the Rectangular rule:

**Theorem (Rectangular Rule error)**
Assuming $f$ is differentiable we have
$$
∫_a^b f(x) {\rm d}x =  h ∑_{j=1}^n f(x_j) +  δ
$$
where $|δ| ≤ M (b-a) h$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$, $h = (b-a)/n$ and $x_j = a + jh$.

**Proof**
We split the integral into a sum of smaller integrals:
$$
∫_a^b f(x) {\rm d}x = ∑_{j=1}^n  ∫_{x_{j-1}}^{x_j} f(x) {\rm d}x =
∑_{j=1}^n  \br[(x_j - x_{j-1}) f(x_j) + δ_j] =  h ∑_{j=1}^n f(x_j) +  \underbrace{∑_{j=1}^n δ_j}_δ
$$
where $δ_j$, the error on each panel as in the preceding lemma, satisfies
$$
|δ_j| ≤ (x_j-x_{j-1})^2 \sup_{x_{j-1} ≤ x ≤ x_j}|f'(x)| ≤ M h^2.
$$
Thus using the triangular inequality we have
$$
|δ| = \abs{ ∑_{j=1}^n δ_j} ≤ ∑_{j=1}^n |δ_j| ≤ M n h^2 = M(b-a)h.
$$
∎

Note a consequence of this lemma is that the approximation converges as $n → ∞$ (i.e. $h → 0$). In the labs and problem
sheets we will consider the left-sided rule:
$$
∫_a^b f(x) {\rm d}x ≈  h ∑_{j=0}^{n-1} f(x_j).
$$
We also consider the _Trapezium rule_. Here we approximate an integral  by an affine function:
$$
∫_a^b f(x) {\rm d} x ≈ ∫_a^b {(b-x)f(a) + (x-a)f(b) \over b-a} \dx
= {b-a \over 2} \br[f(a) + f(b)].
$$
Subdividing an interval $a = x_0 < x_1 < … < x_n = b$ and applying this approximation separately on
each subinterval $[x_{j-1},x_j]$, where $h = (b-a)/n$ and $x_j = a + jh$, leads to the approximation
$$
∫_a^b f(x) {\rm d}x ≈  {h \over 2} f(a) + h ∑_{j=1}^{n-1} f(x_j) + {h \over 2} f(b)
$$
We shall see both experimentally and provably that this approximation converges faster than the
rectangular rule.






