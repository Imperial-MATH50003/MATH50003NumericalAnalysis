\documentclass[12pt,a4paper]{article}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}




\hypersetup
       {   pdfauthor = {  },
           pdftitle={  },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }




\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    upquote=true,
    breaklines=true,
    breakindent=0pt,
    keepspaces=true,
    showspaces=false,
    columns=fullflexible,
    showtabs=false,
    showstringspaces=false,
    escapeinside={(*@}{@*)},
    extendedchars=true,
}
\newcommand{\HLJLt}[1]{#1}
\newcommand{\HLJLw}[1]{#1}
\newcommand{\HLJLe}[1]{#1}
\newcommand{\HLJLeB}[1]{#1}
\newcommand{\HLJLo}[1]{#1}
\newcommand{\HLJLk}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkc}[1]{\textcolor[RGB]{59,151,46}{\textit{#1}}}
\newcommand{\HLJLkd}[1]{\textcolor[RGB]{214,102,97}{\textit{#1}}}
\newcommand{\HLJLkn}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkp}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkr}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkt}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLn}[1]{#1}
\newcommand{\HLJLna}[1]{#1}
\newcommand{\HLJLnb}[1]{#1}
\newcommand{\HLJLnbp}[1]{#1}
\newcommand{\HLJLnc}[1]{#1}
\newcommand{\HLJLncB}[1]{#1}
\newcommand{\HLJLnd}[1]{\textcolor[RGB]{214,102,97}{#1}}
\newcommand{\HLJLne}[1]{#1}
\newcommand{\HLJLneB}[1]{#1}
\newcommand{\HLJLnf}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnfm}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnp}[1]{#1}
\newcommand{\HLJLnl}[1]{#1}
\newcommand{\HLJLnn}[1]{#1}
\newcommand{\HLJLno}[1]{#1}
\newcommand{\HLJLnt}[1]{#1}
\newcommand{\HLJLnv}[1]{#1}
\newcommand{\HLJLnvc}[1]{#1}
\newcommand{\HLJLnvg}[1]{#1}
\newcommand{\HLJLnvi}[1]{#1}
\newcommand{\HLJLnvm}[1]{#1}
\newcommand{\HLJLl}[1]{#1}
\newcommand{\HLJLld}[1]{\textcolor[RGB]{148,91,176}{\textit{#1}}}
\newcommand{\HLJLs}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsa}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsb}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsc}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsd}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdC}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLse}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLsh}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsi}[1]{#1}
\newcommand{\HLJLso}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsr}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLss}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLssB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLnB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnbB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnfB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnh}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLni}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnil}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnoB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLoB}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLow}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLp}[1]{#1}
\newcommand{\HLJLc}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLch}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcm}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcp}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcpB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcs}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcsB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLg}[1]{#1}
\newcommand{\HLJLgd}[1]{#1}
\newcommand{\HLJLge}[1]{#1}
\newcommand{\HLJLgeB}[1]{#1}
\newcommand{\HLJLgh}[1]{#1}
\newcommand{\HLJLgi}[1]{#1}
\newcommand{\HLJLgo}[1]{#1}
\newcommand{\HLJLgp}[1]{#1}
\newcommand{\HLJLgs}[1]{#1}
\newcommand{\HLJLgsB}[1]{#1}
\newcommand{\HLJLgt}[1]{#1}


\def\endash{â€“}
\def\bbD{ {\mathbb D} }
\def\bbZ{ {\mathbb Z} }
\def\bbR{ {\mathbb R} }
\def\bbC{ {\mathbb C} }

\def\x{ {\vc x} }
\def\a{ {\vc a} }
\def\b{ {\vc b} }
\def\e{ {\vc e} }
\def\f{ {\vc f} }
\def\u{ {\vc u} }
\def\v{ {\vc v} }
\def\y{ {\vc y} }
\def\z{ {\vc z} }
\def\w{ {\vc w} }

\def\bt{ {\tilde b} }
\def\ct{ {\tilde c} }
\def\Ut{ {\tilde U} }
\def\Qt{ {\tilde Q} }
\def\Rt{ {\tilde R} }
\def\Xt{ {\tilde X} }
\def\acos{ {\rm acos}\, }

\def\red#1{ {\color{red} #1} }
\def\blue#1{ {\color{blue} #1} }
\def\green#1{ {\color{ForestGreen} #1} }
\def\magenta#1{ {\color{magenta} #1} }

\input{somacros}

\begin{document}



\textbf{Numerical Analysis MATH50003 (2025\ensuremath{\endash}26) Problem Sheet 3}

In this problem sheet we explore floating point numbers and bounding errors in arithmetic with rounding.  We begin with some simple examples of expressing real numbers in binary format and  representing them as floating point numbers:

\rule{\textwidth}{1pt}
\textbf{Problem 1} What is $\ensuremath{\pi}$ to 5 binary places? Hint: recall that $\ensuremath{\pi} \ensuremath{\approx} 3.14$.

\textbf{SOLUTION} We subtract off powers of two until we get 5 places. Eg we have
\[
\ensuremath{\pi} = 3.14\ensuremath{\ldots} = 2 + 1.14\ensuremath{\ldots} = 2 + 1 + 0.14\ensuremath{\ldots} = 2 + 1 + 1/8 + 0.016\ensuremath{\ldots} = 2 + 1 + 1/8  + 1/64  + 0.000\ensuremath{\ldots}
\]
Thus we have $\ensuremath{\pi} = (11.001001\ensuremath{\ldots})_2$. The question is slightly ambiguous whether we want to round to 5 digits so either \texttt{11.00100} or \texttt{11.00101} would be acceptable. \textbf{END}

\textbf{Problem 2} What are the single precision $F_{32} = F_{127,8,23}$ floating point representations for the following: 
\[
2, \quad 31, \quad 32, \quad 23/4, \quad (23/4)\times 2^{100}
\]
\textbf{SOLUTION} Recall that we have \texttt{\ensuremath{\sigma},Q,S = 127,8,23}. Thus we write
\[
2 = 2^{128-127} * (1.00000000000000000000000)_2
\]
The exponent bits are those of
\[
128 = 2^7 = (10000000)_2
\]
Hence we get the bits 

\begin{verbatim}
0 10000000 00000000000000000000000
\end{verbatim}
We write
\[
31 = (11111)_2 = 2^{131-127} * (1.1111)_2
\]
And note that $131 = (10000011)_2$ Hence we have the bits

\begin{verbatim}
0 10000011 11110000000000000000000
\end{verbatim}
On the other hand,
\[
32 = (100000)_2 = 2^{132-127}
\]
and $132 = (10000100)_2$ hence we have the bits 

\begin{verbatim}
0 10000100 00000000000000000000000
\end{verbatim}
Note that
\[
23/4 = 2^{-2} * (10111)_2 = 2^{129-127} * (1.0111)_2
\]
and $129 = (10000001)_2$ hence we get:

\begin{verbatim}
0 10000001 01110000000000000000000
\end{verbatim}
Finally,
\[
23/4 * 2^{100} = 2^{229-127} * (1.0111)_2
\]
and $229 = (11100101)_2$ giving us:

\begin{verbatim}
0 11100101 01110000000000000000000
\end{verbatim}
\textbf{END}

\rule{\textwidth}{1pt}
Floating point numbers cannot represent every real number. the next question explores the spacing between consecutive  floating point numbers:

\rule{\textwidth}{1pt}
\textbf{Problem 3} Let $m(y) = \min\{x \in F_{32} : x > y \}$ be the smallest single precision number greater than $y$. What is $m(2) - 2$ and $m(1024) - 1024$? 

\textbf{SOLUTION} The next float after $2$ is $2 * (1 + 2^{-23})$ hence we get $m(2) - 2 = 2^{-22}$:


\begin{lstlisting}
(*@\HLJLnf{nextfloat}@*)(*@\HLJLp{(}@*)(*@\HLJLnfB{2f0}@*)(*@\HLJLp{)}@*) (*@\HLJLoB{-}@*) (*@\HLJLni{2}@*)(*@\HLJLp{,}@*) (*@\HLJLni{2}@*)(*@\HLJLoB{{\textasciicircum}}@*)(*@\HLJLp{(}@*)(*@\HLJLoB{-}@*)(*@\HLJLni{22}@*)(*@\HLJLp{)}@*)
\end{lstlisting}

\begin{lstlisting}
(2.3841858f-7, 2.384185791015625e-7)
\end{lstlisting}


similarly, for $1024 = 2^{10}$ we find that the difference $m(1024) - 1024$ is $2^{10-23} = 2^{-13}$:


\begin{lstlisting}
(*@\HLJLnf{nextfloat}@*)(*@\HLJLp{(}@*)(*@\HLJLnfB{1024f0}@*)(*@\HLJLp{)}@*) (*@\HLJLoB{-}@*) (*@\HLJLni{1024}@*)(*@\HLJLp{,}@*) (*@\HLJLni{2}@*)(*@\HLJLoB{{\textasciicircum}}@*)(*@\HLJLp{(}@*)(*@\HLJLoB{-}@*)(*@\HLJLni{13}@*)(*@\HLJLp{)}@*)
\end{lstlisting}

\begin{lstlisting}
(0.00012207031f0, 0.0001220703125)
\end{lstlisting}


\textbf{END}

\rule{\textwidth}{1pt}
Not every calculation involving floating point numbers has errors: sometimes they are exact. The next questions explore cases where they are exact, and where we have to round the number to represent them as a floating point number:

\rule{\textwidth}{1pt}
\textbf{Problem 4} Suppose $x = 1.25$ and consider 16-bit floating point arithmetic ($F_{16}$). What is the error in approximating $x$ by the nearest float point number ${\rm fl}(x)$? What is the error in approximating $2x$, $x/2$, $x + 2$ and $x - 2$ by $2 \otimes x$, $x \oslash 2$, $x \ensuremath{\oplus} 2$ and $x \ominus 2$?

\textbf{SOLUTION} None of these computations have errors since they are all exactly representable as floating point numbers. \textbf{END}

\textbf{Problem 5} Show that $1/5 = 2^{-3} (1.1001100110011\ensuremath{\ldots})_2$. What are the exact bits for $1 \ensuremath{\oslash} 5$, $1 \ensuremath{\oslash} 5 \ensuremath{\oplus} 1$ computed using  half-precision arithmetic ($F_{16} := F_{15,5,10}$) (using default rounding)?

\textbf{SOLUTION}

For the first part we use Geometric series:
\meeq{
 2^{-3} (1.1001100110\magenta{011\ensuremath{\ldots}})_2 =  2^{-3} \left(\ensuremath{\sum}_{k=0}^\ensuremath{\infty} {1 \over 2^{4k}} +
{1 \over 2} \ensuremath{\sum}_{k=0}^\ensuremath{\infty} {1 \over 2^{4k}}\right) \ccr
= {3 \over 2^4} {1 \over 1-1/2^4} = {3 \over 2^4-1} = {1 \over 5}
}
Write $-3 = 12 - 15$ hence we have $q = 12 = (01100)_2$. Since $1/5$ is below the midpoint (the midpoint would have been the first magenta bit was 1 and all other bits are 0) we round down and hence have the bits:
\[
\red{0}\ \green{01100}\ \blue{1001100110}
\]
Adding $1$ we get:
\[
1 + 2^{-3} * (1.1001100110)_2 = (1.0011001100\magenta{11})_2 \ensuremath{\approx} (1.0011001101)_2
\]
Here we write the exponent as $0 = 15 - 15$ where $q = 15 = (01111)_2$. Thus we have the bits:
\[
\red{0}\ \green{01111}\ \blue{0011001101}
\]
\textbf{END}

\rule{\textwidth}{1pt}
Arithmetic with floating point numbers is exact up to rounding and the \emph{round bound} gives a way of precisely bounding the errors involved. To simplify the discussion we use \emph{idealised floating point numbers} $F_{\ensuremath{\infty},S}$,  which avoids technicalities of subnormal numbers. We first see how error bounds can be deduced for two very simple expressions:

\rule{\textwidth}{1pt}
\textbf{Problem 6} Prove the following bounds on the \emph{absolute error} of a floating point calculation in idealised floating-point arithmetic $F_{\ensuremath{\infty},S}$ (i.e., you may assume all operations involve normal floating point numbers):
\begin{align*}
({\rm fl}(1.1) \ensuremath{\otimes} {\rm fl}(1.2)) &\ensuremath{\oplus} {\rm fl}(1.3) = 2.62 + \ensuremath{\varepsilon}_1 \\
({\rm fl}(1.1) \ensuremath{\ominus} 1) & \ensuremath{\oslash} {\rm fl}(0.1) = 1 + \ensuremath{\varepsilon}_2
\end{align*}
such that $|\ensuremath{\varepsilon}_1| \ensuremath{\leq} 11 \ensuremath{\epsilon}_{\rm m}$ and $|\ensuremath{\varepsilon}_2| \ensuremath{\leq} 40 \ensuremath{\epsilon}_{\rm m}$, where $\ensuremath{\epsilon}_{\rm m}$ is machine epsilon.

\textbf{SOLUTION}

The first problem is very similar to what we saw in lecture. Write
\[
({\rm fl}(1.1)\ensuremath{\otimes} {\rm fl}(1.2)) \ensuremath{\oplus} {\rm fl}(1.3) = ( 1.1(1 + \ensuremath{\delta}_1)1.2(1+\ensuremath{\delta}_2)(1+\ensuremath{\delta}_3) + 1.3(1+\ensuremath{\delta}_4))(1+\ensuremath{\delta}_5)
\]
where we have $|\ensuremath{\delta}_1|,\ensuremath{\ldots},|\ensuremath{\delta}_5| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/2$. We first write
\[
1.1(1 + \ensuremath{\delta}_1)1.2(1+\ensuremath{\delta}_2)(1+\ensuremath{\delta}_3) = 1.32( 1+ \ensuremath{\varepsilon}_1)
\]
where, using the bounds:
\[
|\ensuremath{\delta}_1\ensuremath{\delta}_2|,|\ensuremath{\delta}_1\ensuremath{\delta}_3|,|\ensuremath{\delta}_2\ensuremath{\delta}_3| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/4, |\ensuremath{\delta}_1\ensuremath{\delta}_2\ensuremath{\delta}_3| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/8
\]
we find that
\[
|\ensuremath{\varepsilon}_1| \ensuremath{\leq} |\ensuremath{\delta}_1| + |\ensuremath{\delta}_2| + |\ensuremath{\delta}_3| + |\ensuremath{\delta}_1\ensuremath{\delta}_2| + |\ensuremath{\delta}_1\ensuremath{\delta}_3| + |\ensuremath{\delta}_2\ensuremath{\delta}_3|+ |\ensuremath{\delta}_1\ensuremath{\delta}_2\ensuremath{\delta}_3|
     \ensuremath{\leq} (3/2 + 3/4 + 1/8) \ensuremath{\leq} 5/2 \ensuremath{\epsilon}_{\rm m}
\]
Then we have
\[
1.32 (1 + \ensuremath{\varepsilon}_1) + 1.3 (1 + \ensuremath{\delta}_4) = 2.62 + \underbrace{1.32 \ensuremath{\varepsilon}_1 + 1.3\ensuremath{\delta}_4}_{\ensuremath{\varepsilon}_2}
\]
where
\[
|\ensuremath{\varepsilon}_2| \ensuremath{\leq} (15/4 + 3/4) \ensuremath{\epsilon}_{\rm m} \ensuremath{\leq} 5\ensuremath{\epsilon}_{\rm m}.
\]
Finally,
\[
(2.62 + \ensuremath{\varepsilon}_2)(1+\ensuremath{\delta}_5) = 2.62 + \underbrace{\ensuremath{\varepsilon}_2 + 2.62\ensuremath{\delta}_5 + \ensuremath{\varepsilon}_2 \ensuremath{\delta}_5}_{\ensuremath{\varepsilon}_3}
\]
where, using $|\ensuremath{\varepsilon}_2 \ensuremath{\delta}_5| \ensuremath{\leq} 3\ensuremath{\epsilon}_{\rm m}$ we get,
\[
|\ensuremath{\varepsilon}_3| \ensuremath{\leq} (5 + 3/2 + 3) \ensuremath{\epsilon}_{\rm m}  \ensuremath{\leq} 10\ensuremath{\epsilon}_{\rm m}.
\]
For the second part, we do:
\[
({\rm fl}(1.1) \ensuremath{\ominus} 1) \ensuremath{\oslash} {\rm fl}(0.1) = {(1.1 (1 + \ensuremath{\delta}_1) - 1)(1 + \ensuremath{\delta}_2) \over 0.1 (1 + \ensuremath{\delta}_3)} (1 + \ensuremath{\delta}_4)
\]
where we have $|\ensuremath{\delta}_1|,\ensuremath{\ldots},|\ensuremath{\delta}_4| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/2$. Write
\[
{1 \over 1 + \ensuremath{\delta}_3} = 1 + \ensuremath{\varepsilon}_1
\]
where, using that $|\ensuremath{\delta}_3| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/2 \ensuremath{\leq} 1/2$, we have
\[
|\ensuremath{\varepsilon}_1| \ensuremath{\leq} \left| {\ensuremath{\delta}_3 \over 1 + \ensuremath{\delta}_3} \right| \ensuremath{\leq}  {\ensuremath{\epsilon}_{\rm m} \over 2} {1 \over 1 - 1/2} \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}.
\]
Further write
\[
(1 + \ensuremath{\varepsilon}_1)(1 + \ensuremath{\delta}_4) = 1 + \ensuremath{\varepsilon}_2
\]
where
\[
|\ensuremath{\varepsilon}_2| \ensuremath{\leq} |\ensuremath{\varepsilon}_1| + |\ensuremath{\delta}_4| + |\ensuremath{\varepsilon}_1| |\ensuremath{\delta}_4| \ensuremath{\leq} (1 + 1/2 + 1/2) \ensuremath{\epsilon}_{\rm m} =   2\ensuremath{\epsilon}_{\rm m}.
\]
We also write
\[
{(1.1 (1 + \ensuremath{\delta}_1) - 1)(1 + \ensuremath{\delta}_2) \over 0.1} = 1 + \underbrace{11\ensuremath{\delta}_1 + \ensuremath{\delta}_2 + 11\ensuremath{\delta}_1\ensuremath{\delta}_2}_{\ensuremath{\varepsilon}_3}
\]
where
\[
|\ensuremath{\varepsilon}_3| \ensuremath{\leq} (11/2 + 1/2  + 11/4) \ensuremath{\leq} 9 \ensuremath{\epsilon}_{\rm m}
\]
Then we get
\[
({\rm fl}(1.1) \ensuremath{\ominus} 1) \ensuremath{\oslash} {\rm fl}(0.1) = (1 + \ensuremath{\varepsilon}_3) (1 + \ensuremath{\varepsilon}_2) =  1 + \underbrace{\ensuremath{\varepsilon}_3 + \ensuremath{\varepsilon}_2 + \ensuremath{\varepsilon}_2 \ensuremath{\varepsilon}_3}_{\ensuremath{\varepsilon}_4}
\]
and the error is bounded by:
\[
|\ensuremath{\varepsilon}_4| \ensuremath{\leq} (9 + 2 + 18) \ensuremath{\epsilon}_{\rm m} \ensuremath{\leq} 29 \ensuremath{\epsilon}_{\rm m}.
\]
\textbf{END}

\rule{\textwidth}{1pt}
In the lectures/notes we saw how the effect of rounding errors in right-sided divided differences could be bounded.  Here we extend this to central differences which yields a different heuristic for the optimal choice of $h$:

\rule{\textwidth}{1pt}
\textbf{Problem 7(a)} Assume that $f^{\rm FP} : F_{\ensuremath{\infty},S} \ensuremath{\rightarrow} F_{\ensuremath{\infty},S}$ satisfies $f^{\rm FP}(x) = f(x) + \ensuremath{\delta}_x$ where $|\ensuremath{\delta}_x| \ensuremath{\leq} c \ensuremath{\epsilon}_{\rm m}$ for all $x \ensuremath{\in} F_{\ensuremath{\infty},S}$. Show that
\[
{f^{\rm FP}(x+h) \ensuremath{\ominus} f^{\rm FP}(x-h) \over  2h} = f'(x) + \ensuremath{\varepsilon}
\]
where the (absolute) error is bounded by
\[
|\ensuremath{\varepsilon}| \ensuremath{\leq} {|f'(x)| \over 2} \ensuremath{\epsilon}_{\rm m} + {M \over 3} h^2 + {2 c \ensuremath{\epsilon}_{\rm m} \over h}.
\]
\textbf{SOLUTION}

In floating point we have
\begin{align*}
{f^{\rm FP}(x + h) \ensuremath{\ominus} f^{\rm FP}(x-h) \over 2h} &= {f(x + h) +  \ensuremath{\delta}_{x+h} - f(x-h) - \ensuremath{\delta}_{x-h} \over 2h} (1 + \ensuremath{\delta}_1) \\
&= {f(x+h) - f(x-h) \over 2h} (1 + \ensuremath{\delta}_1) + {\ensuremath{\delta}_{x+h}- \ensuremath{\delta}_{x-h} \over 2h} (1 + \ensuremath{\delta}_1)
\end{align*}
From PS1 Q4 we get the error term
\[
f'(x) = {f(x + h) - f(x-h) \over 2h} + \ensuremath{\delta}^{\rm T}
\]
where
\[
|\ensuremath{\delta}^{\rm T}| \ensuremath{\leq} Mh^2/6.
\]
Thus
\[
(f^{\rm FP}(x + h) \ensuremath{\ominus} f^{\rm FP}(x-h)) / (2h) =
f'(x) + \underbrace{f'(x) \ensuremath{\delta}_1 - \ensuremath{\delta}^{\rm T} (1 + \ensuremath{\delta}_1) + {\ensuremath{\delta}_{x+h}- \ensuremath{\delta}_{x-h} \over 2h} (1 + \ensuremath{\delta}_1)}_{\ensuremath{\varepsilon}}
\]
where
\[
|\ensuremath{\varepsilon}| \ensuremath{\leq} {|f'(x)| \over 2} \ensuremath{\epsilon}_{\rm m} + {M \over 3} h^2 + {2 c \ensuremath{\epsilon}_{\rm m} \over h}.
\]
\textbf{END}

\textbf{Problem 7(b)} Use the previous result to deduce, heuristically, an $\ensuremath{\alpha}$ such that choosing $h = C \ensuremath{\epsilon}_{\rm m}^\ensuremath{\alpha}$ will result in, roughly, optimal accuracy.

\textbf{SOLUTION}

We want to balance the errors
\[
{M \over 3} h^2 \approx {2 c \ensuremath{\epsilon}_{\rm m} \over h}
\]
I.e. we want $h^3 = C \ensuremath{\epsilon}_{\rm m}$, that is, $\ensuremath{\alpha} = 1/3$.

\textbf{END}

\rule{\textwidth}{1pt}
We can also bound the errors in expressions involving many floating point numbers, a property that is necessary for bounding the error in more complicated algorithms.  We first deduce a convenient function $E_{n,\ensuremath{\epsilon}}$ which will be used in the resulting error bounds:

\rule{\textwidth}{1pt}
\textbf{Problem 8(a)} Suppose $|\ensuremath{\epsilon}_k| \ensuremath{\leq} \ensuremath{\epsilon}$ and $n \ensuremath{\epsilon} < 1$. Show that $\ensuremath{\prod}_{k=1}^n (1+\ensuremath{\epsilon}_k) = 1+\ensuremath{\theta}_n$ for some constant $\ensuremath{\theta}_n$ satisfying
\[
|\ensuremath{\theta}_n| \ensuremath{\leq} \underbrace{n \ensuremath{\epsilon} \over 1-n\ensuremath{\epsilon}}_{E_{n,\ensuremath{\epsilon}}}.
\]
\textbf{SOLUTION}
\[
\ensuremath{\prod}_{k=1}^{n+1} (1+\ensuremath{\epsilon}_k) = \ensuremath{\prod}_{k=1}^n (1+\ensuremath{\epsilon}_k) (1+\ensuremath{\epsilon}_{n+1}) = (1+\ensuremath{\theta}_n)(1+\ensuremath{\epsilon}_{n+1}) = 1 + \underbrace{\ensuremath{\theta}_n + \ensuremath{\epsilon}_{n+1} + \ensuremath{\theta}_n\ensuremath{\epsilon}_{n+1}}_{\ensuremath{\theta}_{n+1}}
\]
where
\begin{align*}
|\ensuremath{\theta}_{n+1}| &\ensuremath{\leq} {n \ensuremath{\epsilon} \over 1-n\ensuremath{\epsilon}}(1+\ensuremath{\epsilon}) + \ensuremath{\epsilon} \\
&={n \ensuremath{\epsilon} + n \ensuremath{\epsilon}^2 \over 1-(n+1)\ensuremath{\epsilon}} \underbrace{{1-(n+1)\ensuremath{\epsilon} \over 1-n\ensuremath{\epsilon}}}_{\ensuremath{\leq} 1}  + {\ensuremath{\epsilon}-(n+1)\ensuremath{\epsilon}^2 \over 1-(n+1)\ensuremath{\epsilon}} \\
&\ensuremath{\leq} {(n+1)  -  \ensuremath{\epsilon} \over 1-(n+1)\ensuremath{\epsilon}} \ensuremath{\epsilon} \ensuremath{\leq} {(n+1) \ensuremath{\epsilon} \over 1-(n+1)\ensuremath{\epsilon}} = E_{n+1,\ensuremath{\epsilon}}.
\end{align*}
\textbf{END}

\rule{\textwidth}{1pt}
We finally are in a place to bound products and additions of $n$ floating point numbers. This lays the ground-work for understanding errors in simple algorithms such as in  linear algebra.

\rule{\textwidth}{1pt}
\textbf{Problem 8(b)} Show if $x_1,\ensuremath{\ldots},x_n \ensuremath{\in} F_{\ensuremath{\infty},S}$ then
\[
x_1 \ensuremath{\otimes} \ensuremath{\cdots} \ensuremath{\otimes} x_n = x_1 \ensuremath{\cdots} x_n (1 + \ensuremath{\theta}_{n-1})
\]
where $|\ensuremath{\theta}_n| \ensuremath{\leq} E_{n,\ensuremath{\epsilon}_{\rm m}/2}$, assuming $n \ensuremath{\epsilon}_{\rm m} < 2$.

\textbf{SOLUTION}

We can expand out:
\[
x_1 \ensuremath{\otimes} \ensuremath{\cdots} \ensuremath{\otimes} x_n = (\ensuremath{\cdots}((x_1 x_2)(1+\ensuremath{\delta}_1) x_3(1+\ensuremath{\delta}_2)\ensuremath{\cdots} x_n(1+\ensuremath{\delta}_{n-1})) = x_1 \ensuremath{\cdots} x_n (1+\ensuremath{\delta}_1) \ensuremath{\cdots} (1+\ensuremath{\delta}_{n-1})
\]
where $|\ensuremath{\delta}_k| \ensuremath{\leq} \ensuremath{\epsilon}_{\rm m}/2$. The result then follows from the previous result.

\textbf{END}

\textbf{Problem 8(c)} Show if $x_1,\ensuremath{\ldots},x_n \ensuremath{\in} F_{\ensuremath{\infty},S}$ then
\[
x_1 \ensuremath{\oplus} \ensuremath{\cdots} \ensuremath{\oplus} x_n = x_1 +  \ensuremath{\cdots} + x_n + \ensuremath{\sigma}_n
\]
where, for $M = \ensuremath{\Sigma}_{k=1}^n |x_k|$, $|\ensuremath{\sigma}_n| \ensuremath{\leq} M E_{n-1,\ensuremath{\epsilon}_{\rm m}/2},$ assuming $n \ensuremath{\epsilon}_{\rm m} < 2$.

\textbf{SOLUTION}

Using Problem 8(a) we write:
\meeq{
(\ensuremath{\cdots}((x_1 + x_2)(1+\ensuremath{\delta}_1) + x_3)(1+\ensuremath{\delta}_2)\ensuremath{\cdots}+x_n)(1+\ensuremath{\delta}_{n-1})
= x_1 \ensuremath{\prod}_{k=1}^{n-1} (1+\ensuremath{\delta}_k) +  \ensuremath{\sum}_{j=2}^n x_j \ensuremath{\prod}_{k=j-1}^{n-1} (1+\ensuremath{\delta}_j) \ccr
= x_1(1+\ensuremath{\theta}_{n-1}) + \ensuremath{\sum}_{j=2}^n x_j (1 + \ensuremath{\theta}_{n-j+1})
}
where we have for $j = 2,\ensuremath{\ldots},n$
\[
|\ensuremath{\theta}_{n-j+1}| \ensuremath{\leq} E_{n-j+1,\ensuremath{\epsilon}_{\rm m}/2} \ensuremath{\leq} E_{n-1,\ensuremath{\epsilon}_{\rm m}/2}.
\]
Thus we have 
\[
\ensuremath{\sum}_{j=1}^n x_j (1 + \ensuremath{\theta}_{n-j+1})= \ensuremath{\sum}_{j=1}^n x_j + \underbrace{\ensuremath{\sum}_{j=1}^n x_j \ensuremath{\theta}_{n-j+1}}_{\ensuremath{\sigma}_n}
\]
where
\[
|\ensuremath{\sigma}_n| \ensuremath{\leq}  \ensuremath{\sum}_{j=1}^n |x_j \ensuremath{\theta}_{n-j+1}| \ensuremath{\leq} \sup_j |\ensuremath{\theta}_{n-j+1}| \ensuremath{\sum}_{j=1}^n |x_j|  \ensuremath{\leq} \| \ensuremath{\bm{\x}}\|_1 E_{n-1,\ensuremath{\epsilon}_{\rm m}/2}.
\]
\textbf{END}

\rule{\textwidth}{1pt}


\end{document}