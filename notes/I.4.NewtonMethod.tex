
\section{Newton's method}
In school you may recall learning Newton's method: a way of approximating zeros/roots to a function by using a local approximation by an affine function. That is, approximate a function $f(x)$ locally around an initial guess $x_0$ by its first order Taylor series:
\[
f(x) \ensuremath{\approx} f(x_0) + f'(x_0) (x-x_0)
\]
and then find the root of the right-hand side which is
\[
 f(x_0) + f'(x_0) (x-x_0) = 0 \ensuremath{\Leftrightarrow} x = x_0 - {f(x_0) \over f'(x_0)}.
\]
We can then repeat using this root as the new initial guess. In other words we have a sequence of \emph{hopefully} more accurate approximations:
\[
x_{k+1} = x_k - {f(x_k) \over f'(x_k)}.
\]
Thus \emph{if} we can compute derivatives, we can (sometimes) compute roots. The lab will explore using dual numbers to accomplish this task. This is in some sense a baby version of how Machine Learning algorithms train neural networks; but where Newton uses derivatives (or in higher-dimensions, gradients) to find roots of functions Machine Learning uses gradients to roughly minimise functions that represent the error between a neural network and training data.

In terms of analysis, we can guarantee convergence provided our initial guess is accurate enough. The first step is the bound the error of an iteration in terms of the previous error:

\begin{theorem}[Newton error] Suppose $f$ is twice-differentiable in a neighbourhood $B$ of $r$  such that $f(r) = 0$, and $f'$ does not vanish in $B$. Denote the error of the $k$-th Newton iteration as $\ensuremath{\varepsilon}_k := r - x_k$. If $x_k \ensuremath{\in} B$ then
\[
|\ensuremath{\varepsilon}_{k+1}| \ensuremath{\leq} M |\ensuremath{\varepsilon}_k|^2
\]
where
\[
M := {1 \over 2} \sup_{x \ensuremath{\in} B}  | f''(x)|   \sup_{x \ensuremath{\in} B} \left| {1 \over f'(x)} \right|.
\]
\end{theorem}
\textbf{Proof} Using Taylor's theorem we find that
\[
0 = f(r) = f(x_k + \ensuremath{\varepsilon}_k) = f(x_k) + f'(x_k) \ensuremath{\varepsilon}_k + {f''(t) \over 2} \ensuremath{\varepsilon}_k^2.
\]
for some $t \ensuremath{\in} B$ between $r$ and $x_k$.  Rearranging this we get an expression for $f(x_k)$ that tells us that
\[
\ensuremath{\varepsilon}_{k+1} = r - \underbrace{x_{k+1}}_{x_k - f(x_k)/f'(x_k)} = \ensuremath{\varepsilon}_k +  {f(x_k) \over f'(x_k)} = -{f''(t) \over 2f'(x_k)} \ensuremath{\varepsilon}_k^2.
\]
Taking absolute values of each side gives the result.

\ensuremath{\QED}

Hidden in this result is a guarantee of convergence provided $x_0$ is sufficiently close to $r$.

\begin{corollary}[Newton convergence] If $x_0 \ensuremath{\in} B$ is sufficiently close to $r$ then $x_k \ensuremath{\rightarrow} r$.

\end{corollary}
\textbf{Proof}

Suppose $x_k \ensuremath{\in} B$ satisfies $|\ensuremath{\varepsilon}_k| = |r-x_k| \ensuremath{\leq} M^{-1}$. Then
\[
|\ensuremath{\varepsilon}_{k+1}| \ensuremath{\leq} M |\ensuremath{\varepsilon}_k|^2 \ensuremath{\leq} |\ensuremath{\varepsilon}_k|,
\]
hence $x_{k+1} \ensuremath{\in} B$. Thus from induction if $x_0$ satisfies the condition $|\ensuremath{\varepsilon}_0| < M^{-1}$ condition then $x_k \ensuremath{\in} B$ for all $k$ and satisfies $|\ensuremath{\varepsilon}_k| \ensuremath{\leq} M^{-1}$.  Thus we find
\[
|\ensuremath{\varepsilon}_k| \ensuremath{\leq} M^k |\ensuremath{\varepsilon}_0|^{2k}.
\]
Provided $x_0$ also satisfies $|\ensuremath{\varepsilon}_0| < M^{-1/2}$ this will go to zero as $k \ensuremath{\rightarrow} \ensuremath{\infty}$.

\ensuremath{\QED}



