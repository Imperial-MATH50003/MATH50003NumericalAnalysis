
\section{Orthogonal and Unitary Matrices}
To solve least squares problems, we will  factorise a matrix $A$ as $A = QR$ where $R$ is a \emph{right-triangular matrix} and $Q$ is either an \emph{orthogonal} or \emph{unitary} matrix.

\begin{definition}[orthogonal/unitary matrix] A square real matrix is \emph{orthogonal} if its inverse is its transpose:
\[
O(n) = \{Q \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n} : Q^\ensuremath{\top}Q = I \}
\]
A square complex matrix is \emph{unitary} if its inverse is its adjoint:
\[
U(n) = \{Q \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n} : Q^\ensuremath{\star}Q = I \}.
\]
Here the adjoint is the same as the conjugate-transpose: $Q^\ensuremath{\star} := \bar Q^\ensuremath{\top}$.  \end{definition}

Note that $O(n) \ensuremath{\subset} U(n)$ as for real matrices $Q^\ensuremath{\star} = Q^\ensuremath{\top}$. Because in either case $Q^{-1} = Q^\ensuremath{\star}$ we also have $Q Q^\ensuremath{\star} = I$ (which for real matrices is $Q Q^\ensuremath{\top} = I$). These matrices are particularly important for numerical linear algebra for a number of reasons (we'll explore these properties in the problem sheets):

\begin{itemize}
\item[1. ] They are norm-preserving: for any vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$ and $Q \ensuremath{\in} U(n)$    we have $\|Q \ensuremath{\bm{\x}} \| = \| \ensuremath{\bm{\x}}\|$ where $\| \ensuremath{\bm{\x}} \|^2 := \ensuremath{\sum}_{k=1}^n x_k^2$ (i.e. the 2-norm).


\item[2. ] All eigenvalues have absolute value equal to $1$.


\item[3. ] For $Q \ensuremath{\in} O(n)$,  $\det Q = \ensuremath{\pm}1$.


\item[4. ] They are trivially invertible (just take the adjoint).


\item[5. ] They are generally \ensuremath{\ldq}stable": errors due to rounding when multiplying a vector by $Q$ are controlled.


\item[6. ] They are \emph{normal matrices}: they commute with their adjoint ($Q Q^\ensuremath{\star} = Q Q^\ensuremath{\star}$). 


\item[7. ] Both $O(n)$ and $U(n)$ are groups, in particular, they are closed under multiplication.

\end{itemize}
On a computer there are multiple ways of representing orthogonal/unitary matrices, and it is almost never to store a dense matrix, that is, we do not want to store all the entries. In the appendices we have seen permutation matrices, which are a special type of orthogonal matrices where we can store only the order the entries are permuted as a vector. 

More generally, we will use the group structure: represent general orthogonal/unitary matrices as products of simpler elements of the group. In partular we will use two building blocks:

\begin{itemize}
\item[1. ] \emph{Rotations}: Rotations are equivalent to special orthogonal matrices $SO(2)$  and correspond to rotations in 2D.


\item[2. ] \emph{Reflections}:  Reflections are elements of $U(n)$ that are defined in terms of a single unit vector $\ensuremath{\bm{\v}} \ensuremath{\in} \ensuremath{\bbC}^n$ which is reflected.

\end{itemize}
We remark a related concept to orthogonal/unitary matrices are rectangular matrices with orthonormal columns, e.g.
\[
U = [\ensuremath{\bm{\u}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\u}}_n] \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}
\]
where $m \ensuremath{\geq} n$ such that $U^\ensuremath{\star} U =  I_n$ (the $n \ensuremath{\times} n$ identity matrix). In this case we must have $UU^\ensuremath{\star} \ensuremath{\neq} I_m$ as the rank of $U$ is $n < m$. 

\subsection{Rotations}
We begin with a general definition:

\begin{definition}[Special Orthogonal and Rotations] \emph{Special Orthogonal Matrices} are
\[
SO(n) := \{Q \ensuremath{\in} O(n) | \det Q = 1 \}
\]
And (simple) \emph{rotations} are $SO(2)$. \end{definition}

In what follows we use the following for writing the angle of a vector:

\begin{definition}[two-arg arctan] The two-argument arctan function gives the angle \texttt{\ensuremath{\theta}} through the point $[a,b]^\ensuremath{\top}$, i.e., 
\[
\sqrt{a^2 + b^2} \begin{bmatrix} \cos \ensuremath{\theta} \\ \sin \ensuremath{\theta} \end{bmatrix} =  \begin{bmatrix} a \\ b \end{bmatrix}.
\]
It can be defined in terms of the standard arctan as follows:
\[
{\rm atan}(b,a) := \begin{cases} {\rm atan}{b \over a} & a > 0 \\
                            {\rm atan}{b \over a} + \ensuremath{\pi} & a < 0\hbox{ and }b >0 \\
                            {\rm atan}{b \over a} - \ensuremath{\pi} & a < 0\hbox{ and }b < 0 \\
                            \ensuremath{\pi}/2 & a = 0\hbox{ and }b >0 \\
                            -\ensuremath{\pi}/2 & a = 0\hbox{ and }b < 0 
                            \end{cases}
\]
\end{definition}

We show $SO(2)$ are exactly equivalent to standard rotations:

\begin{proposition}[simple rotation] A 2\ensuremath{\times}2 \emph{rotation matrix} through angle $\ensuremath{\theta}$ is
\[
Q_\ensuremath{\theta} := \begin{bmatrix} \cos \ensuremath{\theta} & -\sin \ensuremath{\theta} \cr \sin \ensuremath{\theta} & \cos \ensuremath{\theta} \end{bmatrix}.
\]
We have $Q \ensuremath{\in} SO(2)$ if and only if $Q = Q_\ensuremath{\theta}$ for some $\ensuremath{\theta} \ensuremath{\in} \ensuremath{\bbR}$.

\end{proposition}
\textbf{Proof}

We will write $c = \cos \ensuremath{\theta}$ and $s = \sin \ensuremath{\theta}$. Then we have
\[
Q_\ensuremath{\theta}^\ensuremath{\top}Q_\ensuremath{\theta} = \begin{pmatrix} c & s \\ -s & c \end{pmatrix} \begin{pmatrix} c & -s \\ s & c \end{pmatrix} = 
\begin{pmatrix} c^2 + s^2 & 0 \\ 0 & c^2 + s^2 \end{pmatrix} = I
\]
and $\det Q_\ensuremath{\theta} = c^2 + s^2 = 1$ hence $Q_\ensuremath{\theta} \ensuremath{\in} SO(2)$. 

Now suppose $Q = [\ensuremath{\bm{\q}}_1, \ensuremath{\bm{\q}}_2] \ensuremath{\in} SO(2)$ where we know its columns have norm 1, i.e. $\|\ensuremath{\bm{\q}}_k\| = 1$, and are orthogonal. Write $\ensuremath{\bm{\q}}_1 = [c,s]$ where we know $c = \cos \ensuremath{\theta}$ and $s = \sin \ensuremath{\theta}$ for $\ensuremath{\theta} = {\rm atan}(s, c)$.  Since $\ensuremath{\bm{\q}}_1\cdot \ensuremath{\bm{\q}}_2 = 0$ we can deduce $\ensuremath{\bm{\q}}_2 = \ensuremath{\pm} [-s,c]$. The sign is positive as $\det Q = \ensuremath{\pm}(c^2 + s^2) = \ensuremath{\pm}1$.

\ensuremath{\QED}

We can rotate an arbitrary vector in $\ensuremath{\bbR}^2$ to the unit axis using rotations, which are useful in linear algebra decompositions. Interestingly it only requires basic algebraic functions (no trigonometric functions):

\begin{proposition}[rotation of a vector]  The matrix
\[
Q = {1 \over \sqrt{a^2 + b^2}}
\begin{bmatrix}
 a & b \cr -b & a
\end{bmatrix}
\]
is a rotation matrix ($Q \ensuremath{\in} SO(2)$) satisfying
\[
Q \begin{bmatrix} a \\ b \end{bmatrix} = \sqrt{a^2 + b^2} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\]
\end{proposition}
\textbf{Proof} 

The last equation is trivial so the only question is that it is a rotation matrix. This follows immediately:
\[
Q^\ensuremath{\top} Q = {1 \over a^2 + b^2}  \begin{bmatrix}
 a^2 + b^2 & 0 \cr 0 & a^2 + b^2
\end{bmatrix} = I
\]
and $\det Q = 1$.

\ensuremath{\QED}

\begin{example}[rotating a vector] Consider the vector
\[
\ensuremath{\bm{\x}} = \Vectt[-1,-\sqrt{3}].
\]
We can use the proposition above to deduce the rotation matrix that rotates this vector to the positive real axis is:
\[
{1 \over \sqrt{1+3}} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix} = 
{1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
\]
Alternatively, we could determine the matrix by computing the angle of the vector via:
\[
\ensuremath{\theta} =  {\rm atan}(-\sqrt{3}, -1) = {\rm atan}(\sqrt{3}) - \ensuremath{\pi} = -{2\ensuremath{\pi} \over 3}.
\]
We thus compute:
\[
Q_{-\ensuremath{\theta}} = \begin{bmatrix}
\cos(2\ensuremath{\pi}/3) & -\sin(2\ensuremath{\pi}/3) \\
\sin(2\ensuremath{\pi}/3) & \cos(2\ensuremath{\pi}/3)
\end{bmatrix} = {1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
\]
\end{example}

More generally, we can consider rotations that operate on two entries of a vector at a time. This will be explored in the problem sheet/lab.

\subsection{Reflections}
In addition to rotations, another type of orthogonal/unitary matrix are reflections. These are specified by a single vector which is reflected, with everything orthogonal to the vector left fixed. 

\begin{definition}[reflection matrix]  Given a unit vector $\ensuremath{\bm{\v}} \ensuremath{\in} \ensuremath{\bbC}^n$ (satisfying $\|\ensuremath{\bm{\v}}\|=1$), define the corresponding \emph{reflection matrix} as:
\[
Q_{\ensuremath{\bm{\v}}} := I - 2 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star}
\]
\end{definition}

These are indeed reflections in the direction of $\ensuremath{\bm{\v}}$. We can show this as follows:

\begin{proposition}[Householder properties] $Q_{\ensuremath{\bm{\v}}}$ satisfies:

\begin{itemize}
\item[1. ] Symmetry: $Q_{\ensuremath{\bm{\v}}} = Q_{\ensuremath{\bm{\v}}}^\ensuremath{\star}$


\item[2. ] Orthogonality: $Q_{\ensuremath{\bm{\v}}} \ensuremath{\in} U(n)$


\item[3. ] The vector $\ensuremath{\bm{\v}}$ is an eigenvector of $Q_{\ensuremath{\bm{\v}}}$ with eigenvalue $-1$


\item[4. ] For the dimension $n-1$ space $W := \{\ensuremath{\bm{\w}} : \ensuremath{\bm{\w}}^\ensuremath{\star} \ensuremath{\bm{\v}} = 0 \}$, all vectors $\ensuremath{\bm{\w}} \ensuremath{\in} W$ satisfy $Q_{\ensuremath{\bm{\v}}}\ensuremath{\bm{\w}} = \ensuremath{\bm{\w}}$.


\item[5. ] Not a rotation: $\det Q_{\ensuremath{\bm{\v}}} = -1$

\end{itemize}
\end{proposition}
\textbf{Proof}

Property 1 follows immediately. Property 2 follows from
\[
Q_{\ensuremath{\bm{\v}}}^\ensuremath{\star} Q_{\ensuremath{\bm{\v}}} = Q_{\ensuremath{\bm{\v}}}^2 = I - 4 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} + 4 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\star} = I.
\]
Property 3 follows since
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\v}} = \ensuremath{\bm{\v}} - 2\ensuremath{\bm{\v}} (\ensuremath{\bm{\v}}^\ensuremath{\star}\ensuremath{\bm{\v}}) = -\ensuremath{\bm{\v}}.
\]
Property 4 follows from:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\w}} = \ensuremath{\bm{\w}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\w}}^\ensuremath{\star} \ensuremath{\bm{\v}}) =  \ensuremath{\bm{\w}}
\]
Property 5 then follows: Property 4 tells us that $1$ is an eigenvalue with multiplicity $n-1$. Since $-1$ is an eigenvalue with multiplicity 1,  the determinant, which is product of the eigenvalues, is $-1$.

\ensuremath{\QED}

\begin{example}[reflection through 2-vector] Consider reflection through $\ensuremath{\bm{\x}} = [1,2]^\ensuremath{\top}$.  We first need to normalise $\ensuremath{\bm{\x}}$:
\[
\ensuremath{\bm{\v}} = {\ensuremath{\bm{\x}} \over \|\ensuremath{\bm{\x}}\|} = \begin{bmatrix} {1 \over \sqrt{5}} \\ {2 \over \sqrt{5}} \end{bmatrix}
\]
The reflection matrix is:
\[
Q_{\ensuremath{\bm{\v}}} = I - 2 \ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\top} = \begin{bmatrix}1 \\ & 1 \end{bmatrix} - {2 \over 5} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}
 =  {1 \over 5} \begin{bmatrix} 3 & -4 \\ -4 & -3 \end{bmatrix}
\]
Indeed it is symmetric, and orthogonal. It sends $\ensuremath{\bm{\x}}$ to $-\ensuremath{\bm{\x}}$:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\x}} = {1 \over 5} \begin{bmatrix}3 - 8 \\ -4 - 6 \end{bmatrix} = -\ensuremath{\bm{\x}}
\]
Any vector orthogonal to $\ensuremath{\bm{\x}}$, like $\ensuremath{\bm{\y}} = [-2,1]^\ensuremath{\top}$, is left fixed:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\y}} = {1 \over 5} \begin{bmatrix}-6 -4 \\ 8 - 3 \end{bmatrix} = \ensuremath{\bm{\y}}
\]
\end{example}

Note that \emph{building} the matrix $Q_{\ensuremath{\bm{\v}}}$ will be expensive ($O(n^2)$ operations), but we can \emph{apply} $Q_{\ensuremath{\bm{\v}}}$ to a vector in $O(n)$ operations using the expression:
\[
Q_{\ensuremath{\bm{\v}}} \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\v}}^\ensuremath{\star} \ensuremath{\bm{\x}}) = \ensuremath{\bm{\x}} - 2 \ensuremath{\bm{\v}} (\ensuremath{\bm{\v}} \ensuremath{\cdot} \ensuremath{\bm{\x}}).
\]
\subsubsection{Householder reflections}
Just as rotations can be used to rotate vectors to be aligned with coordinate axis, so can reflections, but in this case it works for vectors in $\ensuremath{\bbC}^n$, not just $\ensuremath{\bbR}^2$. We begin with the real case:

\begin{definition}[Householder reflection, real case] For a given vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n$, define the Householder reflection
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} := Q_{\ensuremath{\bm{\w}}}
\]
for $\ensuremath{\bm{\y}} = \ensuremath{\mp} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}$ and $\ensuremath{\bm{\w}} = {\ensuremath{\bm{\y}} \over \|\ensuremath{\bm{\y}}\|}$. The default choice in sign is:
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} := Q_{\ensuremath{\bm{\x}}}^{-\hbox{sign}(x_1),\rm H}.
\]
\end{definition}

\begin{lemma}[Householder reflection maps to axis] For $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n$,
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} \ensuremath{\bm{\x}} = \ensuremath{\pm}\|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1
\]
\end{lemma}
\textbf{Proof} Note that
\begin{align*}
\| \ensuremath{\bm{\y}} \|^2 &= 2\|\ensuremath{\bm{\x}}\|^2 \ensuremath{\mp} 2 \|\ensuremath{\bm{\x}}\| x_1, \\
\ensuremath{\bm{\y}}^\ensuremath{\top} \ensuremath{\bm{\x}} &= \|\ensuremath{\bm{\x}}\|^2 \ensuremath{\mp}  \|\ensuremath{\bm{\x}}\| x_1
\end{align*}
where $x_1 = \ensuremath{\bm{\e}}_1^\ensuremath{\top} \ensuremath{\bm{\x}}$. Therefore:
\[
Q_{\ensuremath{\bm{\x}}}^{\ensuremath{\pm},\rm H} \ensuremath{\bm{\x}}  =  (I - 2 \ensuremath{\bm{\w}} \ensuremath{\bm{\w}}^\ensuremath{\top}) \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 {\ensuremath{\bm{\y}}  \|\ensuremath{\bm{\x}}\|  \over \|\ensuremath{\bm{\y}}\|^2} (\|\ensuremath{\bm{\x}}\|\ensuremath{\mp}x_1) = \ensuremath{\bm{\x}} - \ensuremath{\bm{\y}} =  \ensuremath{\pm}\|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1.
\]
\ensuremath{\QED}

\textbf{Remark} Why do we choose the the opposite sign of $x_1$ for the default reflection? For stability, but we won't discuss this in more detail.

We can extend this definition for complexes:

\begin{definition}[Householder reflection, complex case] For a given vector $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$, define the Householder reflection as
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} := Q_{\ensuremath{\bm{\w}}}
\]
for $\ensuremath{\bm{\y}} = {\rm csign}(x_1) \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}$ and $\ensuremath{\bm{\w}} = {\ensuremath{\bm{\y}} \over \|\ensuremath{\bm{\y}}\|}$, for ${\rm csign}(z) = {\rm e}^{{\rm i} \arg z}$.  \end{definition}

\begin{lemma}[Householder reflection maps to axis, complex case] For $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^n$,
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} \ensuremath{\bm{\x}} = -{\rm csign}(x_1) \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1
\]
\end{lemma}
\textbf{Proof} Denote $\ensuremath{\alpha} := {\rm csign}(x_1)$.  Note that $\baralpha x_1 = {\rm e}^{-{\rm i} \arg x_1} x_1 = |x_1|$.  Now we have
\begin{align*}
\| \ensuremath{\bm{\y}} \|^2 &= (\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}})^\ensuremath{\star}(\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}}) = |\ensuremath{\alpha}|\| \ensuremath{\bm{\x}} \|^2 + \| \ensuremath{\bm{\x}} \|  \ensuremath{\alpha} \bar x_1 + \baralpha x_1 \| \ensuremath{\bm{\x}} \| + \| \ensuremath{\bm{\x}} \|^2 \\
&= 2\| \ensuremath{\bm{\x}} \|^2 + 2|x_1| \| \ensuremath{\bm{\x}} \| \\
\ensuremath{\bm{\y}}^\ensuremath{\star} \ensuremath{\bm{\x}} &= \baralpha x_1 \| \ensuremath{\bm{\x}} \| + \|\ensuremath{\bm{\x}} \|^2 = \|\ensuremath{\bm{\x}} \|^2 + |x_1| \| \ensuremath{\bm{\x}} \|
\end{align*}
Therefore:
\[
Q_{\ensuremath{\bm{\x}}}^{\rm H} \ensuremath{\bm{\x}}  =  (I - 2 \ensuremath{\bm{\w}} \ensuremath{\bm{\w}}^\ensuremath{\star}) \ensuremath{\bm{\x}} = \ensuremath{\bm{\x}} - 2 {\ensuremath{\bm{\y}}    \over \|\ensuremath{\bm{\y}}\|^2} (\|\ensuremath{\bm{\x}} \|^2 + |x_1| \|\ensuremath{\bm{\x}} \|) = \ensuremath{\bm{\x}} - \ensuremath{\bm{\y}} =  -\ensuremath{\alpha} \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1.
\]
\ensuremath{\QED}



