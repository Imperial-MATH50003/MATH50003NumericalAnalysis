
\section{Reals}
In this chapter, we introduce  the  \href{https://en.wikipedia.org/wiki/IEEE_754}{IEEE Standard for Floating-Point Arithmetic}. There are multiplies ways of representing real numbers on a computer, as well as  the precise behaviour of operations such as addition, multiplication, etc. One can use

\begin{itemize}
\item[1. ] \href{https://en.wikipedia.org/wiki/Fixed-point_arithmetic}{Fixed-point arithmetic}: essentially representing a real number as an integer where a decimal point is inserted at a fixed position. This turns out to be impractical in most applications, e.g., due to loss of relative accuracy for small numbers.


\item[2. ] \href{https://en.wikipedia.org/wiki/Floating-point_arithmetic}{Floating-point arithmetic}: essentially scientific notation where an exponent is stored alongside a fixed number of digits. This is what is used in practice.


\item[3. ] \href{https://en.wikipedia.org/wiki/Symmetric_level-index_arithmetic}{Level-index arithmetic}: stores numbers as iterated exponents. This is the most beautiful mathematically but unfortunately is not as useful for most applications and is not implemented in hardware.

\end{itemize}
Before the 1980s each processor had potentially a different representation for  floating-point numbers, as well as different behaviour for operations.  IEEE introduced in 1985  standardised this across processors so that algorithms would produce consistent and reliable results.

This chapter may seem very low level for a mathematics course but there are two important reasons to understand the behaviour of floating-point numbers in details:

\begin{itemize}
\item[1. ] Floating-point arithmetic is precisely defined, and can even be used in rigorous computations as we shall see in the labs. But it is not exact and its important to understand how errors in computations can accumulate.


\item[2. ] Failure to understand floating-point arithmetic can cause catastrophic issues in practice, with the extreme example being the  \href{https://youtu.be/N6PWATvLQCY?t=86}{explosion of the Ariane 5 rocket}.

\end{itemize}
\subsection{Real numbers in binary}
We begin by describing how both integers and real numbers can be written in binary, that is, base-2. In this case the digits are either 0 or 1, which matches how a computer stores data. 

Integers can be written in binary as follows:

\begin{definition}[binary format] For $B_0,\ldots,B_p \in \{0,1\}$ denote an integer in \emph{binary format} by:
\[
\ensuremath{\pm}(B_p\ldots B_1B_0)_2 := \ensuremath{\pm}\sum_{k=0}^p B_k 2^k
\]
\end{definition}

Reals can also be presented in binary format, that is, a sequence of \texttt{0}s and \texttt{1}s alongside a decimal point:

\begin{definition}[real binary format] For $b_1,b_2,\ensuremath{\ldots}\in \{0,1\}$, Denote a non-negative real number in \emph{binary format} by:
\[
(B_p \ensuremath{\ldots}B_0.b_1b_2b_3\ensuremath{\ldots})_2 := (B_p \ensuremath{\ldots}B_0)_2 +  \sum_{k=1}^\ensuremath{\infty} {b_k \over 2^k}.
\]
\end{definition}

\begin{example}[rational in binary] Consider the number \texttt{1/3}.  In decimal recall that:
\[
1/3 = 0.3333\ensuremath{\ldots}=  \sum_{k=1}^\ensuremath{\infty} {3 \over 10^k}
\]
We will see that in binary
\[
1/3 = (0.010101\ensuremath{\ldots})_2 = \sum_{k=1}^\ensuremath{\infty} {1 \over 2^{2k}}
\]
Both results can be proven using the geometric series:
\[
\sum_{k=0}^\ensuremath{\infty} z^k = {1 \over 1 - z}
\]
provided $|z| < 1$. That is, with $z = {1 \over 4}$ we verify the binary expansion:
\[
\sum_{k=1}^\ensuremath{\infty} {1 \over 4^k} = {1 \over 1 - 1/4} - 1 = {1 \over 3}
\]
A similar argument with $z = 1/10$ shows the decimal case. \end{example}

\subsection{Floating-point numbers}
Floating-point numbers are a subset of real numbers that are representable using a fixed number of bits.

\begin{definition}[floating-point numbers] Given integers $\ensuremath{\sigma}$ (the \emph{exponent shift}), $Q$ (the number of \emph{exponent bits}) and  $S$ (the \emph{precision}), define the set of \emph{Floating-point numbers} by as the union of \emph{normal}, \emph{sub-normal}, and \emph{special} floating point numbers:
\[
F_{\ensuremath{\sigma},Q,S} := F^{\rm normal}_{\ensuremath{\sigma},Q,S} \cup F^{\rm sub}_{\ensuremath{\sigma},Q,S} \cup F^{\rm special}.
\]
The \emph{normal numbers} $F^{\rm normal}_{\ensuremath{\sigma},Q,S} \ensuremath{\subset} \ensuremath{\bbR}$ are
\[
F^{\rm normal}_{\ensuremath{\sigma},Q,S} := \{\red{\ensuremath{\pm}} 2^{\green{q}-\ensuremath{\sigma}} \ensuremath{\times} (1.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2 : 1 \ensuremath{\leq} q < 2^Q-1 \}.
\]
The \emph{sub-normal numbers} $F^{\rm sub}_{\ensuremath{\sigma},Q,S} \ensuremath{\subset} \ensuremath{\bbR}$ are
\[
F^{\rm sub}_{\ensuremath{\sigma},Q,S} := \{\red{\ensuremath{\pm}} 2^{\green{1}-\ensuremath{\sigma}} \ensuremath{\times} (0.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2\}.
\]
The \emph{special numbers} $F^{\rm special} \ensuremath{\nsubset} \ensuremath{\bbR}$ are 
\[
F^{\rm special} :=  \{\ensuremath{\infty}, -\ensuremath{\infty}, {\rm NaN}\}
\]
where ${\rm NaN}$ is a special symbol representing \ensuremath{\ldq}not a number", essentially an error flag. \end{definition}

Note this set of real numbers has no nice \emph{algebraic structure}: it is not closed under addition, subtraction, etc. On the other hand, we can control errors effectively hence it is extremely useful for analysis.

Floating-point numbers are stored in $1 + Q + S$ total number of bits, in the format
\[
\red{s}\ \green{q_{Q-1}\ensuremath{\ldots}q_0}\ \blue{b_1\ensuremath{\ldots}b_S}
\]
The first bit ($s$) is the \emph{sign bit}: 0 means positive and 1 means negative. The bits $q_{Q-1}\ensuremath{\ldots}q_0$ are the \emph{exponent bits}: they are the binary digits of the unsigned integer $q$: 
\[
q = (\green{q_{Q-1}\ensuremath{\ldots}q_0})_2.
\]
Finally, the bits $b_1\ensuremath{\ldots}b_S$ are the \emph{significand bits}. If $1 \ensuremath{\leq} q < 2^Q-1$ then the bits represent the normal number
\[
x = \red{\ensuremath{\pm}} 2^{\green{q}-\ensuremath{\sigma}} \ensuremath{\times} (1.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2.
\]
If $q = 0$ (i.e. all bits are 0) then the bits represent the sub-normal number
\[
x = \red{\ensuremath{\pm}} 2^{\green{1}-\ensuremath{\sigma}} \ensuremath{\times} (0.\blue{b_1b_2b_3\ensuremath{\ldots}b_S})_2.
\]
If $q = 2^Q-1$  (i.e. all bits are 1) then the bits represent a special number. If all sigificand bits are $0$ then it represents $\ensuremath{\pm}\ensuremath{\infty}$. Otherwise if any significand bit is $1$ then it represents ${\tt NaN}$.

\textbf{Remark} A common point of confusion is the difference between a \emph{number} whose digits are 0 and 1 but may have a sign (\ensuremath{\pm}) and a decimal point and a \emph{sequence of bits}, which is how a number is stored in memory in a computer.

\subsection{IEEE floating-point numbers}
\begin{definition}[IEEE floating-point numbers]  IEEE has 3 standard floating-point formats: 16-bit (half precision), 32-bit (single precision) and 64-bit (double precision) defined by (you \emph{do not} need to memorise these):
\begin{align*}
F_{16} &:= F_{15,5,10} \\
F_{32} &:= F_{127,8,23} \\
F_{64} &:= F_{1023,11,52}
\end{align*}
\end{definition}

We now see a simple example of relating the bits of a floating point number with the number it represents:

\begin{example}[interpreting 16-bits as a float] Consider the number with bits
\[
\red{0}\ \green{10000}\ \blue{1010000000}
\]
assuming it is a half-precision float ($F_{16}$).  Since the sign bit is \texttt{0} it is positive. The exponent bits encode 
\[
q = (10000)_2 = 2^4
\]
hence the exponent is
\[
q - \ensuremath{\sigma} = 2^4 - 15 = 1
\]
and the number is:
\[
2^1 (1.1010000000)_2 = 2 (1 + 1/2 + 1/8) = 3+1/4 = 3.25.
\]
\end{example}

\begin{example}[rational to 16-bits] How is the number $1/3$ stored in $F_{16}$? Recall that
\[
1/3 = (0.010101\ensuremath{\ldots})_2 = 2^{-2} (1.0101\ensuremath{\ldots})_2 = 2^{13-15} (1.0101\ensuremath{\ldots})_2
\]
and since $13 = (1101)_2$  the exponent bits are \texttt{01101}. For the significand we round the last bit to the nearest element of $F_{16}$,  (the exact rule for rounding is explained in detail later), so we have
\[
1.010101010101010101010101\ensuremath{\ldots}\approx 1.0101010101 \in F_{16} 
\]
and the significand bits are \texttt{0101010101}. Thus the stored bits for $1/3$ are:
\[
\red{0}\ \green{01101}\ \blue{0101010101}
\]
\end{example}

\subsection{Sub-normal and special numbers}
For sub-normal numbers, the simplest example is zero, which has $q=0$ and all significand bits zero: \texttt{0 00000 0000000000}. Unlike integers, we also have a negative zero, which has bits: \texttt{1 00000 0000000000}. This is treated as identical to positive \texttt{0} (except for degenerate operations as explained in the lab).

\begin{example}[subnormal in 16-bits] Consider the number with bits
\[
\red{1}\ \green{00000}\ \blue{1100000000}
\]
assuming it is a half-precision float ($F_{16}$).  Since all exponent bits are zero it is sub-normal. Since the sign bit is \texttt{1} it is negative.  Hence this number is:
\[
-2^{1-\ensuremath{\sigma}} (0.1100000000)_2 = -2^{-14} (2^{-1} + 2^{-2}) = -3 \ensuremath{\times} 2^{-16}
\]
\end{example}

The special numbers extend the real line by adding $\ensuremath{\pm}\ensuremath{\infty}$ but also a notion of ``not-a-number" ${\rm NaN}$. Whenever the bits of $q$ of a floating-point number are all 1 then they represent an element of $F^{\rm special}$. If all $b_k=0$, then the number represents either $\ensuremath{\pm}\ensuremath{\infty}$. All other special floating-point numbers represent ${\rm NaN}$. 

\begin{example}[special in 16-bits] The number with bits
\[
\red{1}\ \green{11111}\ \blue{0000000000}
\]
has all exponent bits equal to $1$, and significand bits $0$ and sign bit $1$, hence represents $-\ensuremath{\infty}$. On the other hand, the number with bits
\[
\red{1}\ \green{11111}\ \blue{0000000001}
\]
has all exponent bits equal to $1$ but does not have all significand bits equal to $0$, hence is one of many representations for  ${\rm NaN}$. \end{example}

\subsection{Lab and problem sheet}
In the lab we explore how integers and floating point numbers are stored in a computer via different \emph{type}s.  We begin with a description both signed and unsigned integers, whose mathematical behaviour are detailed in the (non-examinable) appendix. We see how different sequences of bits can be reinterpreted as different numbers, and explore using string manipulation to construct different types of numbers. We also see how integers can sometimes be output in hexadecimal (base-16) format, which aligns better with the underlying binary storage. We then explore floating point numbers including construction  by specifying the bits directly. We finally investigate some of the degenerate behaviour such as arithmetic with special numbers. In the problem sheet we investigate some simple examples of representing numbers in floating point format.



