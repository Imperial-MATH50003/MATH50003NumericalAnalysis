\documentclass[12pt,a4paper]{article}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}




\hypersetup
       {   pdfauthor = {  },
           pdftitle={  },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }




\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    upquote=true,
    breaklines=true,
    breakindent=0pt,
    keepspaces=true,
    showspaces=false,
    columns=fullflexible,
    showtabs=false,
    showstringspaces=false,
    escapeinside={(*@}{@*)},
    extendedchars=true,
}
\newcommand{\HLJLt}[1]{#1}
\newcommand{\HLJLw}[1]{#1}
\newcommand{\HLJLe}[1]{#1}
\newcommand{\HLJLeB}[1]{#1}
\newcommand{\HLJLo}[1]{#1}
\newcommand{\HLJLk}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkc}[1]{\textcolor[RGB]{59,151,46}{\textit{#1}}}
\newcommand{\HLJLkd}[1]{\textcolor[RGB]{214,102,97}{\textit{#1}}}
\newcommand{\HLJLkn}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkp}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkr}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkt}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLn}[1]{#1}
\newcommand{\HLJLna}[1]{#1}
\newcommand{\HLJLnb}[1]{#1}
\newcommand{\HLJLnbp}[1]{#1}
\newcommand{\HLJLnc}[1]{#1}
\newcommand{\HLJLncB}[1]{#1}
\newcommand{\HLJLnd}[1]{\textcolor[RGB]{214,102,97}{#1}}
\newcommand{\HLJLne}[1]{#1}
\newcommand{\HLJLneB}[1]{#1}
\newcommand{\HLJLnf}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnfm}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnp}[1]{#1}
\newcommand{\HLJLnl}[1]{#1}
\newcommand{\HLJLnn}[1]{#1}
\newcommand{\HLJLno}[1]{#1}
\newcommand{\HLJLnt}[1]{#1}
\newcommand{\HLJLnv}[1]{#1}
\newcommand{\HLJLnvc}[1]{#1}
\newcommand{\HLJLnvg}[1]{#1}
\newcommand{\HLJLnvi}[1]{#1}
\newcommand{\HLJLnvm}[1]{#1}
\newcommand{\HLJLl}[1]{#1}
\newcommand{\HLJLld}[1]{\textcolor[RGB]{148,91,176}{\textit{#1}}}
\newcommand{\HLJLs}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsa}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsb}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsc}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsd}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdC}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLse}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLsh}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsi}[1]{#1}
\newcommand{\HLJLso}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsr}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLss}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLssB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLnB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnbB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnfB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnh}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLni}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnil}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnoB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLoB}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLow}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLp}[1]{#1}
\newcommand{\HLJLc}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLch}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcm}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcp}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcpB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcs}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcsB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLg}[1]{#1}
\newcommand{\HLJLgd}[1]{#1}
\newcommand{\HLJLge}[1]{#1}
\newcommand{\HLJLgeB}[1]{#1}
\newcommand{\HLJLgh}[1]{#1}
\newcommand{\HLJLgi}[1]{#1}
\newcommand{\HLJLgo}[1]{#1}
\newcommand{\HLJLgp}[1]{#1}
\newcommand{\HLJLgs}[1]{#1}
\newcommand{\HLJLgsB}[1]{#1}
\newcommand{\HLJLgt}[1]{#1}


\def\endash{â€“}
\def\bbD{ {\mathbb D} }
\def\bbZ{ {\mathbb Z} }
\def\bbR{ {\mathbb R} }
\def\bbC{ {\mathbb C} }

\def\x{ {\vc x} }
\def\a{ {\vc a} }
\def\b{ {\vc b} }
\def\e{ {\vc e} }
\def\f{ {\vc f} }
\def\u{ {\vc u} }
\def\v{ {\vc v} }
\def\y{ {\vc y} }
\def\z{ {\vc z} }
\def\w{ {\vc w} }

\def\bt{ {\tilde b} }
\def\ct{ {\tilde c} }
\def\Ut{ {\tilde U} }
\def\Qt{ {\tilde Q} }
\def\Rt{ {\tilde R} }
\def\Xt{ {\tilde X} }
\def\acos{ {\rm acos}\, }

\def\red#1{ {\color{red} #1} }
\def\blue#1{ {\color{blue} #1} }
\def\green#1{ {\color{ForestGreen} #1} }
\def\magenta#1{ {\color{magenta} #1} }

\input{somacros}

\begin{document}



\textbf{Numerical Analysis MATH50003 (2024\ensuremath{\endash}25) Problem Sheet 7}

This problem sheet explores approximation of functions by polynomials using interpolation and regression via least squares. Part of the nuance is understanding the relationship between Lagrange interpolation and interpolation via the Vandermonde matrices. We also discuss the singular value decomposition, in particular, we explore how itr can be used to construct the \emph{(Moore\ensuremath{\endash}Penrose) pseudoinverse}: a way of making sense of an ``inverse'' for rectangular or non-invertible matrices in a way that is consistent with least squares.

We begin with simple examples of constructing interpolants (using Lagrange interpolation) and linea regressions (using QR factorisations):

\rule{\textwidth}{1pt}
\textbf{Problem 1(a)} Use Lagrange interpolation to interpolate the function $\cos x$ by a polynomial at the points $[0,2,3,4]$ and evaluate at $x = 1$.

\textbf{Problem 1(b)} Find a least squares fit to $\cos x$ by a polynomial of degree $1$ (linear regression) at the points $[0,2,3,4]$ using QR. Hint: Choose the (non-default) sign so that $R[1,1]$ is positive to arrive at simpler computations. Since we are doing arithmetic by-hand without rounding the benefits of the default sign no longer apply.

\rule{\textwidth}{1pt}
The Vandermonde matrix used in interpolation is a fundamental mathematical object, in particular, its determinant comes up in differential equations, representation theory, and theoretical physics. The following problems investigate the relationship between the LU factorisation of a (transposed) Vandermonde matrix and its determinant.

\rule{\textwidth}{1pt}
\textbf{Problem 2(a)} Compute the LU factorisation of the following transposed Vandermonde matrices:
\[
\begin{bmatrix}
1 & 1 \\
x & y
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 \\
x & y & z \\
x^2 & y^2 & z^2
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 & 1 \\
x & y & z & t \\
x^2 & y^2 & z^2 & t^2 \\
x^3 & y^3 & z^3 & t^3
\end{bmatrix}
\]
Can you spot a pattern? Test your conjecture with a $5 \ensuremath{\times} 5$ Vandermonde matrix.

\textbf{Problem 2(b)} Use the LU factorisation to deduce the determinant of the above Vandermonde matrices. What happens to the determinants when you swap two points (i.e., swapping $x$ and $y$)?

\rule{\textwidth}{1pt}
The following problem is a simple example of an interpolatory quadrature rule, which can be constructed using the Lagrange basis.

\rule{\textwidth}{1pt}
\textbf{Problem 3} Compute the interpolatory quadrature rule
\[
\ensuremath{\int}_{-1}^1 f(x) w(x) \dx \ensuremath{\approx} \ensuremath{\sum}_{j=1}^n w_j f(x_j)
\]
for the points $[x_1,x_2,x_3] = [-1,1/2,1]$, for the weights $w(x) = 1$ and $w(x) = \sqrt{1-x^2}$.

\rule{\textwidth}{1pt}
We now turn to the (real-valued) SVD $A = U \ensuremath{\Sigma} V^\ensuremath{\top} \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} n}$, where $U \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} r}$ and $V \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} r}$ have orthonormal columns and $\ensuremath{\Sigma}$ is a diagonal matrix with non-increasing positive entries. We can use these to define a generalisation of the inverse of a matrix, known as a  \emph{pseudo-inverse}:
\[
A^+ := V \ensuremath{\Sigma}^{-1} U^\ensuremath{\top}.
\]
The pseudo-inverse has a number of properties similar to inverses:

\rule{\textwidth}{1pt}
\textbf{Problem 4}  Show that $A^+$ satisfies the \emph{Moore-Penrose conditions}: (1) $A A^+ A = A$, (2) $A^+ A A^+ = A^+$, and (3) $(A A^+)^\ensuremath{\top} = A A^+$ and $(A^+ A)^\ensuremath{\top} = A^+ A$

\rule{\textwidth}{1pt}
An important feature of the pseudo-inverse is that it can be used to solve least squares problems. That is, the SVD provides an alternative approach to QR for solving least squares, albeit one that is computationally less efficient. One advantage of the SVD for least squares is that it also allows for the case where the matrix involved is not of full column rank.

\rule{\textwidth}{1pt}
\textbf{Problem 5(a)} Show for $A \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} n}$ with $m \ensuremath{\geq} n$ and full rank that $\ensuremath{\bm{\x}} =  A^+ \ensuremath{\bm{\b}}$ is the least squares solution, i.e., minimises $\| A \ensuremath{\bm{\x}} - \ensuremath{\bm{\b}} \|$. Hint: extend $U$ in the SVD to be a square orthogonal matrix.

\textbf{Problem 5(b)} If $A \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} n}$ has a non-empty kernel there are multiple solutions to the least squares problem as  we can add any element of the kernel. Show that $\ensuremath{\bm{\x}} = A^+ \ensuremath{\bm{\b}}$ gives the least squares solution such that $\| \ensuremath{\bm{\x}} \|$ is minimised.

\rule{\textwidth}{1pt}


\end{document}