# Orthogonal and Unitary Matrices


PLU factorisations are an effective scheme for inverting systems, however, we saw in the lab that for very special matrices it can fail to be
accurate. In the next two sections we introduce an alternative approach that is guaranteed to be stable: factorise a matrix as
$$
A = QR
$$
where $Q$ is an orthogonal/unitary matrix and $R$ is a _right-triangular matrix_, which for square matrices is another name for upper-triangular.

This factorisation is valid for rectangular matrices $A âˆˆ â„‚^{m Ã— n}$, where now _right-triangular_ is a rectangular version of upper-triangular. For rectangular systems
we can no longer solve linear systems of the form $Að± = ð›$ (unless $ð›$ lies in the column span of $A$) but instead we want to solve $Að± â‰ˆ ð›$,
where $ð± âˆˆ â„‚^n$ and $ð› âˆˆ â„‚^m$. More precisely,
we can use a QR factorisation to solve _least squares_ problems, find $ð±$ that minimises the 2-norm:
$$
\|A ð± - ð› \|
$$

Before we discuss the computation of a QR factorisation and its role in solving least-squares problems, we introduce orthogonal and unitary matrices.
In particular we will discuss reflections and rotations, which can be used to represent more general orthogonal matrices.

**Definition (orthogonal/unitary matrix)** A square real matrix is _orthogonal_ if its inverse is its transpose:
$$
O(n) = \{Q âˆˆ â„^{n Ã— n} : Q^âŠ¤Q = I \}
$$
A square complex matrix is _unitary_ if its inverse is its adjoint:
$$
U(n) = \{Q âˆˆ â„‚^{n Ã— n} : Q^â‹†Q = I \}.
$$
Here the adjoint is the same as the conjugate-transpose: $Q^â‹† := \bar Q^âŠ¤$. 
âˆŽ


Note that $O(n) âŠ‚ U(n)$ as for real matrices $Q^â‹† = Q^âŠ¤$. Because in either case $Q^{-1} = Q^â‹†$ we also have
$Q Q^â‹† = I$ (which for real matrices is $Q Q^âŠ¤ = I$). These matrices are particularly important for
numerical linear algebra for a number of reasons (we'll explore these properties in the problem sheets):

1. They are norm-preserving: for any vector $ð± âˆˆ â„‚^n$ and $Q âˆˆ U(n)$    we have $\|Q ð± \| = \| ð±\|$ where $\| ð± \|^2 := âˆ‘_{k=1}^n x_k^2$ (i.e. the 2-norm).
2. All eigenvalues have absolute value equal to $1$.
3. For $Q âˆˆ O(n)$,  $\det Q = Â±1$.
2. They are trivially invertible (just take the adjoint).
3. They are generally â€œstable": errors due to rounding when multiplying a vector by $Q$ are controlled.
4. They are _normal matrices_: they commute with their adjoint ($Q Q^â‹† = Q Q^â‹†$). 
5. Both $O(n)$ and $U(n)$ are groups, in particular, they are closed under multiplication.

On a computer there are multiple ways of representing orthogonal/unitary matrices. The obvious way is to store
entries  as a dense matrix, however, this is very inefficient.
In the appendices we have seen permutation matrices, which
are a special type of orthogonal matrices where we can store only the order the entries are permuted as a vector. 

More generally, we will use the group structure: represent general orthogonal/unitary matrices as products of simpler
elements of the group. In partular we will use two building blocks:


1. _Rotations_: Rotations are equivalent to special orthogonal matrices $SO(2)$  and correspond to rotations in 2D.
2. _Reflections_:  Reflections are elements of $U(n)$ that are defined in terms of a single unit vector $ð¯ âˆˆ â„‚^n$ which is reflected.

We remark a related concept to orthogonal/unitary matrices are rectangular matrices with orthonormal columns, e.g.
$$
U = [ð®_1 | â‹¯ | ð®_n] âˆˆ â„‚^{m Ã— n}
$$
where $m â‰¥ n$ such that $U^â‹† U =  I_n$ (the $n Ã— n$ identity matrix). In the case where $m > n$ we must have $UU^â‹† â‰  I_m$ as the rank of $U$ is $n < m$. 


## Rotations

We begin with a general definition:

**Definition (Special Orthogonal and Rotations)** _Special Orthogonal Matrices_ are
$$
SO(n) := \{Q âˆˆ O(n) | \det Q = 1 \}
$$
And (simple) _rotations_ are $SO(2)$.
âˆŽ

In what follows we use the following for writing the angle of a vector:

**Definition (two-arg arctan)** The two-argument arctan function gives the angle `Î¸` through the point
$[a,b]^âŠ¤$, i.e., 
$$
\sqrt{a^2 + b^2} \begin{bmatrix} \cos Î¸ \\ \sin Î¸ \end{bmatrix} =  \begin{bmatrix} a \\ b \end{bmatrix}.
$$
It can be defined in terms of the standard arctan as follows:
$$
{\rm atan}(b,a) := \begin{cases} {\rm atan}{b \over a} & a > 0 \\
                            {\rm atan}{b \over a} + Ï€ & a < 0\hbox{ and }b >0 \\
                            {\rm atan}{b \over a} - Ï€ & a < 0\hbox{ and }b < 0 \\
                            Ï€/2 & a = 0\hbox{ and }b >0 \\
                            -Ï€/2 & a = 0\hbox{ and }b < 0 
                            \end{cases}
$$
âˆŽ


We show $SO(2)$ are exactly equivalent to standard rotations:


**Proposition (simple rotation)**
A 2Ã—2 _rotation matrix_ through angle $Î¸$ is
$$
Q_Î¸ := \begin{bmatrix} \cos Î¸ & -\sin Î¸ \cr \sin Î¸ & \cos Î¸ \end{bmatrix}.
$$
We have $Q âˆˆ SO(2)$ if and only if $Q = Q_Î¸$ for some $Î¸ âˆˆ â„$.

**Proof**

First assume $Q_Î¸$ is of that form and write $c = \cos Î¸$ and $s = \sin Î¸$. Then we have
$$
Q_Î¸^âŠ¤Q_Î¸ = \begin{pmatrix} c & s \\ -s & c \end{pmatrix} \begin{pmatrix} c & -s \\ s & c \end{pmatrix} = 
\begin{pmatrix} c^2 + s^2 & 0 \\ 0 & c^2 + s^2 \end{pmatrix} = I
$$
and $\det Q_Î¸ = c^2 + s^2 = 1$ hence $Q_Î¸ âˆˆ SO(2)$. 

Now suppose $Q = [ðª_1, ðª_2] âˆˆ SO(2)$ where we know its columns have norm 1, i.e. $\|ðª_k\| = 1$, and are orthogonal.
Write $ðª_1 = [c,s]$ where we know $c = \cos Î¸$ and $s = \sin Î¸$ for $Î¸ = {\rm atan}(s, c)$. 
Since $ðª_1\cdot ðª_2 = 0$ we can deduce $ðª_2 = Â± [-s,c]$. The sign is positive as $\det Q = Â±(c^2 + s^2) = Â±1$.

âˆŽ




We can rotate an arbitrary vector in $â„^2$ to the unit axis using rotations, which are useful in
linear algebra decompositions. Interestingly it only requires
basic algebraic functions (no trigonometric functions):



**Proposition (rotation of a vector)** 
The matrix
$$
Q = {1 \over \sqrt{a^2 + b^2}}
\begin{bmatrix}
 a & b \cr -b & a
\end{bmatrix}
$$
is a rotation matrix ($Q âˆˆ SO(2)$) satisfying
$$
Q \begin{bmatrix} a \\ b \end{bmatrix} = \sqrt{a^2 + b^2} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

**Proof** 

The last equation is trivial so the only question is that it is a rotation matrix. This follows immediately:
$$
Q^âŠ¤ Q = {1 \over a^2 + b^2}  \begin{bmatrix}
 a^2 + b^2 & 0 \cr 0 & a^2 + b^2
\end{bmatrix} = I
$$
and $\det Q = 1$.

âˆŽ

**Example (rotating a vector)** Consider the vector
$$
ð± = \Vectt[-1,-\sqrt{3}].
$$
We can use the proposition above to deduce the rotation matrix that rotates this
vector to the positive real axis is:
$$
{1 \over \sqrt{1+3}} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix} = 
{1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
$$
Alternatively, we could determine the matrix by computing the angle of the vector via:
$$
Î¸ =  {\rm atan}(-\sqrt{3}, -1) = {\rm atan}(\sqrt{3}) - Ï€ = -{2Ï€ \over 3}.
$$
We thus compute:
$$
Q_{-Î¸} = \begin{bmatrix}
\cos(2Ï€/3) & -\sin(2Ï€/3) \\
\sin(2Ï€/3) & \cos(2Ï€/3)
\end{bmatrix} = {1 \over 2} \begin{bmatrix} -1 & -\sqrt{3} \\ \sqrt{3} & -1 \end{bmatrix}.
$$
âˆŽ

More generally, we can consider rotations that operate on two entries of a vector at a time.
This will be explored in the problem sheet/lab.


## Reflections

In addition to rotations, another type of orthogonal/unitary matrix are reflections. These are
specified by a single vector which is reflected, with everything orthogonal to the vector left fixed. 

**Definition (reflection matrix)** 
Given a unit vector $ð¯ âˆˆ â„‚^n$ (satisfying $\|ð¯\|=1$), define the corresponding _reflection matrix_ as:
$$
Q_{ð¯} := I - 2 ð¯ ð¯^â‹†
$$
âˆŽ


These are indeed reflections in the direction of $ð¯$. We can show this as follows:

**Proposition (Householder properties)** $Q_{ð¯}$ satisfies:
1. Symmetry: $Q_{ð¯} = Q_{ð¯}^â‹†$
2. Orthogonality: $Q_{ð¯} âˆˆ U(n)$
3. The vector $ð¯$ is an eigenvector of $Q_{ð¯}$ with eigenvalue $-1$
4. For the dimension $n-1$ space $W := \{ð° : ð°^â‹† ð¯ = 0 \}$, all vectors $ð° âˆˆ W$ satisfy $Q_{ð¯}ð° = ð°$.
5. Not a rotation: $\det Q_{ð¯} = -1$


**Proof**

Property 1 follows immediately. Property 2 follows from
$$
Q_{ð¯}^â‹† Q_{ð¯} = Q_{ð¯}^2 = I - 4 ð¯ ð¯^â‹† + 4 ð¯ ð¯^â‹† ð¯ ð¯^â‹† = I.
$$
Property 3 follows since
$$
Q_{ð¯} ð¯ = ð¯ - 2ð¯ (ð¯^â‹†ð¯) = -ð¯.
$$
Property 4 follows from:
$$
Q_{ð¯} ð° = ð° - 2 ð¯ (ð°^â‹† ð¯) =  ð°
$$
Property 5 then follows: Property 4 tells us that
$1$ is an eigenvalue with multiplicity $n-1$. Since $-1$ is an eigenvalue with multiplicity 1,
 the determinant, which is product of the eigenvalues, is $-1$.

âˆŽ



**Example (reflection through 2-vector)** Consider reflection through $ð± = [1,2]^âŠ¤$. 
We first need to normalise $ð±$:
$$
ð¯ = {ð± \over \|ð±\|} = \begin{bmatrix} {1 \over \sqrt{5}} \\ {2 \over \sqrt{5}} \end{bmatrix}
$$
The reflection matrix is:
$$
Q_{ð¯} = I - 2 ð¯ ð¯^âŠ¤ = \begin{bmatrix}1 \\ & 1 \end{bmatrix} - {2 \over 5} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}
 =  {1 \over 5} \begin{bmatrix} 3 & -4 \\ -4 & -3 \end{bmatrix}
$$
Indeed it is symmetric, and orthogonal. It sends $ð±$ to $-ð±$:
$$
Q_{ð¯} ð± = {1 \over 5} \begin{bmatrix}3 - 8 \\ -4 - 6 \end{bmatrix} = -ð±
$$
Any vector orthogonal to $ð±$, like $ð² = [-2,1]^âŠ¤$, is left fixed:
$$
Q_{ð¯} ð² = {1 \over 5} \begin{bmatrix}-6 -4 \\ 8 - 3 \end{bmatrix} = ð²
$$
âˆŽ


Note that _building_ the matrix $Q_{ð¯}$ will be expensive ($O(n^2)$ operations), but we can _apply_
$Q_{ð¯}$ to a vector in $O(n)$ operations using the expression:
$$
Q_{ð¯} ð± = ð± - 2 ð¯ (ð¯^â‹† ð±) = ð± - 2 ð¯ (ð¯ â‹… ð±).
$$

### Householder reflections

Just as rotations can be used to rotate vectors to be aligned with coordinate axes, so can reflections,
but in this case it works for vectors in $â„‚^n$, not just $â„^2$. We begin with the real case:

**Definition (Householder reflection, real case)** For a given vector
$ð± âˆˆ â„^n$, define the Householder reflection
$$
Q_{ð±}^{Â±,\rm H} := Q_{ð°}
$$
for $ð² = âˆ“ \|ð±\| ðž_1 + ð±$ and $ð° = {ð² \over \|ð²\|}$.
The default choice in sign is:
$$
Q_{ð±}^{\rm H} := Q_{ð±}^{-\hbox{sign}(x_1),\rm H}.
$$
âˆŽ

**Lemma (Householder reflection maps to axis)** For $ð± âˆˆ â„^n$,
$$
Q_{ð±}^{Â±,\rm H} ð± = Â±\|ð±\| ðž_1
$$

**Proof**
Note that
$$
\begin{align*}
\| ð² \|^2 &= 2\|ð±\|^2 âˆ“ 2 \|ð±\| x_1, \\
ð²^âŠ¤ ð± &= \|ð±\|^2 âˆ“  \|ð±\| x_1
\end{align*}
$$
where $x_1 = ðž_1^âŠ¤ ð±$. Therefore:
$$
Q_{ð±}^{Â±,\rm H} ð±  =  (I - 2 ð° ð°^âŠ¤) ð± = ð± - 2 {ð²  \|ð±\|  \over \|ð²\|^2} (\|ð±\|âˆ“x_1) = ð± - ð² =  Â±\|ð±\| ðž_1.
$$

âˆŽ

**Remark** Why do we choose the the opposite sign of $x_1$ for the default reflection? For stability, but
we won't discuss this in more detail.

We can extend this definition for complex vectors. In this case the choice of the sign is delicate and so we
only generalise the default choice using a complex-analogue of the sign fuunction.

**Definition (Householder reflection, complex case)** For a given vector
$ð± âˆˆ â„‚^n$, define the Householder reflection as
$$
Q_{ð±}^{\rm H} := Q_{ð°}
$$
for $ð² = {\rm csign}(x_1) \|ð±\| ðž_1 + ð±$ and $ð° = {ð² \over \|ð²\|}$, for ${\rm csign}(z) = {\rm e}^{{\rm i} \arg z}$. 
âˆŽ


**Lemma (Householder reflection maps to axis, complex case)** For $ð± âˆˆ â„‚^n$,
$$
Q_{ð±}^{\rm H} ð± = -{\rm csign}(x_1) \|ð±\| ðž_1
$$

**Proof**
Denote $Î± := {\rm csign}(x_1)$. 
Note that $\baralpha x_1 = {\rm e}^{-{\rm i} \arg x_1} x_1 = |x_1|$.  Now we have
$$
\begin{align*}
\| ð² \|^2 &= (Î± \|ð±\| ðž_1 + ð±)^â‹†(Î± \|ð±\| ðž_1 + ð±) = |Î±|\| ð± \|^2 + \| ð± \|  Î± \bar x_1 + \baralpha x_1 \| ð± \| + \| ð± \|^2 \\
&= 2\| ð± \|^2 + 2|x_1| \| ð± \| \\
ð²^â‹† ð± &= \baralpha x_1 \| ð± \| + \|ð± \|^2 = \|ð± \|^2 + |x_1| \| ð± \|
\end{align*}
$$
Therefore:
$$
Q_{ð±}^{\rm H} ð±  =  (I - 2 ð° ð°^â‹†) ð± = ð± - 2 {ð²    \over \|ð²\|^2} (\|ð± \|^2 + |x_1| \|ð± \|) = ð± - ð² =  -Î± \|ð±\| ðž_1.
$$

âˆŽ