
\section{LU and Cholesky factorisations}
In this section we consider the following factorisations for square invertible  matrices $A$:

\begin{itemize}
\item[1. ] The \emph{LU factorisation}: $A = LU$ where $L$ is lower triangular and $U$ is upper triangular. This is equivalent to Gaussian elimination without pivoting, so may not exist (e.g. if $a_{11} = 0$).


\item[2. ] The \emph{PLU factorisation}: $A = P^\ensuremath{\top} LU$ where $P$ is a permutation matrix (a matrix when multiplying a vector is equivalent to permuting its rows), $L$ is lower triangular and $U$ is upper triangular. This is equivalent to Gaussian elimination with pivoting. It always exists but may be unstable in extremely rare cases. We won't discuss the details of computing the PLU factorisation but will explore practical usage in the lab.


\item[3. ] For a real square \emph{symmetric positive definite} ($A \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ such that $A^\ensuremath{\top} = A$ and $\ensuremath{\bm{\x}}^\ensuremath{\top} A \ensuremath{\bm{\x}} > 0$ for all $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n$, $\ensuremath{\bm{\x}} \ensuremath{\neq} 0$)  matrix the LU decomposition has a special form called the \emph{Cholesky factorisation}: $A = L L^\ensuremath{\top}$. This provides an algorithmic way to \emph{prove} that a matrix is symmetric positive definite, and is roughly twice as fast as the LU factorisation to compute.

\end{itemize}
\subsection{Outer products}
In what follows we will use outer products extensively:

\begin{definition}[outer product] Given $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbF}^m$ and $\ensuremath{\bm{\y}} \ensuremath{\in} \ensuremath{\bbF}^n$ the \emph{outer product} is:
\[
\ensuremath{\bm{\x}} \ensuremath{\bm{\y}}^\ensuremath{\top} := [\ensuremath{\bm{\x}} y_1 | \ensuremath{\cdots} | \ensuremath{\bm{\x}} y_n] = \begin{bmatrix} x_1 y_1 & \ensuremath{\cdots} & x_1 y_n \\
                        \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                        x_m y_1 & \ensuremath{\cdots} & x_m y_n \end{bmatrix} \ensuremath{\in} \ensuremath{\bbF}^{m \ensuremath{\times} n}.
\]
Note this is equivalent to matrix-matrix multiplication if we view $\ensuremath{\bm{\x}}$ as a $m \ensuremath{\times} 1$ matrix and $\ensuremath{\bm{\y}}^\ensuremath{\top}$ as a $1 \ensuremath{\times} n$ matrix. \end{definition}

\begin{proposition}[rank-1] A matrix $A \ensuremath{\in} \ensuremath{\bbF}^{m\ensuremath{\times}n}$ has rank 1 if and only if there exists $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbF}^m$ and $\ensuremath{\bm{\y}} \ensuremath{\in} \ensuremath{\bbF}^n$ such that
\[
A = \ensuremath{\bm{\x}} \ensuremath{\bm{\y}}^\ensuremath{\top}.
\]
\end{proposition}
\textbf{Proof} This follows immediately as if $A = \ensuremath{\bm{\x}} \ensuremath{\bm{\y}}^\ensuremath{\top}$ then all columns are multiples of $\ensuremath{\bm{\x}}$. On the other hand, if $A$ has rank-1 there exists a nonzero column $\ensuremath{\bm{\x}} := \ensuremath{\bm{\a}}_j$ that all other columns are multiples of. \ensuremath{\QED}

\subsection{LU factorisation}
Gaussian elimination  can be interpreted as an LU factorisation. Write a matrix $A \ensuremath{\in} \ensuremath{\bbF}^{n \ensuremath{\times} n}$ as follows:
\[
A =  \begin{bmatrix} \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\ \ensuremath{\bm{\v}}_1 & K_1 \end{bmatrix}
\]
where $\ensuremath{\alpha}_1 = a_{11}$, $\ensuremath{\bm{\v}}_1 = A[2:n, 1]$ and $\ensuremath{\bm{\w}}_1 = A[1, 2:n]$ (that is, $\ensuremath{\bm{\v}}_1 \ensuremath{\in} \ensuremath{\bbF}^{n-1}$ is a vector whose entries are the 2nd through last row of the first column of $A$ whilst $\ensuremath{\bm{\w}}_1 \ensuremath{\in} \ensuremath{\bbF}^{n-1}$ is a vector containing the 2nd through last entries in the last column of $A$). Gaussian elimination consists of taking the first row, dividing by $\ensuremath{\alpha}_1$ and subtracting from all other rows. That is equivalent to multiplying by a lower triangular matrix:
\[
\begin{bmatrix}
1 \\
-\ensuremath{\bm{\v}}_1/\ensuremath{\alpha}_1 & I \end{bmatrix} A = \begin{bmatrix} \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\  & K_1 -\ensuremath{\bm{\v}}_1\ensuremath{\bm{\w}}_1^\ensuremath{\top} /\ensuremath{\alpha}_1 \end{bmatrix}
\]
where $A_2 := K_1 -\ensuremath{\bm{\v}}_1\ensuremath{\bm{\w}}_1^\ensuremath{\top} /\ensuremath{\alpha}_1$  happens to be a rank-1 perturbation of $K_1$. We can write this another way:
\[
A = \underbrace{\begin{bmatrix}
1 \\
\ensuremath{\bm{\v}}_1/\ensuremath{\alpha}_1 & I \end{bmatrix}}_{L_1}  \begin{bmatrix} \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\  & A_2 \end{bmatrix}
\]
Now assume we continue this process and manage to deduce an LU factorisation $A_2 = L_2 U_2$. Then
\[
A = L_1 \begin{bmatrix} \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\  & L_2U_2 \end{bmatrix}
= \underbrace{L_1 \begin{bmatrix}
1 \\
 & L_2 \end{bmatrix}}_L  \underbrace{\begin{bmatrix} \ensuremath{\alpha}_1 & \ensuremath{\bm{\w}}_1^\ensuremath{\top} \\  & U_2 \end{bmatrix}}_U
\]
Note we can multiply through to find
\[
L = \begin{bmatrix}
1 \\
\ensuremath{\bm{\v}}_1/\ensuremath{\alpha}_1 & L_2 \end{bmatrix}.
\]
Noting that if $A \ensuremath{\in} \ensuremath{\bbF}^{1 \ensuremath{\times} 1}$ then it has a trivial LU factorisation we can use the above construction to proceed recursively until we arrive at the trivial case.

\begin{example}[LU by-hand] Consider the matrix
\[
A = \begin{bmatrix} 1 & 1 & 1 \\
                    2 & 4 & 8 \\
                    1 & 4 & 9
                    \end{bmatrix} = \underbrace{\begin{bmatrix} 1  \\
                    2 & 1 &  \\
                    1 &  & 1
                    \end{bmatrix}}_{L_1} \begin{bmatrix} 1 & 1 & 1 \\
                    0 & 2 & 6 \\
                    0 & 3 & 8
                    \end{bmatrix}
\]
In more detail, for $\ensuremath{\alpha}_1 := a_{11} = 1$, $\ensuremath{\bm{\v}}_1 := A[2:3,1] = \vectt[2,1]$, $\ensuremath{\bm{\w}}_1 = A[1,2:3] = \vectt[1,1]$ and
\[
K_1 := A[2:3,2:3] = \begin{bmatrix} 4 & 8 \\ 4 & 9 \end{bmatrix}
\]
we have
\[
A_2 := K_1 -\ensuremath{\bm{\v}}_1\ensuremath{\bm{\w}}_1^\ensuremath{\top} /\ensuremath{\alpha}_1 = \begin{bmatrix} 4 & 8 \\ 4 & 9 \end{bmatrix} - \begin{bmatrix} 2 & 2 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 6 \\ 3 & 8 \end{bmatrix}.
\]
We then repeat the process and determine (with $\ensuremath{\alpha}_2 := A_2[1,1] = 2$, $\ensuremath{\bm{\v}}_2 := A_2[2:2,1] = [3]$, $\ensuremath{\bm{\w}}_2 := A_2[1,2:2] = [6]$ and $K_2 := A_2[2:2,2:2] = [8]$):
\[
A_2 =  \begin{bmatrix}2 & 6 \\ 3 & 8 \end{bmatrix} =
\underbrace{\begin{bmatrix}
1 \\
3/2 & 1
\end{bmatrix}}_{L_2} \begin{bmatrix} 2 & 6 \\
            & -1 \end{bmatrix}
\]
The last \ensuremath{\ldq}matrix" is 1 x 1 so we get the trivial decomposition:
\[
A_3 := K_2 - \ensuremath{\bm{\v}}_2 \ensuremath{\bm{\w}}_2^\ensuremath{\top} /\ensuremath{\alpha}_2 =  [-1] = \underbrace{[1]}_{L_3} [-1]
\]
Putting everything together and placing the $j$-th column of $L_j$ inside the $j$-th column of $L$ we have
\[
A = \underbrace{\begin{bmatrix} 1  \\
                    2 & 1 &  \\
                    1 & 3/2 & 1
                    \end{bmatrix}}_{L} \underbrace{\begin{bmatrix} 1 & 1 & 1 \\
                     & 2 & 6 \\
                     &  & -1
                    \end{bmatrix}}_U
\]
\end{example}

\subsection{PLU factorisation}
We learned in first year linear algebra that if a diagonal entry is zero when doing Gaussian elimination one has to \emph{row pivot}. For stability, in implementation one may wish to pivot even if the diagonal entry is nonzero: swap the largest in magnitude entry for the entry on the diagonal turns out to be significantly more stable than standard LU.

This is equivalent to a PLU decomposition. Here we use a \emph{permutation matrix}, whose action on a vector permutes its entries, as discussed in the appendix. That is, consider a permutation which we identify with a vector ${\mathbf \ensuremath{\sigma}} = [\ensuremath{\sigma}_1,\ensuremath{\ldots},\ensuremath{\sigma}_n]$ containing the integers $1,\ensuremath{\ldots},n$ exactly once. The permutation operator represents the action of permuting the entries in a vector:
\[
P_\ensuremath{\sigma}(\ensuremath{\bm{\v}}) := \ensuremath{\bm{\v}}[{\mathbf \ensuremath{\sigma}}] = \Vectt[v_{\ensuremath{\sigma}_1},\ensuremath{\vdots},v_{\ensuremath{\sigma}_n}]
\]
This is a linear operator, and hence we can identify it with a \emph{permutation matrix} $P_\ensuremath{\sigma} \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ (more precisely the entries  of $P_\ensuremath{\sigma}$ are either 1 or 0). Importantly, products of permutation matrices are also permutation matrices and permutation matrices are orthogonal, that is, $P_\ensuremath{\sigma}^{-1} = P_\ensuremath{\sigma}^\top$.

\begin{theorem}[PLU] A matrix $A \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}$ is invertible if and only if it has a PLU decomposition:
\[
A = P^\ensuremath{\top} L U
\]
where the diagonal of $L$ are all equal to 1 and the diagonal of $U$ are all non-zero, and $P$ is a permutation matrix.

\end{theorem}
\textbf{Proof}

If we have a PLU decomposition of this form then $L$ and $U$ are invertible and hence the inverse is simply $A^{-1} = U^{-1} L^{-1} P$. Hence we consider the orther direction.

If $A \ensuremath{\in} \ensuremath{\bbC}^{1 \ensuremath{\times} 1}$ we trivially have an LU decomposition $A = [1] * [a_{11}]$ as all $1 \ensuremath{\times} 1$ matrices are triangular. We now proceed by induction: assume all invertible matrices of lower dimension have a PLU factorisation. As $A$ is invertible not all entries in the first column are zero. Therefore there exists a permutation $P_1$ so that $\ensuremath{\alpha} := (P_1 A)[1,1] \ensuremath{\neq}Â 0$. Hence we write
\[
P_1 A = \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\
                        \ensuremath{\bm{\v}} & K
                        \end{bmatrix} = \underbrace{\begin{bmatrix}
1 \\
\ensuremath{\bm{\v}}/\ensuremath{\alpha} & I \end{bmatrix}}_{L_1}  \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\  & K - \ensuremath{\bm{\v}} \ensuremath{\bm{\w}}^\ensuremath{\top}/\ensuremath{\alpha} \end{bmatrix}
\]
We deduce that $A_2 := K - \ensuremath{\bm{\v}} \ensuremath{\bm{\w}}^\ensuremath{\top}/\ensuremath{\alpha}$ is invertible because $A$ and $L_1$ are invertible (Exercise).

By assumption we can write $A_2 = P_2^\ensuremath{\top} L_2 U_2$. Thus we have:
\begin{align*}
\underbrace{\begin{bmatrix} 1 \\
            & P_2 \end{bmatrix} P_1}_P A &= \begin{bmatrix} 1 \\
            & P_2 \end{bmatrix}  \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\
                        \ensuremath{\bm{\v}} & A_2
                        \end{bmatrix}  =
            \begin{bmatrix} 1 \\ & P_2 \end{bmatrix} L_1  \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\  & P_2^\ensuremath{\top} L_2  U_2 \end{bmatrix} \\
            &= \begin{bmatrix}
1 \\
P_2 \ensuremath{\bm{\v}}/\ensuremath{\alpha} & P_2 \end{bmatrix} \begin{bmatrix} 1 &  \\  &  P_2^\ensuremath{\top} L_2  \end{bmatrix}  \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\  &  U_2 \end{bmatrix} \\
&= \underbrace{\begin{bmatrix}
1 \\
P_2 \ensuremath{\bm{\v}}/\ensuremath{\alpha} & L_2  \end{bmatrix}}_L \underbrace{\begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\w}}^\ensuremath{\top} \\  &  U_2 \end{bmatrix}}_U. \\
\end{align*}
\ensuremath{\QED}

We don't discuss the practical implementation of this factorisation (though an algorithm is hidden in the above proof). We also note that for stability one uses the permutation that always puts the largest in magnitude entry in the top row. In the lab we explore the practical usage of this factorisation.

\subsection{Cholesky factorisation}
A \emph{Cholesky factorisation} is a form of Gaussian elimination (without pivoting) that exploits symmetry in the problem, resulting in a substantial speedup. It is only applicable for \emph{symmetric positive definite} (SPD) matrices, or rather, the algorithm for computing it succeeds if and only if the matrix is SPD. In other words, it gives an algorithmic way to prove whether or not a matrix is SPD.

\begin{definition}[positive definite] A square matrix $A \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ is \emph{positive definite} if for all $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n, x \ensuremath{\neq} 0$ we have
\[
\ensuremath{\bm{\x}}^\ensuremath{\top} A \ensuremath{\bm{\x}} > 0
\]
\end{definition}

First we establish some basic properties of positive definite matrices:

\begin{proposition}[conj. pos. def.] If  $A \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ is positive definite and $V \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ is non-singular then
\[
V^\ensuremath{\top} A V
\]
is positive definite. \end{proposition}
\textbf{Proof}

For all  $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^n, \ensuremath{\bm{\x}} \ensuremath{\neq} 0$, define $\ensuremath{\bm{\y}} = V \ensuremath{\bm{\x}} \ensuremath{\neq} 0$ (since $V$ is non-singular). Thus we have
\[
\ensuremath{\bm{\x}}^\ensuremath{\top} V^\ensuremath{\top} A V \ensuremath{\bm{\x}} = \ensuremath{\bm{\y}}^\ensuremath{\top} A \ensuremath{\bm{\y}} > 0.
\]
\ensuremath{\QED}

\begin{proposition}[diag positivity] If $A \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ is positive definite then its diagonal entries are positive: $a_{kk} > 0$. \end{proposition}
\textbf{Proof}
\[
a_{kk} = \ensuremath{\bm{\e}}_k^\ensuremath{\top} A \ensuremath{\bm{\e}}_k > 0.
\]
\ensuremath{\QED}

\begin{lemma}[subslice pos. def.] If $A \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}$ is positive definite and $\ensuremath{\bm{\k}} = [k_1,\ensuremath{\ldots},k_m]^\ensuremath{\top} \ensuremath{\in} \{1,\ensuremath{\ldots},n\}^m$ is a vector of $m$ integers where any integer appears only once,  then $A[\ensuremath{\bm{\k}},\ensuremath{\bm{\k}}] \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} m}$ is also positive definite. \end{lemma}
\textbf{Proof} For all $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbR}^m, \ensuremath{\bm{\x}} \ensuremath{\neq} 0$, consider $\ensuremath{\bm{\y}} \ensuremath{\in} \ensuremath{\bbR}^n$ such that $y_{k_j} = x_j$ and zero otherwise. Then we have
\[
\ensuremath{\bm{\x}}^\ensuremath{\top} A[\ensuremath{\bm{\k}},\ensuremath{\bm{\k}}] \ensuremath{\bm{\x}} = \ensuremath{\sum}_{\ensuremath{\ell}=1}^m \ensuremath{\sum}_{j=1}^m x_\ensuremath{\ell} x_j a_{k_\ensuremath{\ell},k_j} = \ensuremath{\sum}_{\ensuremath{\ell}=1}^m \ensuremath{\sum}_{j=1}^m y_{k_\ensuremath{\ell}} y_{k_j} a_{k_\ensuremath{\ell},k_j}  = \ensuremath{\sum}_{\ensuremath{\ell}=1}^n \ensuremath{\sum}_{j=1}^n y_\ensuremath{\ell} y_j a_{\ensuremath{\ell},j} = \ensuremath{\bm{\y}}^\ensuremath{\top} A \ensuremath{\bm{\y}} > 0.
\]
\ensuremath{\QED}

Here is the key result:

\begin{theorem}[Cholesky and SPD] A matrix $A$ is symmetric positive definite if and only if it has a Cholesky factorisation
\[
A = L L^\ensuremath{\top}
\]
where $L$ is lower triangular with positive diagonal entries.

\end{theorem}
\textbf{Proof} If $A$ has a Cholesky factorisation it is symmetric ($A^\ensuremath{\top} = (L L^\ensuremath{\top})^\ensuremath{\top} = A$) and for $\ensuremath{\bm{\x}} \ensuremath{\neq} 0$ we have
\[
\ensuremath{\bm{\x}}^\ensuremath{\top} A \ensuremath{\bm{\x}} = (L^\ensuremath{\top}\ensuremath{\bm{\x}})^\ensuremath{\top} L^\ensuremath{\top} \ensuremath{\bm{\x}} = \|L^\ensuremath{\top}\ensuremath{\bm{\x}}\|^2 > 0
\]
where we use the fact that $L$ is non-singular.

For the other direction we will prove it by induction, with the $1 \ensuremath{\times} 1$ case being trivial. Assume all lower dimensional symmetric positive definite matrices have Cholesky decompositions. Write
\[
A = \begin{bmatrix} \ensuremath{\alpha} & \ensuremath{\bm{\v}}^\ensuremath{\top} \\
                    \ensuremath{\bm{\v}}   & K
                    \end{bmatrix} = \underbrace{\begin{bmatrix} \sqrt{\ensuremath{\alpha}} \\
                                    {\ensuremath{\bm{\v}} \over \sqrt{\ensuremath{\alpha}}} & I \end{bmatrix}}_{L_1}
                                    \begin{bmatrix} 1  \\ & K - {\ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\top} \over \ensuremath{\alpha}} \end{bmatrix}
                                    \underbrace{\begin{bmatrix} \sqrt{\ensuremath{\alpha}} & {\ensuremath{\bm{\v}}^\ensuremath{\top} \over \sqrt{\ensuremath{\alpha}}} \\
                                     & I \end{bmatrix}}_{L_1^\ensuremath{\top}}.
\]
Note that $A_2 := K - {\ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\top} \over \ensuremath{\alpha}}$ is a subslice of $L_1^{-1} A L_1^{-\ensuremath{\top}}$, hence by combining the previous propositions is itself SPD. Thus we can write
\[
A_2 = K - {\ensuremath{\bm{\v}} \ensuremath{\bm{\v}}^\ensuremath{\top} \over \ensuremath{\alpha}} = L_2 L_2^\ensuremath{\top}
\]
and hence $A = L L^\ensuremath{\top}$ for
\[
L= L_1 \begin{bmatrix}1 \\ & L_2 \end{bmatrix} = \begin{bmatrix} \sqrt{\ensuremath{\alpha}} \\ {\ensuremath{\bm{\v}} \over \sqrt{\ensuremath{\alpha}}} & L_2 \end{bmatrix}
\]
satisfies $A = L L^\ensuremath{\top}$. \ensuremath{\QED}

\begin{example}[Cholesky by hand] Consider the matrix
\[
A = \begin{bmatrix}
2 &1 &1 &1 \\
1 & 2 & 1 & 1 \\
1 & 1 & 2 & 1 \\
1 & 1 & 1 & 2
\end{bmatrix}
\]
Then $\ensuremath{\alpha}_1 = 2$, $\ensuremath{\bm{\v}}_1 = [1,1,1]$, and
\[
A_2 = \begin{bmatrix}
2 &1 &1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{bmatrix} - {1 \over 2} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}
={1 \over 2} \begin{bmatrix}
3 & 1 & 1 \\
1 & 3 & 1 \\
1 & 1 & 3
\end{bmatrix}.
\]
Continuing, we have $\ensuremath{\alpha}_2 = 3/2$, $\ensuremath{\bm{\v}}_2 = [1/2,1/2]$, and
\[
A_3 = {1 \over 2} \left( \begin{bmatrix}
3 & 1 \\ 1 & 3
\end{bmatrix} - {1 \over 3} \begin{bmatrix} 1 \\ 1  \end{bmatrix} \begin{bmatrix} 1 & 1  \end{bmatrix}
\right)
= {1 \over 3} \begin{bmatrix} 4 & 1 \\ 1 & 4 \end{bmatrix}
\]
Next, $\ensuremath{\alpha}_3 = 4/3$, $\ensuremath{\bm{\v}}_3 = [1]$, and
\[
A_4 = [4/3 - 3/4 * (1/3)^2] = [5/4]
\]
i.e. $\ensuremath{\alpha}_4 = 5/4$.

Thus we get
\[
L= \begin{bmatrix}
\sqrt{\ensuremath{\alpha}_1} \\
{\ensuremath{\bm{\v}}_1[1] \over \sqrt{\ensuremath{\alpha}_1}} & \sqrt{\ensuremath{\alpha}_2} \\
{\ensuremath{\bm{\v}}_1[2] \over \sqrt{\ensuremath{\alpha}_1}} & {\ensuremath{\bm{\v}}_2[1] \over \sqrt{\ensuremath{\alpha}_2}}  & \sqrt{\ensuremath{\alpha}_3} \\
{\ensuremath{\bm{\v}}_1[3] \over \sqrt{\ensuremath{\alpha}_1}} & {\ensuremath{\bm{\v}}_2[2] \over \sqrt{\ensuremath{\alpha}_2}}  & {\ensuremath{\bm{\v}}_3[1] \over \sqrt{\ensuremath{\alpha}_3}}  & \sqrt{\ensuremath{\alpha}_4}
\end{bmatrix}
 = \begin{bmatrix} \sqrt{2} \\ {1 \over \sqrt{2}} & \sqrt{3 \over 2} \\
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {2 \over \sqrt{3}} \\
{1 \over \sqrt{2}} & {1 \over \sqrt 6} & {1 \over \sqrt{12}} & {\sqrt{5} \over 2}
\end{bmatrix}
\]
\end{example}



