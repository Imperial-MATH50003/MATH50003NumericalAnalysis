
\section{Singular Value Decomposition and Matrix Compression}
In the previous section we saw an application of least squares to regression, where the best 2-norm fit to a vector of function samples was computed. But what if the data is a matrix, eg. corresponding to a 2D function? Here we consider finding the best approximation to a matrix in the 2-norm sense by by a matrix with a  lower rank.  This concept has numerous applications in compressing matrices, including  Principle Component Analysis (PCA) and Machine Learning (where one might want to compress the ``weight'' matrices to minimise degrees of freedom). 

We will use induced matrix norms (see appendix), in particular the 2-norm of a matrix $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ is defined via
\[
\| A \| :=  \sup_{\ensuremath{\bm{\v}} \ensuremath{\in} \ensuremath{\bbC}^m : \|\ensuremath{\bm{\v}}\|_X=1} \|A \ensuremath{\bm{\v}}\| =  \sup_{\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbC}^m} {\|A \ensuremath{\bm{\x}}\| \over \| \ensuremath{\bm{\x}}\|}
\]
The matrix 1- and $\ensuremath{\infty}$-norms have simple definitions in terms of column/row sums. On the other hand, the 2-norm has no simple formula. 

In order to define the 2-norm we will introduce the  \emph{Singular Value Decomposition (SVD)}: a matrix factorisation that encodes how much a matrix ``stretches''  vectors:

\begin{definition}[singular value decomposition] For $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ with rank $r > 0$,  the \emph{(reduced) singular value decomposition (SVD)} is
\[
A = U \ensuremath{\Sigma} V^\ensuremath{\star}
\]
where $U \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} r}$ and $V \ensuremath{\in}  \ensuremath{\bbC}^{n \ensuremath{\times} r}$ have orthonormal columns and $\ensuremath{\Sigma} \ensuremath{\in} \ensuremath{\bbR}^{r \ensuremath{\times} r}$ is  diagonal whose diagonal entries, which which we call \emph{singular values}, are all positive and non-increasing: $\ensuremath{\sigma}_1 \ensuremath{\geq} \ensuremath{\cdots} \ensuremath{\geq} \ensuremath{\sigma}_r > 0$. The \emph{full singular value decomposition (SVD)} is
\[
A = \tilde{U} \tilde{\Sigma} \tilde{V}^\ensuremath{\star}
\]
where $\tilde{U} \ensuremath{\in} U(m)$ and $\tilde{V} \ensuremath{\in}  U(n)$ are unitary matrices and $\tilde{\Sigma} \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} n}$ has only diagonal non-zero entries, i.e., if $m > n$,
\[
\tilde{\Sigma} = \begin{bmatrix} \ensuremath{\sigma}_1 \\ & \ensuremath{\ddots} \\ && \ensuremath{\sigma}_n \\ && 0 \\ && \ensuremath{\vdots} \\ && 0 \end{bmatrix}
\]
and if $m < n$,
\[
\tilde{\Sigma} = \begin{bmatrix} \ensuremath{\sigma}_1 \\ & \ensuremath{\ddots} \\ && \ensuremath{\sigma}_m & 0 & \ensuremath{\cdots} & 0\end{bmatrix}
\]
where $\ensuremath{\sigma}_k = 0$ if $k > r$. \end{definition}

In particular, we discuss:

\begin{itemize}
\item[1. ] Existence of the SVD: we show that an SVD exists by relating it to the eigenvalue Decomposition of $A^\ensuremath{\star}A$ and $AA^\ensuremath{\star}$.


\item[2. ] 2-norm and SVD: the 2-norm of a matrix is equal to  the largest singular value $\ensuremath{\sigma}_1$.


\item[3. ] Best rank-$k$ approximation and compression: the best approximation of a matrix by a smaller rank matrix can be constructed using the SVD, which gives an effective way to compress matrices. 

\end{itemize}
What we do not discuss is computation of the SVD. There are reliable and efficient iterative algorithms for computing the SVD, similar to computing eigen-decompositions, but this is beyond the scope of this module.

\subsection{Existence}
To show the SVD exists we first establish some properties of a \emph{Gram matrix} ($A^\ensuremath{\star} A$), which has within its eigendecomposition part of the SVD. The Gram matrix is best viewed as the matrix of inner products of the columns of a matrix: if $A = \begin{bmatrix} \ensuremath{\bm{\a}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\a}}_n \end{bmatrix} \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ then the Gram matrix is the Hermitian matrix
\[
A^\ensuremath{\star} A = \begin{bmatrix} \ensuremath{\bm{\a}}_1^\ensuremath{\star} \ensuremath{\bm{\a}}_1 & \ensuremath{\cdots} &\ensuremath{\bm{\a}}_1^\ensuremath{\star} \ensuremath{\bm{\a}}_n \\
                        \ensuremath{\vdots} &  \ensuremath{\ddots} & \ensuremath{\vdots} \\
                        \ensuremath{\bm{\a}}_n^\ensuremath{\star} \ensuremath{\bm{\a}}_1 & \ensuremath{\cdots} &\ensuremath{\bm{\a}}_n^\ensuremath{\star} \ensuremath{\bm{\a}}_n \end{bmatrix} \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}.
\]
We first establish that the kernels match:

\begin{proposition}[Gram matrix kernel] The kernel of $A$ equals the kernel of $A^\ensuremath{\star} A$. 

\end{proposition}
\textbf{Proof} If $\ensuremath{\bm{\x}} \ensuremath{\in} \hbox{ker}(A)$, i.e., $A \ensuremath{\bm{\x}} = 0$, then clearly $A^\ensuremath{\star}A \ensuremath{\bm{\x}} = 0 = A^\ensuremath{\star} 0 = 0$. On the other hand, if $\ensuremath{\bm{\x}} \ensuremath{\in} \hbox{ker}(A^\ensuremath{\star}A)$ so  that $A^\ensuremath{\star} A \ensuremath{\bm{\x}} = 0$ then we have
\[
0 = \ensuremath{\bm{\x}}^\ensuremath{\star} A^\ensuremath{\star} A \ensuremath{\bm{\x}} = \| A \ensuremath{\bm{\x}} \|^2
\]
which means $A \ensuremath{\bm{\x}} = 0$ and $\ensuremath{\bm{\x}} \ensuremath{\in} \hbox{ker}(A)$. \ensuremath{\QED}

As mentioned in PS6, the spectral theorem states that any normal matrix is unitarily diagonalisable: if $A$ is normal then $A = Q \ensuremath{\Lambda} Q^\ensuremath{\star}$ where $Q \ensuremath{\in} U(n)$ and $\ensuremath{\Lambda}$ is diagonal. In the special case where $A$ is symmetric/Hermitian you would have seen a proof of this in first year. We can use this to ensure that the Gram matrix is diagonalisable:

\begin{proposition}[Gram matrix diagonalisation] The Gram matrix satisfies
\[
A^\ensuremath{\star} A = Q \ensuremath{\Lambda} Q^\ensuremath{\star} \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}
\]
is a Hermitian matrix where $Q \ensuremath{\in} U(n)$ and the eigenvalues $\ensuremath{\lambda}_k$ are real and non-negative. If $A \ensuremath{\in} \ensuremath{\bbR}^{m \ensuremath{\times} n}$ then we can choose $Q \ensuremath{\in} O(n)$.

\end{proposition}
\textbf{Proof} $A^\ensuremath{\star} A$ is Hermitian so we appeal to the spectral theorem for the existence of the decomposition.  To see that the eigenvalues are real and positive  note for the corresponding (orthonormal) eigenvector $\ensuremath{\bm{\q}}_k$ we have
\[
\ensuremath{\lambda}_k = \ensuremath{\lambda}_k \ensuremath{\bm{\q}}_k^\ensuremath{\star} \ensuremath{\bm{\q}}_k = \ensuremath{\bm{\q}}_k^\ensuremath{\star} A^\ensuremath{\star} A \ensuremath{\bm{\q}}_k = \| A \ensuremath{\bm{\q}}_k \|^2 \ensuremath{\geq} 0.
\]
The fact that real $A$  implies $Q \ensuremath{\in} O(n)$ (i.e. has real entries) follows since if $\ensuremath{\bm{\q}}_k \ensuremath{\in} \ensuremath{\bbC}^n$ is an eigenvector than so is $\ensuremath{\mathfrak{R}} \ensuremath{\bm{\q}}_k$:
\[
A^\ensuremath{\top} A \ensuremath{\mathfrak{R}} \ensuremath{\bm{\q}}_k = \ensuremath{\mathfrak{R}} A^\ensuremath{\top} A  \ensuremath{\bm{\q}}_k  = \ensuremath{\mathfrak{R}} \ensuremath{\lambda}_k \ensuremath{\bm{\q}}_k = \ensuremath{\lambda}_k \ensuremath{\mathfrak{R}} \ensuremath{\bm{\q}}_k.
\]
\ensuremath{\QED}

This connection allows us to prove existence:

\begin{theorem}[SVD existence] Every $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ has an SVD.

\end{theorem}
\textbf{Proof} Consider
\[
A^\ensuremath{\star} A = Q \ensuremath{\Lambda} Q^\ensuremath{\star}.
\]
Assume (as usual) that the eigenvalues are sorted in decreasing modulus, and so $\ensuremath{\lambda}_1,\ensuremath{\ldots},\ensuremath{\lambda}_r$ are an enumeration of the non-zero eigenvalues and
\[
V := \begin{bmatrix} \ensuremath{\bm{\q}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_r \end{bmatrix}
\]
the corresponding (orthonormal) eigenvectors, with the columns
\[
K = \begin{bmatrix} \ensuremath{\bm{\q}}_{r+1} | \ensuremath{\cdots} | \ensuremath{\bm{\q}}_n \end{bmatrix}
\]
spanning the kernel of $A^\ensuremath{\star} A$ (and hence $A$).  Define
\[
\ensuremath{\Sigma} :=  \begin{bmatrix} \sqrt{\ensuremath{\lambda}_1} \\ & \ensuremath{\ddots} \\ && \sqrt{\ensuremath{\lambda}_r} \end{bmatrix}
\]
Now define $U := AV \ensuremath{\Sigma}^{-1}$. Since $A^\ensuremath{\star} A V = V \ensuremath{\Sigma}^2$ we can verify that $U$ has orthonormal columns:
\[
U^\ensuremath{\star} U = \ensuremath{\Sigma}^{-1} V^\ensuremath{\star} A^\ensuremath{\star} A V \ensuremath{\Sigma}^{-1} = I.
\]
Thus we have
\[
U \ensuremath{\Sigma} V^\ensuremath{\star} = A V V^\ensuremath{\star} = A \underbrace{\begin{bmatrix} V | K \end{bmatrix}}_Q\underbrace{\begin{bmatrix} V^\ensuremath{\star} \\ K^\ensuremath{\star} \end{bmatrix}}_{Q^\ensuremath{\star}} = A
\]
since $Q Q^\ensuremath{\star} = I$, and where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.

\ensuremath{\QED}

\subsection{The matrix 2-norm}
Somewhat surprisingly, the 2-norm for matrices does not have a simple formula but instead, as we shall show, can be defined in terms of the singular values. We begin with two cases where we do have simple formul\ensuremath{\ae}:

\begin{proposition}[diagonal/orthogonal column 2-norms] If $\ensuremath{\Lambda}$ is diagonal with entries $\ensuremath{\lambda}_k$ then $\|\ensuremath{\Lambda}\| = \max_k |\ensuremath{\lambda}_k|$. If $U$ has orthonormal columns then $\|U\| = \|U^\ensuremath{\star}\| = 1$.

\end{proposition}
\textbf{Proof}

The first property follows from
\[
\| \ensuremath{\Lambda} \ensuremath{\bm{\x}}\| = \sqrt{\ensuremath{\sum}_{k=1}^n |\ensuremath{\lambda}_k|^2 |x_k|^2} \ensuremath{\leq}  \max_k |\ensuremath{\lambda}_k| \| \ensuremath{\bm{\x}} \|
\]
hence $\|\ensuremath{\Lambda}\| \ensuremath{\leq}  \max_k |\ensuremath{\lambda}_k|$. If $k$ is an index where this maximum is achieved we see this upper bound is achieved by $\| \ensuremath{\Lambda}\ensuremath{\bm{\e}}_k\| = |\ensuremath{\lambda}_k|$.

$U$ with orthonormal columns has unit norm because:
\[
 \|U \ensuremath{\bm{\x}}\|^2 = \ensuremath{\bm{\x}}^\ensuremath{\star} \underbrace{U^\ensuremath{\star}U}_I \ensuremath{\bm{\x}} =  \|\ensuremath{\bm{\x}}\|^2.
\]
To show $U^\ensuremath{\star}$ also has unit norm is more challenging. We first note from the spectral theorem we know we can diagonalise
\[
U U^\ensuremath{\star} = Q \ensuremath{\Lambda} Q^\ensuremath{\star}
\]
and therefore 
\[
\| U U^\ensuremath{\star} \| \ensuremath{\leq} \|Q \| \| \ensuremath{\Lambda} \| \| Q^\ensuremath{\star} \| =\| \ensuremath{\Lambda} \|.
\]
We further note this is a projection, that is, it equals its square:
\[
(U U^\ensuremath{\star})^2 = U \underbrace{U^\ensuremath{\star} U}_{I} U^\ensuremath{\star} = U U^\ensuremath{\star}.
\]
Thus the eigenvalues are either 0 or 1 since:
\[
\ensuremath{\lambda}_j = \ensuremath{\lambda}_j \ensuremath{\bm{\q}}_j^\ensuremath{\star} \ensuremath{\bm{\q}}_j = \ensuremath{\bm{\q}}_j^\ensuremath{\star} U U^\ensuremath{\star} \ensuremath{\bm{\q}}_j = \ensuremath{\bm{\q}}_j^\ensuremath{\star} (U U^\ensuremath{\star})^2 \ensuremath{\bm{\q}}_j = \ensuremath{\lambda}_j^2
\]
and thus $\| U U^\ensuremath{\star} \| \ensuremath{\leq} \| \ensuremath{\Lambda}\| \ensuremath{\leq} 1$. We thus have  (using the fact that multiplication by $U$ preserves norm)
\[
\|U^\ensuremath{\star} \ensuremath{\bm{\x}} \| = \|U U^ \ensuremath{\star} \ensuremath{\bm{\x}} \| \ensuremath{\leq} \| U U^\ensuremath{\star}\| \| \ensuremath{\bm{\x}}\| \ensuremath{\leq} \| \ensuremath{\bm{\x}}\|
\]
hence $\| U^\ensuremath{\star}\| \ensuremath{\leq} 1$. But this bound is achieved using any column of $U$ since, eg. for $\ensuremath{\bm{\u}}_1 = U\ensuremath{\bm{\e}}_1$ we have
\[
\sup_{\|\ensuremath{\bm{\v}}\|=1} \| U^\ensuremath{\star} \ensuremath{\bm{\v}}\| \ensuremath{\geq}\| U^\ensuremath{\star} \ensuremath{\bm{\u}}_1 \| = \| \ensuremath{\bm{\e}}_1 \| = 1.
\]
\ensuremath{\QED}

These two facts allow us to deduce the 2-norm from the SVD of a matrix:

\begin{corollary}[singular values and norm]
\[
\|A \| = \ensuremath{\sigma}_1
\]
and if $A \ensuremath{\in} \ensuremath{\bbC}^{n \ensuremath{\times} n}$ is invertible, then
\[
\|A^{-1} \| = \ensuremath{\sigma}_n^{-1}
\]
\end{corollary}
\textbf{Proof}

First we establish the upper-bound using the fact for induced norms that $\|A B\| \ensuremath{\leq} \|A\| \|B\|$ (see appendix):
\[
\|A \| \ensuremath{\leq}  \|U \| \| \ensuremath{\Sigma} \| \| V^\ensuremath{\star}\| = \| \ensuremath{\Sigma} \|  = \ensuremath{\sigma}_1
\]
This is attained using the first right singular vector: for $\ensuremath{\bm{\v}}_1 = V \ensuremath{\bm{\e}}_1$ we have
\[
\|A \ensuremath{\bm{\v}}_1\| = \|\ensuremath{\Sigma} V^\ensuremath{\star} \ensuremath{\bm{\v}}_1\| = \|\ensuremath{\Sigma}  \ensuremath{\bm{\e}}_1\| = \ensuremath{\sigma}_1
\]
The inverse result follows since the inverse has SVD
\[
A^{-1} = V \ensuremath{\Sigma}^{-1} U^\ensuremath{\star} = (V W) (W \ensuremath{\Sigma}^{-1} W) (W U)^\ensuremath{\star}
\]
is the SVD of $A^{-1}$, i.e. $VW \ensuremath{\in} U(n)$ are the left singular vectors and $W U$ are the right singular vectors, where
\[
W := P_\ensuremath{\sigma} = \begin{bmatrix} && 1 \\ & \ensuremath{\adots} \\ 1 \end{bmatrix}
\]
is the permutation that reverses the entries of a vector, that is, $\ensuremath{\sigma}$ has Cauchy notation
\[
\begin{pmatrix}
1 & 2 & \ensuremath{\cdots} & n \\
n & n-1 & \ensuremath{\cdots} & 1
\end{pmatrix}.
\]
\ensuremath{\QED}

\subsection{Best rank-$k$ approximation and compression}
One of the main usages for SVDs is low-rank compression: approximating a (possibly full rank) matrix $A \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} n}$ by a matrix with a much smaller rank $k \ensuremath{\ll} m,n$.

\begin{theorem}[best low rank approximation] The  matrix
\[
A_k := \underbrace{\begin{bmatrix} \ensuremath{\bm{\u}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\u}}_k \end{bmatrix}}_{=: U_k \ensuremath{\in} \ensuremath{\bbC}^{m \ensuremath{\times} k}} \underbrace{\begin{bmatrix}
\ensuremath{\sigma}_1 \\
& \ensuremath{\ddots} \\
&& \ensuremath{\sigma}_k\end{bmatrix}}_{=: \ensuremath{\Sigma}_k \ensuremath{\in} \ensuremath{\bbC}^{k \ensuremath{\times} k}} \underbrace{\begin{bmatrix} \ensuremath{\bm{\v}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\v}}_k \end{bmatrix}^\ensuremath{\star}}_{=: V_k^\ensuremath{\star} \ensuremath{\in} \ensuremath{\bbC}^{k \ensuremath{\times} n}}
\]
is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have 
\[
\ensuremath{\sigma}_{k+1} = \|A - A_k\| \ensuremath{\leq} \|A -B \|.
\]
\end{theorem}
\textbf{Proof} We have
\[
A - A_k = U \begin{bmatrix} 0  \cr &\ddots \cr && 0 \cr &&& \ensuremath{\sigma}_{k+1} \cr &&&& \ddots \cr &&&&& \ensuremath{\sigma}_r\end{bmatrix} V^\ensuremath{\star}
= \begin{bmatrix} \ensuremath{\bm{\u}}_{k+1} | \ensuremath{\cdots} | \ensuremath{\bm{\u}}_r \end{bmatrix}
\begin{bmatrix}
\ensuremath{\sigma}_{k+1} \\
& \ensuremath{\ddots} \\
&& \ensuremath{\sigma}_r \end{bmatrix}\begin{bmatrix} \ensuremath{\bm{\v}}_{k+1} | \ensuremath{\cdots} | \ensuremath{\bm{\v}}_r \end{bmatrix}^\ensuremath{\star}
\]
hence $\| A - A_k \| = \ensuremath{\sigma}_{k+1}$.

Suppose a rank-$k$ matrix $B$ has 
\[
\|A-B\|  < \|A-A_k\| = \ensuremath{\sigma}_{k+1}.
\]
For all $\ensuremath{\bm{\w}} \ensuremath{\in} \ker(B)$ we have 
\[
\|A \ensuremath{\bm{\w}}\| = \|(A-B) \ensuremath{\bm{\w}}\| \ensuremath{\leq} \|A-B\|\|\ensuremath{\bm{\w}}\|  < \ensuremath{\sigma}_{k+1} \|\ensuremath{\bm{\w}}\|
\]
But for all $\ensuremath{\bm{\x}} \ensuremath{\in} {\rm span}(\ensuremath{\bm{\v}}_1,\ensuremath{\ldots},\ensuremath{\bm{\v}}_{k+1})$, that is, $\ensuremath{\bm{\x}} = V[:,1:k+1]\ensuremath{\bm{\c}}$ for some $\ensuremath{\bm{\c}} \ensuremath{\in} \ensuremath{\bbC}^{k+1}$  we have 
\[
\|A \ensuremath{\bm{\x}}\|^2 = \|U \ensuremath{\Sigma}_k \ensuremath{\bm{\c}}\|^2 = \|\ensuremath{\Sigma}_k \ensuremath{\bm{\c}}\|^2 =
\sum_{j=1}^{k+1} (\ensuremath{\sigma}_j c_j)^2 \ensuremath{\geq} \ensuremath{\sigma}_{k+1}^2 \| \ensuremath{\bm{\c}}  \|^2,
\]
i.e., $\|A \ensuremath{\bm{\x}}\| \ensuremath{\geq} \ensuremath{\sigma}_{k+1} \|\ensuremath{\bm{\x}}\|$, where we use the fact that
\[
\|\ensuremath{\bm{\x}}\|^2 = \|V[:,1:k+1]\ensuremath{\bm{\c}}\|^2 = \ensuremath{\bm{\c}}^\ensuremath{\star} V[:,1:k+1]^\ensuremath{\star} V[:,1:k+1]\ensuremath{\bm{\c}} = \ensuremath{\bm{\c}}^\ensuremath{\star}\ensuremath{\bm{\c}} = \|\ensuremath{\bm{\c}}\|^2.
\]
Thus $\ensuremath{\bm{\w}}$ cannot be in this span.

The dimension of the span of $\ker(B)$ is at least $n-k$, but the dimension of ${\rm span}(\ensuremath{\bm{\v}}_1,\ensuremath{\ldots},\ensuremath{\bm{\v}}_{k+1})$ is at least $k+1$. Since these two spaces cannot intersect (apart from at 0) we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  \ensuremath{\QED}

\subsection{Lab and Problem Sheet}
In the lab we explore the relationship between smoothness of a function and the decay in the singular values. We also implement low rank compression and see for samples of some smooth functions that we can massively reduce the data needed to represent the matrix. We also look at the case of the Hilbert matrix
\[
H_n := \begin{bmatrix} 1 & 1/2 & 1/3 & \ensuremath{\cdots} & 1/n \\
                      1/2 & 1/3 & 1/4 & \ensuremath{\cdots} & 1/(n+1) \\
                       1/3 & 1/4 & 1/5 & \ensuremath{\cdots} & 1/(n+2) \\
                       \ensuremath{\vdots} & \ensuremath{\vdots} & \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                       1/n & 1/(n+1) & 1/(n+2) & \ensuremath{\cdots} & 1/(2n-1)
                       \end{bmatrix},
\]
a famous matrix introduced by Hilbert which has a number of applications in approximation theory.

In the problem sheet we see that the SVD can be used to define the \emph{Moore\ensuremath{\endash}Penrose pseudoinverse}, a way of  constructing a notion if an inverse for a rectangular or non-invertible matrix that is consistent with least sqaures.



