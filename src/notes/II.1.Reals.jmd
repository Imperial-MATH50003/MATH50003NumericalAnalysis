# Reals


In this chapter, we introduce  the 
[IEEE Standard for Floating-Point Arithmetic](https://en.wikipedia.org/wiki/IEEE_754).
There are multiplies ways of representing real numbers on a computer, as well as 
the precise behaviour of operations such as addition, multiplication, etc. One can use

1. [Fixed-point arithmetic](https://en.wikipedia.org/wiki/Fixed-point_arithmetic): essentially representing a real number as an integer where a decimal point is inserted at a fixed position. This turns out to be impractical in most applications, e.g., due to loss of relative accuracy for small numbers.
2. [Floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic): essentially scientific notation where an exponent is stored alongside a fixed number of digits. This is what is used in practice.
3. [Level-index arithmetic](https://en.wikipedia.org/wiki/Symmetric_level-index_arithmetic): stores numbers as iterated exponents. This is the most beautiful mathematically but unfortunately is not as useful for most applications and is not implemented in hardware.

Before the 1980s each processor had potentially a different representation for 
floating-point numbers, as well as different behaviour for operations. 
IEEE introduced in 1985 was a means to standardise this across
processors so that algorithms would produce consistent and reliable results.

This chapter may seem very low level for a mathematics course but there are
two important reasons to understand the behaviour of floating-point numbers in details:
1. Floating-point arithmetic is very precisely defined, and can even be used in rigorous computations as we shall see in the labs. But it is not exact and its important to understand how errors in computations can accumulate.
2. Failure to understand floating-point arithmetic can cause catastrophic issues in practice, with the extreme example being the  [explosion of the Ariane 5 rocket](https://youtu.be/N6PWATvLQCY?t=86).


## Real numbers in binary

Reals can also be presented in binary format, that is, a sequence of `0`s and `1`s alongside a decimal point:

**Definition (real binary format)**
For $b_1,b_2,…\in \{0,1\}$, Denote a non-negative real number in _binary format_ by:
$$
(B_p …B_0.b_1b_2b_3…)_2 := (B_p …B_0)_2 +  \sum_{k=1}^∞ {b_k \over 2^k}.
$$
∎

**Example (rational in binary)**
Consider the number `1/3`.  In decimal recall that:
$$
1/3 = 0.3333…=  \sum_{k=1}^∞ {3 \over 10^k}
$$
We will see that in binary
$$
1/3 = (0.010101…)_2 = \sum_{k=1}^∞ {1 \over 2^{2k}}
$$
Both results can be proven using the geometric series:
$$
\sum_{k=0}^∞ z^k = {1 \over 1 - z}
$$
provided $|z| < 1$. That is, with $z = {1 \over 4}$ we verify the binary expansion:
$$
\sum_{k=1}^∞ {1 \over 4^k} = {1 \over 1 - 1/4} - 1 = {1 \over 3}
$$
A similar argument with $z = 1/10$ shows the decimal case.
∎


## Floating-point numbers

Floating-point numbers are a subset of real numbers that are representable using
a fixed number of bits.

**Definition (floating-point numbers)**
Given integers $σ$ (the _exponential shift_), $Q$ (the number of _exponent bits_) and 
$S$ (the _precision_), define the set of
_Floating-point numbers_ by dividing into _normal_, _sub-normal_, and _special number_ subsets:
$$
F_{σ,Q,S} := F^{\rm normal}_{σ,Q,S} \cup F^{\rm sub}_{σ,Q,S} \cup F^{\rm special}.
$$
The _normal numbers_
$F^{\rm normal}_{σ,Q,S} ⊂ ℝ$ are
$$
F^{\rm normal}_{σ,Q,S} := \{\red{±} 2^{\green{q}-σ} × (1.\blue{b_1b_2b_3…b_S})_2 : 1 ≤ q < 2^Q-1 \}.
$$
The _sub-normal numbers_ $F^{\rm sub}_{σ,Q,S} ⊂ ℝ$ are
$$
F^{\rm sub}_{σ,Q,S} := \{\red{±} 2^{\green{1}-σ} × (0.\blue{b_1b_2b_3…b_S})_2\}.
$$
The _special numbers_ $F^{\rm special} ⊄ ℝ$ are 
$$
F^{\rm special} :=  \{∞, -∞, {\rm NaN}\}
$$
where ${\rm NaN}$ is a special symbol representing “not a number", essentially an error flag.
∎

Note this set of real numbers has no nice _algebraic structure_: it is not closed under addition, subtraction, etc.
On the other hand, we can control errors effectively hence it is extremely useful for analysis.

Floating-point numbers are stored in $1 + Q + S$ total number of bits, in the format
$$
\red{s}\ \green{q_{Q-1}…q_0}\ \blue{b_1…b_S}
$$
The first bit ($s$) is the _sign bit_: 0 means positive and 1 means
negative. The bits $q_{Q-1}…q_0$ are the _exponent bits_:
they are the binary digits of the unsigned integer $q$: 
$$
q = (\green{q_{Q-1}…q_0})_2.
$$
Finally, the bits $b_1…b_S$ are the _significand bits_.
If $1 ≤ q < 2^Q-1$ then the bits represent the normal number
$$
x = \red{±} 2^{\green{q}-σ} × (1.\blue{b_1b_2b_3…b_S})_2.
$$
If $q = 0$ (i.e. all bits are 0) then the bits represent the sub-normal number
$$
x = \red{±} 2^{\green{1}-σ} × (0.\blue{b_1b_2b_3…b_S})_2.
$$
If $q = 2^Q-1$  (i.e. all bits are 1) then the bits represent a special number, discussed
later.


## IEEE floating-point numbers

**Definition (IEEE floating-point numbers)** 
IEEE has 3 standard floating-point formats: 16-bit (half precision), 32-bit (single precision) and
64-bit (double precision) defined by (you _do not_ need to memorise these):
$$
\begin{align*}
F_{16} &:= F_{15,5,10} \\
F_{32} &:= F_{127,8,23} \\
F_{64} &:= F_{1023,11,52}
\end{align*}
$$
∎

**Example (interpreting 16-bits as a float)** Consider the number with bits
$$
\red{0}\ \green{10000}\ \blue{1010000000}
$$
assuming it is a half-prevision float ($F_{16}$). 
Since the sign bit is `0` it is positive. The exponent is
$2^4 - σ = 16 - 15 = 1$
Hence this number is:
$$
2^1 (1.1010000000)_2 = 2 (1 + 1/2 + 1/8) = 3+1/4 = 3.25.
$$
∎

**Example (rational to 16-bits)** How is the number $1/3$ stored in $F_{16}$?
Recall that
$$
1/3 = (0.010101…)_2 = 2^{-2} (1.0101…)_2 = 2^{13-15} (1.0101…)_2
$$
and since $13 = (1101)_2$  the exponent bits are `01101`.
For the significand we round the last bit to the nearest element of $F_{16}$, 
(the exact rule for rounding is explained in detail later), so we have
$$
1.010101010101010101010101…\approx 1.0101010101 \in F_{16} 
$$
and the significand bits are `0101010101`.
Thus the stored bits for $1/3$ are:
$$
\red{0}\ \green{01101}\ \blue{0101010101}
$$
∎


## Sub-normal and special numbers

For sub-normal numbers, the simplest example is zero, which has $q=0$ and all significand bits zero:
`0 00000 0000000000`.
Unlike integers, we also have a negative zero, which has bits:
`1 00000 0000000000`.
This is treated as identical to positive `0` (except for degenerate operations as explained in the lab).


**Example (subnormal in 16-bits)** Consider the number with bits
$$
\red{1}\ \green{00000}\ \blue{1100000000}
$$
assuming it is a half-prevision float ($F_{16}$). 
Since all exponent bits are zero it is sub-normal. Since the sign bit is `1` it is negative. 
Hence this number is:
$$
-2^{1-σ} (0.1100000000)_2 = -2^{-14} (2^{-1} + 2^{-2}) = -3 × 2^{-16}
$$
∎

The special numbers extend the real line by adding $±∞$ but also a notion of ``not-a-number" ${\rm NaN}$.
Whenever the bits of $q$ of a floating-point number are all 1 then they represent an element of $F^{\rm special}$.
If all $b_k=0$, then the number represents either $±∞$.
All other special floating-point numbers represent ${\rm NaN}$. 

**Example (special in 16-bits)** The number with bits
$$
\red{1}\ \green{11111}\ \blue{0000000000}
$$
has all exponent bits equal to $1$, and significand bits $0$ and sign bit $1$, hence represents $-∞$. On the other hand,
the number with bits
$$
\red{1}\ \green{11111}\ \blue{0000000001}
$$
has all exponent bits equal to $1$ but does not have all significand bits equal to $0$, hence is one of many representations
for  ${\rm NaN}$.
∎