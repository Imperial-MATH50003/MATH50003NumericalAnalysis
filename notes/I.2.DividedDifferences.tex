
\section{Divided Differences}
Given a function, how can we approximate its derivative at a point? We consider an intuitive approach to this problem using \emph{(Right-sided) Divided Differences}: 
\[
f'(x) \ensuremath{\approx} {f(x+h) - f(x) \over h}
\]
Note by the definition of the derivative we know that this approximation will converge to the true derivative as $h \ensuremath{\rightarrow} 0$. But in numerical approimxations we also need to consider the rate of convergence. 

Now in the previous section I mentioned there are three basic tools in analysis:  (1) integration-by-parts, (2) geometric series or (3) Taylor series. In this case we use (3):

\begin{proposition}[divided differences error] Suppose that $f$ is twice-differentiable on the interval $[x,x+h]$. The error in approximating the derivative using divided differences is
\[
f'(x) = {f(x+h) - f(x) \over h} + \ensuremath{\delta}
\]
where $|\ensuremath{\delta}| \ensuremath{\leq} Mh/2$ for  $M = \sup_{x \ensuremath{\leq} t \ensuremath{\leq} x+h} |f''(t)|$.

\end{proposition}
\textbf{Proof} Follows immediately from Taylor's theorem: recall that
\[
f(x+h) = f(x) + f'(x) h + {f''(t) \over 2} h^2
\]
for some $t \ensuremath{\in} [x,x+h]$. Rearranging we get
\[
f'(x) = {f(x+h) - f(x) \over 2} + \underbrace{\left(-{f''(t)\over 2 h^2} \right)}_\ensuremath{\delta}.
\]
We then bound:
\[
|\ensuremath{\delta}| \ensuremath{\leq} \abs{{f''(t) \over 2} h} \ensuremath{\leq} {M  h \over 2}.
\]
\ensuremath{\QED}

Unlike the rectangular rule, the computational cost of computing the divided difference is independent of $h$! We only need to evaluate a function $f$ twice and do a single division. Here we are assuming that the computational cost of evaluating $f$ is independent of the point of evaluation. Later we will investigate the details of how computers work with numbers via floating point,  and confirm that this is a sensible assumption.

So why not just set $h$ ridiculously small? In the lab we explore this question and observe that there are significant errors introduced in the numerical realisation of this algorithm. We will return to the question of understanding these errors after learning floating point numbers. 

There are alternative versions of divided differences. Left-side divided differences evaluates to the left of the point where we wish to know the derivative:
\[
f'(x) \ensuremath{\approx} {f(x) - f(x-h) \over h}
\]
and central differences:
\[
f'(x) \ensuremath{\approx} {f(x + h) - f(x - h) \over 2h}
\]
We can further arrive at an approximation to the second derivative by composing a left- and right-sided finite difference:
\[
f''(x) \ensuremath{\approx} {f'(x+h) - f'(x) \over h} \ensuremath{\approx} {{f(x+h) - f(x) \over h} - {f(x) - f(x-h) \over h} \over h}
= {f(x+h) - 2f(x)  + f(x-h) \over h^2}
\]
In the lab we investigate the convergence rate of these approximations (in particular, that  central differences is more accurate than standard divided differences) and observe that they too suffer from unexplained (for now) loss of accuracy as $h \ensuremath{\rightarrow} 0$. In the problem sheet we prove the theoretical convergence rate, which is never realised because of these errors.

\subsection{Lab and problem sheet}
In the lab, we explore the practical implementation of the right-sided divided differences and extensions to   central differences and second-order divided differences.   We  see  \emph{experimentally} that the central differences converges much faster  to the true value of the derivative as $h$ becomes moderately small. However, we observe a serious issue: if $h$ becomes too small, the eror mysteriously starts growing extremely large, and hence these rules do not actually converge at all to the true value of the derivatives! 

The problem sheet explores the \emph{analysis} of these different rules, proving the precise theoretical convergence rates observed for moderately small $h$. This presents a bit of a contradiction: why does the theory say the method converges but in practice it diverges, and spectacularly so! This is a mystery that we will return to later.



