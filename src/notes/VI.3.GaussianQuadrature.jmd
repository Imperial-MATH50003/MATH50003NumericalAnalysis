# Gaussian Quadrature


Quadrature is another name for numerical integration, and we have already seen examples such as the Rectangular and Trapezium Rules.
In this section we see that a special quadrature rule can be constructed by using the roots of orthogonal
polynomials, leading to a method that is exact for polynomials of twice the expected degree.
Importantly, we can use quadrature to compute expansions in orthogonal polynomials that
interpolate,  mirroring the link between the Trapezium rule, Fourier series, and interpolation but
now for orthogonal polynomials.



**Example (Chebyshev roots)** We now find the interpolatory quadrature rule for $w(x) = 1/\sqrt{1-x^2}$ on $[-1,1]$ with points equal to the
roots of $T_3(x)$.
 This is a special case of Gaussian quadrature which we will approach in another way below. We use:
$$
\int_{-1}^1 w(x) {\rm d}x = π, \int_{-1}^1 xw(x) {\rm d}x = 0, \int_{-1}^1 x^2 w(x) {\rm d}x = {π \over 2}.
$$
 From the 3-term recurrence we deduce
$$
T_0(x) = 1, T_1(x) =x, T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1, T_3(x) = 2x T_2(x) - T_1(x) = 4x^3-3x
$$
hence we find the 3 roots of $T_3(x)$ are $x_1,x_2,x_3 = \sqrt{3}/2,0,-\sqrt{3}/2$. Thus we have:
$$
\begin{align*}
w_1 = \int_{-1}^1 w(x) ℓ_1(x) {\rm d}x = \int_{-1}^1 {x(x+\sqrt{3}/2) \over (\sqrt{3}/2) \sqrt{3} \sqrt{1-x^2}}{\rm d}x = {π \over 3} \\
w_2 = \int_{-1}^1 w(x) ℓ_2(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2)(x+\sqrt{3}/2) \over (-3/4)\sqrt{1-x^2}}{\rm d}x = {π \over 3} \\
w_3 = \int_{-1}^1 w(x) ℓ_3(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2) x \over (-\sqrt{3})(-\sqrt{3}/2) \sqrt{1-x^2}}{\rm d}x = {π \over 3}
\end{align*}
$$
(It's not a coincidence that they are all the same but this will differ for roots of other OPs.) 
That is we have
$$
Σ_n^{w,𝐱}[f]  = {π \over 3}\br[ f(\sqrt{3}/2) + f(0) + f(-\sqrt{3}/2) ].
$$
This is indeed exact for polynomials up to degree $n-1=2$, but it goes all the way up to $2n-1 = 5$:
$$
\begin{align*}
Σ_n^{w,𝐱}[1] &= π, Σ_n^{w,𝐱}[x] = 0, Σ_n^{w,𝐱}[x^2] = {π \over 2}, \\
Σ_n^{w,𝐱}[x^3] &= 0, Σ_n^{w,𝐱}[x^4] &= {3 π \over 8}, Σ_n^{w,𝐱}[x^5] = 0 \\
Σ_n^{w,𝐱}[x^6] &= {9 π \over 32} ≠ {5 π \over 16}
\end{align*}
$$
We shall explain this miracle in the rest of this section.
∎



## Roots of orthogonal polynomials and truncated Jacobi matrices

We now consider roots (zeros) of orthogonal polynomials $p_n(x)$ which will be essential
to constructing Gaussian quadrature, via interpolation at these points. For interpolation to be well-defined we
first need to guarantee that the roots are distinct.

**Lemma (OP roots)** An orthogonal polynomial $p_n(x)$ has exactly $n$ distinct roots.

**Proof**

Suppose $x_1, …,x_j$ are the roots where $p_n(x)$ changes sign, i.e., the order of the root must be odd and hence
$$
p_n(x) = c_k (x-x_k)^{2p+1} + O((x-x_k)^{2p+2})
$$
for $c_k ≠ 0$ and $k = 1,…,j$ and $p ∈ ℤ$, as $x → x_k$. Then
$$
p_n(x) (x-x_1) ⋯(x-x_j)
$$
does not change signs: it behaves like $c_k (x-x_k)^{2p+2} + O(x-x_k)^{2p+3}$ as $x → x_k$.
In other words:
$$
⟨p_n,(x-x_1) ⋯(x-x_j) ⟩ = \int_a^b p_n(x) (x-x_1) ⋯(x-x_j) w(x) {\rm d} x ≠ 0.
$$
where $w(x)$ is the weight of orthogonality.
This is only possible if $j = n$ as $p_n(x)$ is orthogonal w.r.t. all lower degree
polynomials and hence otherwise this integral would be zero. Since $p_n(x)$ is exactly degree $n$
it follows each root must be first order and hence distinct.

∎

We will now relate these roots to truncations of Jacobi matrices.

**Definition (truncated Jacobi matrix)** Given a Jacobi matrix $J$
associated with a family of orthonormal polynomials,
 the _truncated Jacobi matrix_ is
$$
J_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & ⋱ & ⋱ \\
                         & ⋱ & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} ∈ ℝ^{n × n}
$$
∎


**Lemma (OP roots and Jacobi matrices)** The zeros $x_1, …,x_n$ of an orthonormal polynomial $q_n(x)$
are the eigenvalues of the truncated Jacobi matrix $J_n$.
More precisely,
$$
J_n Q_n = Q_n \begin{bmatrix} x_1 \\ & ⋱ \\ && x_n \end{bmatrix}
$$
for the orthogonal matrix
$$
Q_n = \underbrace{\begin{bmatrix}
q_0(x_1) & ⋯ & q_0(x_n) \\
⋮  & ⋯ & ⋮  \\
q_{n-1}(x_1) & ⋯ & q_{n-1}(x_n)
\end{bmatrix}}_{V_n^⊤} \begin{bmatrix} α_1^{-1} \\ & ⋱ \\ && α_n^{-1} \end{bmatrix}
$$
where $α_j = \sqrt{q_0(x_j)^2 + ⋯ + q_{n-1}(x_j)^2}$.

**Proof**

We construct the eigenvector (noting $b_{n-1} q_n(x_j) = 0$):
$$
J_n \begin{bmatrix} q_0(x_j) \\ ⋮ \\ q_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 q_0(x_j) + b_0 q_1(x_j) \\
 b_0 q_0(x_j) + a_1 q_1(x_j) + b_1 q_2(x_j) \\
⋮ \\
b_{n-3} q_{n-3}(x_j) + a_{n-2} q_{n-2}(x_j) + b_{n-2} q_{n-1}(x_j) \\
b_{n-2} q_{n-2}(x_j) + a_{n-1} q_{n-1}(x_j) + b_{n-1} q_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} q_0(x_j) \\
 q_1(x_j) \\
⋮ \\
q_{n-1}(x_j)
\end{bmatrix}
$$
The spectral theorem guarantees that all symmetric matrices have an orthogonal eigenvector matrix. 
That is, by scaling the columns of the eigenvectors we know there must exist $α_j$ so that 
$$
Q_n = \underbrace{\begin{bmatrix}
q_0(x_1) & ⋯ & q_0(x_n) \\
⋮  & ⋯ & ⋮  \\
q_{n-1}(x_1) & ⋯ & q_{n-1}(x_n)
\end{bmatrix}}_{V_n^⊤} \begin{bmatrix} α_1^{-1} \\ & ⋱ \\ && α_n^{-1} \end{bmatrix}
$$
is orthogonal. We choose $α_j$ so that
$$
𝐞_j^⊤ Q_n^⊤ Q_n 𝐞_j = {∑_{k=0}^{n-1} q_k(x_j)^2 \over α_j^2} = 1.
$$

∎

**Example (Chebyshev roots)** Consider $T_n(x) = \cos n {\rm acos}\, x$. The roots 
are $x_j = \cos θ_j$ where $θ_j = (j-1/2)π/n$ for $j = 1,…,n$ are the roots of $\cos n θ$
that are inside $[0,π]$. 

Consider the $n = 3$ case where we have
$$
x_1,x_2,x_3 = \cos(π/6),\cos(π/2),\cos(5π/6) = \sqrt{3}/2,0,-\sqrt{3}/2
$$
We also have from the 3-term recurrence:
$$
\meeq{
T_0(x) = 1, \ccr
T_1(x) = x, \ccr
T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1, \ccr
T_3(x) = 2x T_2(x) - T_1(x) = 4x^3-3x.
}
$$
As determined as part of the problem sheet, we orthonormalise by rescaling
$$
\begin{align*}
q_0(x) &= 1/\sqrt{π}, \\
q_k(x) &= T_k(x) \sqrt{2}/\sqrt{π}.
\end{align*}
$$
so that the Jacobi matrix is symmetric:
$$
x [q_0(x)|q_1(x)|⋯] = [q_0(x)|q_1(x)|⋯] \underbrace{\begin{bmatrix} 0 & 1/\sqrt{2} \\
                            1/\sqrt{2} & 0 & 1/2 \\
                            &1/2 & 0 & 1/2 \\
                             &   & 1/2 & 0 & ⋱ \\
                              &  && ⋱ & ⋱
\end{bmatrix}}_J
$$
We can then confirm that we have constructed an eigenvector/eigenvalue of the $3 × 3$ truncation of the Jacobi matrix,
e.g. at $x_2 = 0$:
$$
\begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} q_0(0) \\ q_1(0) \\ q_2(0) 
    \end{bmatrix} = {1 \over \sqrt π} \begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ -{\sqrt{2}}
    \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 0
    \end{bmatrix}.
$$
∎

## Gaussian quadrature

We now introduce Gaussian quadrature, which
we shall see is exact for polynomials up to degree $2n-1$, i.e., double
the degree of other interpolatory quadrature rules from other grids.
We will also prove that it is in fact an interpolatory quadrature rule corresponding
to the grid $x_j$ defined as the roots of the orthonormal polynomial $q_n(x)$.




**Definition (Gaussian quadrature)** Given a weight $w(x)$, the Gauss quadrature rule is:
$$
∫_a^b f(x)w(x) {\rm d}x ≈ \underbrace{∑_{j=1}^n w_j f(x_j)}_{Σ_n^w[f]}
$$
where $x_1,…,x_n$ are the roots of the orthonormal polynomials $q_n(x)$ and 
$$
w_j := {1 \over α_j^2} = {1 \over q_0(x_j)^2 + ⋯ + q_{n-1}(x_j)^2}.
$$
Equivalentally, $x_1,…,x_n$ are the eigenvalues of $J_n$ and $w_j$ can be written in terms of the eigenvectors
and the integral of the weight:
$$
w_j = ∫_a^b w(x) {\rm d}x \underbrace{q_0(x_j)^2/α_j^2}_{Q_n[1,j]^2}.
$$
∎

In analogy to how Fourier series are orthogonal with respect to the Trapezium rule,
Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

**Lemma (Discrete orthogonality)**
For $0 ≤ ℓ,m ≤ n-1$, the orthonormal polynomials $q_n(x)$ satisfy
$$
Σ_n^w[q_ℓ q_m] = δ_{ℓm}
$$

**Proof**
$$
Σ_n^w[q_ℓ q_m] = ∑_{j=1}^n {q_ℓ(x_j) q_m(x_j) \over α_j^2}
= \left[{q_ℓ(x_1)\over α_1} | ⋯ | {q_ℓ(x_n) \over α_n}\right] 
\begin{bmatrix}
q_m(x_1)/α_1 \\
⋮ \\
q_m(x_n)/α_n \end{bmatrix} = 𝐞_ℓ^⊤ Q_n Q_n^⊤ 𝐞_m = δ_{ℓm}
$$

∎

Just as approximating Fourier coefficients using Trapezium rule gives a way of
interpolating at the grid, so does Gaussian quadrature:

**Theorem (interpolation via quadrature)**
For the orthonormal polynomials $q_n(x)$,
$$
f_n(x) := ∑_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := Σ_n^w[f q_k]
$$
interpolates $f(x)$ at the Gaussian quadrature points $x_1,…,x_n$.

**Proof**
Consider the Vandermonde-like matrix from above:
$$
V_n := \begin{bmatrix} q_0(x_1) & ⋯ & q_{n-1}(x_1) \\
                ⋮ & ⋱ & ⋮ \\
                q_0(x_n) & ⋯ & q_{n-1}(x_n) \end{bmatrix}
$$
and define
$$
Q_n^w := V_n^⊤ \begin{bmatrix} w_1 \\ &⋱ \\&& w_n \end{bmatrix} = \begin{bmatrix} q_0(x_1)w_1 & ⋯ &  q_0(x_n) w_n \\
                ⋮ & ⋱ & ⋮ \\
                q_{n-1}(x_1) w_1 & ⋯ & q_{n-1}(x_n)w_n \end{bmatrix}
$$
so that
$$
\begin{bmatrix}
c_0^n \\
⋮ \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix} f(x_1) \\ ⋮ \\ f(x_n) \end{bmatrix}.
$$
Note that if $p(x) = [q_0(x) | ⋯ | q_{n-1}(x)] 𝐜$ then
$$
\begin{bmatrix}
p(x_1) \\
⋮ \\
p(x_n)
\end{bmatrix} = V_n 𝐜
$$
But we see that (similar to the Fourier case)
$$
Q_n^w V_n = \begin{bmatrix} Σ_n^w[q_0 q_0] & ⋯ & Σ_n^w[q_0 q_{n-1}]\\
                ⋮ & ⋱ & ⋮ \\
                Σ_n^w[q_{n-1} q_0] & ⋯ & Σ_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I
$$
and hence $V_n^{-1} = Q_n^w$ and we have
$$
f_n(x_j) = [q_0(x_j) | ⋯ | q_{n-1}(x_j)] Q_n^w \Vectt[f(x_1),⋮,f(x_n)] = 𝐞_j^⊤ V_n  Q_n^w \Vectt[f(x_1),⋮,f(x_n)] = f(x_j).
$$

∎


**Example (Chebyshev expansions)** 
Consider the construction of Gaussian quadrature associated with the Chebyshev weight for $n = 3$. 
To determine the weights we need we compute
$$
w_j^{-1} = α_j^2 = q_0(x_j)^2 + q_1(x_j)^2 + q_2(x_j)^2 = 
{1 \over π} + {2 \over π} x_j^2 + {2 \over π} (2x_j^2-1)^2
$$
We can check each case and deduce that $w_j = π/3$.
Thus we recover the interpolatory quadrature rule.
Further, we can construct the transform
$$
\begin{align*}
Q_3^w &= \begin{bmatrix}
w_1 q_0(x_1) & w_2 q_0(x_2) & w_3 q_0(x_3) \\
w_1 q_1(x_1) & w_2 q_1(x_2) & w_3 q_1(x_3) \\
w_1 q_2(x_1) & w_2 q_2(x_2) & w_3 q_2(x_3) 
\end{bmatrix}\\
&= {π \over 3} \begin{bmatrix} 1/\sqrt{π} & 1/\sqrt{π} & 1/\sqrt{π} \\
                                x_1\sqrt{2/π} & x_2\sqrt{2/π} & x_3\sqrt{2/π} \\
                                (2x_1^2-1)\sqrt{2/π} &(2x_2^2-1)\sqrt{2/π} & (2x_3^2-1)\sqrt{2/π}
                                \end{bmatrix} \\
                                &= 
                                {\sqrt{π} \over 3} \begin{bmatrix} 1 & 1 & 1 \\
                                \sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
                                1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
                                \end{bmatrix}
\end{align*}
$$
We can use this to expand a polynomial, e.g. $x^2$:
$$
Q_3^w \begin{bmatrix}
x_1^2 \\
x_2^2 \\
x_3^2 
\end{bmatrix} = {\sqrt{π} \over 3} 
\begin{bmatrix} 1 & 1 & 1 \\
\sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
\end{bmatrix} 
\begin{bmatrix} 3/4 \\ 0 \\ 3/4 \end{bmatrix} =
\begin{bmatrix}
{\sqrt{π} / 2} \\
0 \\
{\sqrt{π} / (2\sqrt{2})}
\end{bmatrix}
$$
In other words:
$$
x^2 = {\sqrt π \over 2} q_0(x) + {\sqrt π \over 2\sqrt 2} q_2(x) = {1 \over 2} T_0(x) + {1 \over 2} T_2(x)
$$
which can be easily confirmed.
∎



**Corollary (Gaussian quadrature is interpolatory)** Gaussian quadrature is an interpolatory quadrature rule
with the interpolation points equal to the roots of $q_n$:
$$
Σ_n^w[f] = ∫_a^b f_n(x) w(x) {\rm d}x.
$$
**Proof**
We want to show that its the same as integrating the interpolatory polynomial:
$$
\int_a^b f_n(x) w(x) {\rm d}x = {1 \over q_0(x)} \sum_{k=0}^{n-1} c_k^n \int_a^b q_k(x) q_0(x) w(x) {\rm d}x
= {c_0^n \over q_0} = Σ_n^w[f].
$$
∎


A consequence of being an interpolatory quadrature rule is that it is exact for all
polynomials of degree $n-1$. The _miracle_ of Gaussian quadrature is it is exact for twice
as many!



**Theorem (Exactness of Gauss quadrature)** If $p(x)$ is a degree $2n-1$ polynomial then
Gauss quadrature is exact:
$$
∫_a^b p(x)w(x) {\rm d}x = Σ_n^w[p].
$$

**Proof**
Using polynomial division algorithm (e.g. by matching terms) we can write
$$
p(x) = q_n(x) s(x) + r(x)
$$
where $s$ and $r$ are degree $n-1$ and $q_n(x)$ is the degree $n$ orthonormal polynomial. Because
Gauss quadrature is interpolatory we know that it is exact for degree $n-1$ polynomials, in particular:
$$
Σ_n^w[r] = ∫_a^b r(x) w(x) {\rm d}x.
$$
But then we find that
$$
\begin{align*}
Σ_n^w[p] &= \underbrace{Σ_n^w[q_n s]}_{\hbox{$0$ since evaluating $q_n$ at zeros}} + Σ_n^w[r] = ∫_a^b r(x) w(x) {\rm d}x\\
&= \underbrace{∫_a^b q_n(x)s(x) w(x) {\rm d}x}_{\hbox{$0$ since $s$ is degree $<n$}}  + ∫_a^b r(x) w(x) {\rm d}x \\
&= ∫_a^b p(x)w(x) {\rm d}x.
\end{align*}
$$
∎


**Example (Double exactness)**
Let's look at an example in completeness for $n = 3$ with uniform weight on $[-1,1]$.
From the 3-term recurrence for Legendre polynomials we get the multiplication matrix
$$
x [P_0(x) | P_1(x) | ⋯ ] = [P_0(x) | P_1(x) | ⋯ ] \underbrace{\begin{bmatrix} 0 & 1/3\\
                                1 & 0 & 2/5 \\
                                    &2/3 & 0 & 3/7 \\
                                    && 3/5 & 0 & ⋱ \\
                                    &&& ⋱ & ⋱
                                    \end{bmatrix}}_X
$$
From this we deduce that
$$
\meeq{
P_0(x) = 1 \ccr
P_1(x) = x \ccr
P_2(x) = 3/2 x P_1(x) - P_0(x)/2 = {3x^2 \over 2} - {1 \over 2} \ccr
P_3(x) = 5/3 x P_2(x) - 2P_1(x)/3 = {5 x^2 \over 2} - {3 x \over 2}.
}
$$
The roots of $P_3(x)$ are
$$
x_1,x_2,x_3 = -\sqrt{3/5}, 0, \sqrt{3/5}.
$$


We know the first orthonormal polynomial is $q_0(x) = 1/\sqrt{2}$, i.e., $k_0 = 1/\sqrt{2}$. We write
$$
[q_0(x) | q_1(x) | ⋯ ] = [P_0(x) | P_1(x) | ⋯ ] \underbrace{\begin{bmatrix} 1/\sqrt{2} \\ & k_1 \\ && k_2 \\ &&&⋱ \end{bmatrix}}_K
$$
Thus from
$$
x [q_0(x) | q_1(x) | ⋯ ] = [q_0(x) | q_1(x) | ⋯ ] \underbrace{K^{-1} X K}_J
$$
we find that
$$
J = \begin{bmatrix} 0 & \sqrt{2}k_1/3 \\
                                1/(\sqrt{2}k_1) & 0 & 2k_2/(5k_1) \\
                                    &2k_1/(3k_2) & 0 & 3k_3/(7k_2) \\
                                    && 3k_2/(5k_3) & 0 & ⋱ \\
                                    &&& ⋱ & ⋱
                                    \end{bmatrix}
$$
For this to be symmetric we find
$$
\meeq{
k_1 = \sqrt{3/2} \ccr
k_2 = \sqrt{10k_1^2/6} = \sqrt{5/2} \ccr
k_3 = \sqrt{21k_2^2/15} = \sqrt{21/6}
}
$$

We thus get the quadrature weights
$$
\meeq{
w_1 = α_1^{-2} = {1 \over q_0(x_1)^2 + q_1(x_1)^2 + q_2(x_1)^2} = {1 \over 1/2 + (3/2) × (3/5) + (5/2) × (4/25)} = {5 \over 9} \ccr
w_2 = α_2^{-2} = {1 \over q_0(x_2)^2 + q_1(x_2)^2 + q_2(x_2)^2} = {1 \over 1/2 + (5/2) × (1/4)} = {8 \over 9} \ccr
w_3 = w_1 = {5 \over 9}.
}
$$
Thus our Gauss–Legendre quadrature formula is
$$
Σ_3^w[f] = {5 \over 9} f(-\sqrt{3/5}) + {8 \over 9} f(0) + {5 \over 9} f(\sqrt{3/5}).
$$

We are exact for all polynomials up to degree $2n-1 = 5$:
$$
\meeq{
Σ_3^w[1] = {5 \over 9} + {8 \over 9} + {5 \over 9} = 2 \ccr
Σ_3^w[x] = -{5 \over 9} \sqrt{3/5} + {5 \over 9} \sqrt{3/5} = 0 \ccr
Σ_3^w[x^2] = {5 \over 9} {3 \over 5} + {5 \over 9} {3 \over 5} = {2 \over 3} \ccr
Σ_3^w[x^3] = -{5 \over 9} (3/5)^{3/2} + {5 \over 9} (3/5)^{3/2} = 0 \ccr
Σ_3^w[x^4] = {5 \over 9} {9 \over 25} + {5 \over 9} {9 \over 25} = {2 \over 5} \ccr
Σ_3^w[x^5] = 0.
}
$$
But the next integral is wrong:
$$
Σ_3^w[x^6] = {5 \over 9} {27 \over 125} + {5 \over 9} {27 \over 125} = {6 \over 25} ≠ {2 \over 7} = ∫_{-1}^1 x^6 {\rm d}x.
$$
∎

Going beyond polynomials,  Gaussian quadrature
achieves faster than algebraic convergence for any smooth function.
 If the function is analytic in a neighbourhood of the support of the interval this
is in fact exponential convergence, far exceeding the convergence rate observed for rectangular and Trapezium rules. 
This is a beautiful example of more sophisticated mathematics leading to powerful numerical methods. 