# QR Factorisation

Let $A âˆˆ â„‚^{m Ã— n}$ be a rectangular or square matrix such that $m â‰¥ n$ (i.e. more rows then columns).
In this section we consider two closely related factorisations, which can be viewed in some sense as a competitor to the PLU factorisation,
but one that extends to rectangular matrices and allows for the solution of least squares problems.

The QR factorisation consists of writing $A$ as a product of a (square)
_orthogonal_ and a (rectangular) _right triangular_ matrix:


**Definition (QR factorisation)** The _QR factorisation_ is
$$
A = Q R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_m \end{bmatrix}}_{Q âˆˆ U(m)} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã— \\ &&0 \\ &&â‹® \\ && 0 \end{bmatrix}}_{R âˆˆ â„‚^{m Ã— n}}
$$
where $Q$ is unitary (i.e., $Q âˆˆ U(m)$, satisfying $Q^â‹†Q = I$, with columns $ğª_j âˆˆ â„‚^m$) and $R$ is _right triangular_, which means it 
is only nonzero on or to the right of the diagonal ($r_{kj} = 0$ if $k > j$).
âˆ


It is often more convenient to work with an alternative version known as the reduced QR factorisation, which is a product of
a (rectangular) matrix with orthonormal columns and a (square) upper triangular matrix:

**Definition (Reduced QR factorisation)** The _reduced QR factorisation_
$$
A = \hat Q \hat R = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} Ã— & â‹¯ & Ã— \\ & â‹± & â‹® \\ && Ã—  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$
where $\hat Q$ has orthonormal columns ($\hat Q^â‹† \hat Q = I$, $ğª_j âˆˆ â„‚^m$) and $\hat R$ is upper triangular.
âˆ

Note for a square matrix the reduced QR factorisation is equivalent to the QR factorisation, in which case $R$ is _upper triangular_.
The importance of these factorisation for square matrices is that their component pieces are easy to invert:
$$
A = QR \qquad â‡’ \qquad A^{-1}ğ› = R^{-1} Q^âŠ¤ ğ›
$$
and we saw previously that triangular and orthogonal matrices are easy to invert when applied to a vector $ğ›$.
On the other hand, in the rectangular case the QR factorisation contains within it the reduced QR factorisation:
$$
A = QR = \begin{bmatrix} \hat Q | ğª_{n+1} | â‹¯ | ğª_m \end{bmatrix} \begin{bmatrix} \hat R \\  ğŸ_{m-n Ã— n} \end{bmatrix} = \hat Q \hat R.
$$


For rectangular matrices we will see that the QR factorisation leads to efficient solutions to the _least squares problem_: find
$ğ±$ that minimizes the 2-norm $\| A ğ± - ğ› \|.$




In this section we discuss the following:

1. Reduced QR and Gramâ€“Schmidt: The Reduced QR factorisation is equivalent to the Gramâ€“Schmidt procedure, which you may have seen in 1st year or in the 1st half of the module.
2. Householder reflections and QR: Alternatively, the  QR factorisation can be computed using a sequence of Householder reflections. This process mimics that of the LU factorisation, but using orthogonal matrices in-place of lower triangular ones to introduces zeros in column-by-column.  This is a more accurate approach for computing QR factorisations than Gramâ€“Schmidt as applying orthogonal matrices is stable.
3. QR and least squares: We discuss the QR factorisation and its usage in solving least squares problems.


## Reduced QR and Gramâ€“Schmidt


How do we compute the QR factorisation? We begin with a method
you may have seen before in another guise. Write
$$
A = \begin{bmatrix} ğš_1 | â‹¯ | ğš_n \end{bmatrix}
$$
where $ğš_k âˆˆ  â„‚^m$ and assume they are linearly independent ($A$ has full column rank).


**Proposition (Column spaces match)** Suppose $A = \hat Q  \hat R$ where $\hat Q = [ğª_1|â€¦|ğª_n]$
has orthonormal columns and $\hat R$ is upper-triangular, and $A$ has full rank.
Then the first $j$ columns of
$\hat Q$ span the same space as the first $j$ columns of $A$:
$$
\hbox{span}(ğš_1,â€¦,ğš_j) = \hbox{span}(ğª_1,â€¦,ğª_j).
$$

**Proof**

Because $A$ has full rank we know $\hat R$ is invertible, i.e. its diagonal entries do not vanish: $r_{jj} â‰ Â 0$.
If $ğ¯ âˆˆ \hbox{span}(ğš_1,â€¦,ğš_j)$ we have for $ğœ âˆˆ â„‚^j$
$$
ğ¯ = \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} ğœ = 
\begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix}  \hat R[1:j,1:j] ğœ âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)
$$
 while if $ğ° âˆˆ \hbox{span}(ğª_1,â€¦,ğª_j)$ we have for $\vc d âˆˆ â„^j$
$$
ğ° = \begin{bmatrix} ğª_1 | â‹¯ | ğª_j \end{bmatrix} \vc d  =  \begin{bmatrix} ğš_1 | â‹¯ | ğš_j \end{bmatrix} \hat R[1:j,1:j]^{-1} \vc d âˆˆ  \hbox{span}(ğš_1,â€¦,ğš_j).
$$

âˆ

 
It is possible to find $\hat Q$ and $\hat R$   using the _Gramâ€“Schmidt algorithm_.
We construct it column-by-column. For $j = 1, 2, â€¦, n$ define
$$
\begin{align*}
ğ¯_j &:= ğš_j - âˆ‘_{k=1}^{j-1} \underbrace{ğª_k^â‹† ğš_j}_{r_{kj}} ğª_k, \\
r_{jj} &:= {\|ğ¯_j\|}, \\
ğª_j &:= {ğ¯_j \over r_{jj}}.
\end{align*}
$$

**Theorem (Gramâ€“Schmidt and reduced QR)** Define $ğª_j$ and $r_{kj}$ as above
(with $r_{kj} = 0$ if $k > j$). Then a reduced QR factorisation is given by:
$$
A = \underbrace{\begin{bmatrix} ğª_1 | â‹¯ | ğª_n \end{bmatrix}}_{ \hat Q âˆˆ â„‚^{m Ã— n}} \underbrace{\begin{bmatrix} r_{11} & â‹¯ & r_{1n} \\ & â‹± & â‹® \\ && r_{nn}  \end{bmatrix}}_{\hat R âˆˆ â„‚^{n Ã— n}}
$$

**Proof**

We first show that $\hat Q$ has orthonormal columns. Assume that $ğª_â„“^â‹† ğª_k = Î´_{â„“k}$ for $k,â„“ < j$. 
For $â„“ < j$ we then have
$$
ğª_â„“^â‹† ğ¯_j = ğª_â„“^â‹† ğš_j - âˆ‘_{k=1}^{j-1}  ğª_â„“^â‹†ğª_k ğª_k^â‹† ğš_j = 0
$$
hence $ğª_â„“^â‹† ğª_j = 0$ and indeed $\hat Q$ has orthonormal columns. Further: from the definition of $ğ¯_j$ we find
$$
ğš_j = ğ¯_j + âˆ‘_{k=1}^{j-1} r_{kj} ğª_k = âˆ‘_{k=1}^j r_{kj} ğª_k  = \hat Q \hat R ğ_j
$$

âˆ

## Householder reflections and QR

As an alternative, we will consider using Householder reflections to introduce zeros below
the diagonal.
Thus, if Gramâ€“Schmidt is a process of _triangular orthogonalisation_ (using triangular matrices
to orthogonalise), Householder reflections is a process of _orthogonal triangularisation_ 
(using orthogonal matrices to triangularise).

Consider multiplication by the Householder reflection corresponding to the first column,
that is, for
$$
Q_1 := Q_{ğš_1}^{\rm H},
$$
consider
$$
Q_1 A = \begin{bmatrix} Ã— & Ã— & â‹¯ & Ã— \\
& Ã— & â‹¯ & Ã— \\
                    & â‹® & â‹± & â‹® \\
                    & Ã— & â‹¯ & Ã— \end{bmatrix} = 
\begin{bmatrix}  Î±_1 & ğ°_1^âŠ¤ \\ 
& A_2   \end{bmatrix}
$$
where 
$$
Î±_1 := -{\rm csign}(a_{11})  \|ğš_1\|, ğ°_1 = (Q_1 A)[1, 2:n]  \qquad \hbox{and} \qquad A_2 = (Q_1 A)[2:m, 2:n],
$$
where as before ${\rm csign}(z) :=  {\rm e}^{{\rm i} \arg z}$.
That is, we have made the first column triangular.
In terms of an algorithm, we then introduce zeros into the first column of $A_2$,
leaving an $A_3$, and so-on. But we can wrap this iterative algorithm into a simple
proof by induction, reminisicent of our proofs for the PLU and Cholesky factorisations:

**Theorem (QR)** 
Every matrix $A âˆˆ â„‚^{m Ã— n}$ has a QR factorisation:
$$
A = QR
$$
where $Q âˆˆ U(m)$ and $R âˆˆ â„‚^{m Ã— n}$ is right triangular.

**Proof**

First assume $m â‰¥ n$. If $A = [ğš_1] âˆˆ â„‚^{m Ã— 1}$ then we have for the Householder
reflection $Q_1 = Q_{ğš_1}^{\rm H}$
$$
Q_1 A = Î± ğ_1
$$
which is right triangular, where $Î± = -{\rm csign}(a_{11}) \|ğš_1\|$. 
In other words 
$$
A = \underbrace{Q_1}_Q \underbrace{Î± ğ_1}_R.
$$

For $n > 1$, assume every matrix with less columns than $n$ has a QR factorisation.
For $A = [ğš_1|â€¦|ğš_n] âˆˆ â„‚^{m Ã— n}$, let $Q_1 = Q_{ğš_1}^{\rm H}$ so that
$$
Q_1 A =  \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & A_2 \end{bmatrix}.
$$
By assumption $A_2 = \tilde Q \tilde R$. Thus we have (recalling that $Q_1^{-1} = Q_1^â‹† = Q_1$):
$$
\begin{align*}
A = Q_1 \begin{bmatrix} Î± & ğ°^âŠ¤ \\ & \tilde Q \tilde R \end{bmatrix} \\
=\underbrace{Q_1 \begin{bmatrix} 1 \\ & \tilde Q \end{bmatrix}}_Q  \underbrace{\begin{bmatrix} Î± & ğ°^âŠ¤ \\ &  \tilde R \end{bmatrix}}_R.
\end{align*}
$$

If $m < n$, i.e., $A$ has more columns then rows, write 
$$
A = \begin{bmatrix} \At & B \end{bmatrix}
$$
where $\At âˆˆ â„‚^{m Ã— m}$. From above we know we can write $\At = Q \Rt$. We thus have
$$
A = Q \underbrace{\begin{bmatrix} \Rt & Q^â‹† B \end{bmatrix}}_R
$$
where $R$ is right triangular.

âˆ



**Example (QR by hand)** We will now do an example by hand. Consider finding the QR factorisation
where the diagonal of $R$ is positive for the $4 Ã— 3$ matrix
$$
A = \begin{bmatrix} 
1 & 1 & 1 \\ 
-1 & 0 & 1 \\
-1 & -1 & 0 \\
-1 & 0 & 0
\end{bmatrix}
$$
For the first column, since the entry $a_{11} > 0$ on a computer 
we would want to choose the Householder reflection that
makes this negative, but in this case we want $R$ to have a positive diagonal (partly because the numbers involved become very complicated
otherwise!). So instead we choose the "wrong" sign and leave it positive. Since $\| ğš_1 \|$ = 2 we have
$$
ğ²_1 = \Vectt[1,-1,-1,-1]  -\Vectt[2,0,0,0] = \Vectt[-1,-1,-1,-1] â‡’ ğ°_1 = {ğ²_1 \over \| ğ²_1 \|} = {1 \over 2} \Vectt[-1,-1,-1,-1].
$$
 Hence
$$
Q_1 :=  I - {1 \over 2} \begin{bmatrix} -1 \\ -1 \\ -1 \\ -1 \end{bmatrix} \begin{bmatrix} -1 & -1 & -1 & -1 \end{bmatrix} =
 {1 \over 2} \begin{bmatrix}
1 & -1 & -1 & -1 \\
-1 & 1 & -1 & -1 \\
-1 & -1 & 1 & -1 \\
-1 & -1 & -1 &  1
\end{bmatrix}
$$
so that
$$
Q_1 A = \begin{bmatrix} 2 &  1 & 0 \\
 & 0 & 0 \\
  & -1 & -1 \\
& 0 & -1
\end{bmatrix}
$$
For the second column we have a zero entry so on a computer we can either send it to positive or negative sign, 
but in this case we are told to make it positive. Thus we have
$$
ğ²_2 :=   [0,-1,0] - \Vectt[1,0,0] = \Vectt[-1,-1,0]  â‡’ ğ°_2 = {ğ²_2 \over \| ğ²_2 \|} = {1 \over \sqrt{2}} \Vectt[-1,-1,0]
$$
Thus we have
$$
Q_2 := I - 
 \begin{bmatrix} -1 \\ -1 \\ 0
\end{bmatrix} \begin{bmatrix} -1 & -1 & 0 \end{bmatrix}
= \begin{bmatrix}
0 & -1 & 0 \\
-1& 0 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$
so that
$$
\tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 0 \\
&  & -1
\end{bmatrix}
$$
The final vector is 
$$
ğ²_3 := \Vectt[0,-1] - \Vectt[1,0] = \Vectt[-1,-1] â‡’ ğ°_3 = -{1 \over \sqrt{2}} \Vectt[1,1].
$$
Hence
$$
Q_3 := I - \Vectt[1,1] \begin{bmatrix} 1 & 1 \end{bmatrix} = \sopmatrix{0 & - 1 \\ -1 & 0}
$$
so that 
$$
\tilde Q_3 \tilde Q_2 Q_1 A = \begin{bmatrix} 2 & 1 & 0 \\
 & 1 & 1 \\
  &  & 1 \\
&  & 0
\end{bmatrix} =: R
$$
and
$$
Q := Q_1 \tilde Q_2 \tilde Q_3 = {1 \over 2}  \begin{bmatrix}
1 & 1 & 1 & -1 \\
-1 & 1 & 1 & 1 \\
-1 & -1 & 1 & -1 \\
-1 & 1 & -1 & -1
\end{bmatrix}.
$$


## QR and least squares

We consider rectangular matrices with more rows than columns. Given $A âˆˆ â„‚^{m Ã— n}$ and $ğ› âˆˆ â„‚^m$,
a least squares problem consists of finding a vector $ğ± âˆˆ â„‚^n$ that minimises the 2-norm: $\| A ğ± - ğ› \|$.
There is a lot of theory around least squares, however, we focus on a simple computational aspect: we can
solve least squares problems using the QR factorisation.


**Theorem (least squares via QR)** Suppose $A âˆˆ â„‚^{m Ã— n}$ with $m â‰¥ n$ has full rank and a QR factorisation $A = Q R$
(which includes within it a reduced QR factorisation $A = \hat Q \hat R$). The vector
$$
ğ± = \hat R^{-1} \hat Q^â‹† ğ›
$$
minimises $\| A ğ± - ğ› \|$. 

**Proof**

The norm-preserving property ($\|Qğ±\| = \|ğ±\|$) of unitary matrices tells us
$$
\| A ğ± - ğ› \| = \| Q R ğ± - ğ› \| = \| Q (R ğ± - Q^â‹† ğ›) \| = \| R ğ± - Q^â‹† ğ› \| = \left \| 
\begin{bmatrix} \hat R \\ ğŸ_{m-n Ã— n} \end{bmatrix} ğ± - \begin{bmatrix} \hat Q^â‹† \\ ğª_{n+1}^â‹† \\ â‹® \\ ğª_m^â‹† \end{bmatrix}     ğ› \right \|
$$
Now note that the rows $k > n$ are independent of $ğ±$ and are a fixed contribution. Thus to minimise this norm it suffices to
drop them and minimise:
$$
\| \hat R ğ± - \hat Q^â‹† ğ› \|.
$$
This norm is minimised if it is zero. Provided the column rank of $A$ is full, $\hat R$ will be invertible.

âˆ

## Lab and Problem Sheet


In the lab we see how Householder reflection can be implemented by an iterative algorithm. To achieve optimal complexity it is important to take advantage of the structure of the Householder reflections, which is explored in the problems. For Tridiagonal matrices it is possible to use rotations in-place of reflections to upper-triangularise a matrix, in which case the resulting matrix is upper-tridiagonal. We investigate implementing this, giving an $O(n)$ algorithm for computing the QR factorisation of a tridiagonal matrix. (This can also be done with sparse Householder reflections, and thereby extended to general banded matrices). Finally, we look at the relationship between least squares and the QR factorisation.

In the problem sheet we see a simple example of a computing the QR factorisation using Householder reflections by-hand. (It's actually very hard to come up with examples where it's reasonable to do it by hand: it's more of a computer-based algorithm than one meant for pen-and-pencil calculations. Therefore this is not examinable but is there to help facilitate understanding.) We also explore some basic properties of QR factorisation such as uniqueness.