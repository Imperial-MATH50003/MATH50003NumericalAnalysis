# Singular Value Decomposition and Matrix Compression

In the previous section we saw an application of least squares, where the best 2-norm fit to a vector was found using the QR
factorisation. But what if the data is a matrix, eg. corresponding to a 2D function? Here we consider finding the best
approximation to a matrix in the 2-norm sense by by a matrix with a  lower rank.  This concept has numerous applications
in compressing matrices, including  Principle Component Analysis (PCA) and Machine Learning (where one might want to compress the ``weight'' matrices to minimise
degrees of freedom). 


We will use induced matrix norms, in particular the 2-norm of a matrix $A âˆˆ â„‚^{m Ã— n}$ is defined via
$$
\| A \| :=  \sup_{ğ¯ âˆˆ â„‚^m : \|ğ¯\|_X=1} \|A ğ¯\| =  \sup_{ğ¯ âˆˆ â„‚^m} {\|A ğ¯\| \over \| ğ¯\|}/
$$
The matrix 1- and $âˆ$-norms have simple definitions in terms of column/row sums (see appendix). On the other hand,
the 2-norm has no simple formula. 


In order to define the 2-norm we will introduce the  _Singular Value Decomposition (SVD)_: a matrix factorisation
that encodes how much a matrix ``stretches'' a random vector. This includes _singular values_, 
the largest of which dictates the $2$-norm of the matirx.

**Definition (singular value decomposition)** For $A âˆˆ â„‚^{m Ã— n}$ with rank $r > 0$, 
the _(reduced) singular value decomposition (SVD)_ is
$$
A = U Î£ V^â‹†
$$
where $U âˆˆ â„‚^{m Ã— r}$ and $V âˆˆ  â„‚^{n Ã— r}$ have orthonormal columns and $Î£ âˆˆ â„^{r Ã— r}$ is  diagonal whose
diagonal entries, which which we call _singular values_, are all positive and non-increasing: $Ïƒ_1 â‰¥ â‹¯ â‰¥ Ïƒ_r > 0$.
The _full singular value decomposition (SVD)_ is
$$
A = UÌƒ Î£Ìƒ VÌƒ^â‹†
$$
where $UÌƒ âˆˆ U(m)$ and $VÌƒ âˆˆ  U(n)$ are unitary matrices and $Î£Ìƒ âˆˆ â„^{m Ã— n}$ has only
diagonal non-zero entries, i.e., if $m > n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_n \\ && 0 \\ && â‹® \\ && 0 \end{bmatrix}
$$
and if $m < n$,
$$
Î£Ìƒ = \begin{bmatrix} Ïƒ_1 \\ & â‹± \\ && Ïƒ_m & 0 & â‹¯ & 0\end{bmatrix}
$$
where $Ïƒ_k = 0$ if $k > r$.
âˆ

In particular, we discuss:

1. Existence of the SVD: we show that an SVD exists by relating it to the eigenvalue Decomposition of $A^â‹†A$ and $AA^â‹†$.
2. 2-norm and SVD: the 2-norm of a matrix is defined in terms of the largest singular value.
3. Best rank-$k$ approximation and compression: the best approximation of a matrix by a smaller rank matrix can be constructed
using the SVD, which gives an effective way to compress matrices. 

What we do not discuss is computation of the SVD. There are reliable and efficient iterative algorithms for computing the SVD, similar to computing
eigen-decompositions, but this is beyond the scope of this module.


## Existence


To show the SVD exists we first establish some properties of a _Gram matrix_ ($A^â‹† A$), which
has within its eigendecomposition part of the SVD. The Gram matrix is best viewed as the matrix of inner products of the columns:
if $A = \begin{bmatrix} ğš_1 | â‹¯ | ğš_m \end{bmatrix}$ then the Gram matrix is the Hermitian matrix
$$
A^â‹† A = \begin{bmatrix} ğš_1^â‹† ğš_1 & â‹¯ &ğš_1^â‹† ğš_m \\
                        â‹® &  â‹± & â‹® \\
                        ğš_m^â‹† ğš_1 & â‹¯ &ğš_m^â‹† ğš_m \end{bmatrix} âˆˆ â„‚^{m Ã— m}.
$$

We first establish that the kernels match:

**Proposition (Gram matrix kernel)** The kernel of $A$ is the also the kernel of $A^â‹† A$. 

**Proof**
If $ğ± âˆˆ \hbox{ker}(A)$, i.e., $A ğ± = 0$, then clearly $A^â‹†A ğ± = 0 = A^â‹† 0 = 0$.
On the other hand, if $ğ± âˆˆ \hbox{ker}(A^â‹†A)$ so  that $A^â‹† A ğ± = 0$ then we have
$$
0 = ğ±^â‹† A^â‹† A ğ± = \| A ğ± \|^2
$$
which means $A ğ± = 0$ and $ğ± âˆˆ \hbox{ker}(A)$.
âˆ

As mentioned in PS6, the spectral theorem states that any normal matrix is unitarily diagonalisable: if $A$ is normal then $A = Q Î› Q^â‹†$
where $Q âˆˆ U(n)$ and $Î›$ is diagonal. In the special case where $A$ is symmetric/Hermitian you would have seen a proof of this in first year.
We can use this to ensure that the Gram matrix is diagonalisable:

**Proposition (Gram matrix diagonalisation)** The Gram-matrix
satisfies
$$
A^â‹† A = Q Î› Q^â‹† âˆˆ â„‚^{n Ã— n}
$$
is a Hermitian matrix where $Q âˆˆ U(n)$ and the eigenvalues $Î»_k$ are real and non-negative.
If $A âˆˆ â„^{m Ã— n}$ then $Q âˆˆ O(n)$.

**Proof**
$A^â‹† A$ is Hermitian so we appeal to the spectral theorem for the
existence of the decomposition. 
To see that the eigenvalues are real and positive  note for the corresponding (orthonormal) eigenvector $ğª_k$ we have
$$
Î»_k = Î»_k ğª_k^â‹† ğª_k = ğª_k^â‹† A^â‹† A ğª_k = \| A ğª_k \|^2 â‰¥ 0.
$$
The fact that when $A$ is real implies that $Q$ is also real follows from direct calculation
since we can choose the first entry of the eigenvector to be real.

âˆ


This connection allows us to prove existence:

**Theorem (SVD existence)** Every $A âˆˆ â„‚^{m Ã— n}$ has an SVD.

**Proof**
Consider
$$
A^â‹† A = Q Î› Q^â‹†.
$$
Assume (as usual) that the eigenvalues are sorted in decreasing modulus, and so $Î»_1,â€¦,Î»_r$
are an enumeration of the non-zero eigenvalues and
$$
V := \begin{bmatrix} ğª_1 | â‹¯ | ğª_r \end{bmatrix}
$$
the corresponding (orthonormal) eigenvectors, with
$$
K = \begin{bmatrix} ğª_{r+1} | â‹¯ | ğª_n \end{bmatrix}
$$
the corresponding kernel. 
Define
$$
Î£ :=  \begin{bmatrix} \sqrt{Î»_1} \\ & â‹± \\ && \sqrt{Î»_r} \end{bmatrix}
$$
Now define
$$
U := AV Î£^{-1}
$$
which is orthogonal since $A^â‹† A V = V Î£^2$:
$$
U^â‹† U = Î£^{-1} V^â‹† A^â‹† A V Î£^{-1} = I.
$$
Thus we have
$$
U Î£ V^â‹† = A V V^â‹† = A \underbrace{\begin{bmatrix} V | K \end{bmatrix}}_Q\underbrace{\begin{bmatrix} V^â‹† \\ K^â‹† \end{bmatrix}}_{Q^â‹†}
$$
where we use the fact that $A K = 0$ so that concatenating $K$ does not change the value.

âˆ

## 2-norm and SVD


Somewhat surprisingly, the 2-norm does not have a simple formula but instead, as we shall show, can
be defined in terms of the SVD. We begin with two cases where we do have simple formulÃ¦:


**Proposition (diagonal/orthogonal 2-norms)** If $Î›$ is diagonal with entries $Î»_k$ then
$\|Î›\|_2 = \max_k |Î»_k|$. If $Q$ is orthogonal then $\|Q\| = 1$.

**Proof**

The first property follows immediately. The second property follows since $Q$ preserves norm:
$$
\|Q \| = \sup_{ğ¯ : \|ğ¯\| =1} \|Q ğ¯\| = \sup_{ğ¯ : \|ğ¯\| =1} \|ğ¯\|  =1.
$$

âˆ


These two facts allow us to deduce the 2-norm from the SVD of a matrix:

**Corollary (singular values and norm)**
$$
\|A \| = Ïƒ_1
$$
and if $A âˆˆ â„‚^{n Ã— n}$ is invertible, then
$$
\|A^{-1} \| = Ïƒ_n^{-1}
$$

**Proof**

First we establish the upper-bound using the fact for induced norms that $\|A B\| â‰¤ \|A\| \|B\|$:
$$
\|A \| â‰¤  \|U \| \| Î£ \| \| V^â‹†\| = \| Î£ \|  = Ïƒ_1
$$
This is attained using the first right singular vector:
$$
\|A ğ¯_1\| = \|Î£ V^â‹† ğ¯_1\| = \|Î£  ğ_1\| = Ïƒ_1
$$
The inverse result follows since the inverse has SVD
$$
A^{-1} = V Î£^{-1} U^â‹† = (V W) (W Î£^{-1} W) (W U)^â‹†
$$
is the SVD of $A^{-1}$, i.e. $VW âˆˆ U(n)$ are the left singular vectors and
$W U$ are the right singular vectors, where
$$
W := P_Ïƒ = \begin{bmatrix} && 1 \\ & â‹° \\ 1 \end{bmatrix}
$$
is the permutation that reverses the entries, that is, $Ïƒ$ has Cauchy notation
$$
\begin{pmatrix}
1 & 2 & â‹¯ & n \\
n & n-1 & â‹¯ & 1
\end{pmatrix}.
$$

âˆ


## Best rank-$k$ approximation and compression

One of the main usages for SVDs is low-rank approximation:

**Theorem (best low rank approximation)** The  matrix
$$
A_k := \begin{bmatrix} ğ®_1 | â‹¯ | ğ®_k \end{bmatrix} \begin{bmatrix}
Ïƒ_1 \\
& â‹± \\
&& Ïƒ_k\end{bmatrix} \begin{bmatrix} ğ¯_1 | â‹¯ | ğ¯_k \end{bmatrix}^â‹†
$$
is the best 2-norm approximation of $A$ by a rank $k$ matrix, that is, for all rank-$k$ matrices $B$, we have 
$$\|A - A_k\|_2 â‰¤ \|A -B \|_2.$$


**Proof**
We have

$$
A - A_k = U \begin{bmatrix} 0  \cr &\ddots \cr && 0 \cr &&& Ïƒ_{k+1} \cr &&&& \ddots \cr &&&&& Ïƒ_r\end{bmatrix} V^â‹†.
$$
Suppose a rank-$k$ matrix $B$ has 
$$
\|A-B\|_2  < \|A-A_k\|_2 = Ïƒ_{k+1}.
$$
For all $ğ° âˆˆ \ker(B)$ we have 
$$
\|A ğ°\|_2 = \|(A-B) ğ°\|_2 â‰¤ \|A-B\|\|ğ°\|_2  < Ïƒ_{k+1} \|ğ°\|_2
$$

But for all $ğ® âˆˆ {\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$, that is, $ğ® = V[:,1:k+1]ğœ$ for some $ğœ âˆˆ â„‚^{k+1}$  we have 
$$
\|A ğ®\|_2^2 = \|U Î£_k ğœ\|_2^2 = \|Î£_k ğœ\|_2^2 =
\sum_{j=1}^{k+1} (Ïƒ_j c_j)^2 â‰¥ Ïƒ_{k+1}^2 \| ğœ  \|^2,
$$
i.e., $\|A ğ®\|_2 â‰¥ Ïƒ_{k+1} \|ğ®\|$, where we use the fact that
$$
\|ğ®\|^2 = \|V[:,1:k+1]ğœ\|^2 = ğœ^â‹† V[:,1:k+1]^â‹† V[:,1:k+1]ğœ = ğœ^â‹†ğœ = \|ğœ\|^2.
$$
Thus $ğ°$ cannot be in this span.


The dimension of the span of $\ker(B)$ is at least $n-k$, but the dimension of ${\rm span}(ğ¯_1,â€¦,ğ¯_{k+1})$ is at least $k+1$.
Since these two spaces cannot intersect (apart from at 0) we have a contradiction, since $(n-r) + (r+1) = n+1 > n$.  âˆ



