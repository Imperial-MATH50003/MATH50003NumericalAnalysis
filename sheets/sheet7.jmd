**Numerical Analysis MATH50003 (2024â€“25) Problem Sheet 7**

This problem sheet explores approximation of functions by polynomials using interpolation and regression via least squares.
Part of the nuance is understanding the relationship between Lagrange interpolation and interpolation via  Vandermonde matrices.
We also discuss the singular value decomposition, in particular, we explore how it can be used to construct the _(Mooreâ€“Penrose) pseudoinverse_:
a way of making sense of an ``inverse'' for rectangular or non-invertible matrices in a way that is consistent with least squares.


We begin with simple examples of constructing interpolants (using Lagrange interpolation) and linear regressions (using QR factorisations):


-----

**Problem 1(a)** Use Lagrange interpolation to
interpolate the function $\cos x$ by a polynomial at the points
$[0,2,3,4]$ and evaluate at $x = 1$.



**Problem 1(b)** Find a least squares fit to 
$$
\sinc x := \begin{cases} 1 & x = 0 \\
{\sin Ï€ x \over Ï€ x} & \hbox{otherwise}
\end{cases}
$$
by a polynomial of degree $1$ (linear regression) at the points $[0,2,3,4]$ using QR.
Hint: Choose the (non-default) positive sign in the Householder reflections to arrive at simpler computations.



-----


The Vandermonde matrix used in interpolation is a fundamental mathematical object, in particular, its determinant comes up
in differential equations, representation theory, and theoretical physics. The following problems investigate the relationship between
the LU factorisation of a (transposed) Vandermonde matrix and its determinant.

---


**Problem 2(a)** Compute the LU factorisation of the following transposed Vandermonde matrices:
$$
\begin{bmatrix}
1 & 1 \\
x & y
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 \\
x & y & z \\
x^2 & y^2 & z^2
\end{bmatrix},
\begin{bmatrix}
1 & 1 & 1 & 1 \\
x & y & z & t \\
x^2 & y^2 & z^2 & t^2 \\
x^3 & y^3 & z^3 & t^3
\end{bmatrix}
$$
Can you spot a pattern? Test your conjecture with a $5 Ã— 5$ Vandermonde matrix.



**Problem 2(b)** Use the LU factorisation to deduce the determinant of the above Vandermonde matrices. What
happens to the determinants when you swap two points (i.e., swapping $x$ and $y$)?



--------


The following problem is a simple example of an interpolatory quadrature rule, which can be constructed
using the Lagrange basis.


------


**Problem 3** Compute the interpolatory quadrature rule
$$
âˆ«_{-1}^1 f(x) w(x) \dx â‰ˆ âˆ‘_{j=1}^n w_j f(x_j)
$$
for the points $[x_1,x_2,x_3] = [-1,1/2,1]$, for the weights $w(x) = 1$ and $w(x) = \sqrt{1-x^2}$.




-----

We now turn to the (real-valued) SVD $A = U Î£ V^âŠ¤ âˆˆ â„^{m Ã— n}$, where $U âˆˆ â„^{m Ã— r}$ and $V âˆˆ â„^{n Ã— r}$ have orthonormal columns
and $Î£$ is a diagonal matrix with non-increasing positive entries. We can use these to define a generalisation of the inverse of a matrix,
known as a  _pseudo-inverse_:
$$
A^+ := V Î£^{-1} U^âŠ¤.
$$
The pseudo-inverse has a number of properties similar to inverses:

-----

**Problem 4**  Show that $A^+$ satisfies the _Moore-Penrose conditions_: (1) $A A^+ A = A$,
(2) $A^+ A A^+ = A^+$, and (3) $(A A^+)^âŠ¤ = A A^+$ and $(A^+ A)^âŠ¤ = A^+ A$



----

An important feature of the pseudo-inverse is that it can be used to solve least squares problems.
That is, the SVD provides an alternative approach to QR for solving least squares, albeit one that
is computationally less efficient.
One advantage of the SVD for least squares is that it also allows for the case where the
matrix involved is not of full column rank.




----

**Problem 5(a)** Show for $A âˆˆ â„^{m Ã— n}$ with $m â‰¥ n$ and full rank
that $ğ± =  A^+ ğ›$ is the least squares solution, i.e., minimises $\| A ğ± - ğ› \|$.
Hint: extend $U$ in the SVD to be a square orthogonal matrix.







**Problem 5(b)**
If $A âˆˆ â„^{m Ã— n}$ has a non-empty kernel there are multiple solutions to the least
squares problem as 
we can add any element of the kernel. Show that $ğ± = A^+ ğ›$ gives the least squares solution
such that $\| ğ± \|$ is minimised.




------
