
\section{Divided Differences}
Given a function, how can we approximate its derivative at a point? We consider an intuitive approach to this problem using \emph{(Right-sided) Divided Differences}: 
\[
f'(x) \ensuremath{\approx} {f(x+h) - f(x) \over h}
\]
Note by the definition of the derivative we know that this approximation will converge to the true derivative as $h \ensuremath{\rightarrow} 0$. But in numerical approimxations we also need to consider the rate of convergence. 

Now in the previous section I mentioned there are three basic tools in analysis:  (1) integration-by-parts, (2) geometric series or (3) Taylor series. In this case we use (3):

\begin{proposition}[divided differences error] Suppose that $f$ is twice-differentiable on the interval $[x,x+h]$. The error in approximating the derivative using divided differences is
\[
f'(x) = {f(x+h) - f(x) \over h} + \ensuremath{\delta}
\]
where $|\ensuremath{\delta}| \ensuremath{\leq} Mh/2$ for  $M = \sup_{x \ensuremath{\leq} t \ensuremath{\leq} x+h} |f''(t)|$.

\end{proposition}
\textbf{Proof} Follows immediately from Taylor's theorem:
\[
f(x+h) = f(x) + f'(x) h + \underbrace{{f''(t) \over 2} h^2}_{h \ensuremath{\delta}}
\]
for some $x \ensuremath{\leq} t \ensuremath{\leq} x+h$, by bounding:
\[
|\ensuremath{\delta}| \ensuremath{\leq} \abs{{f''(t) \over 2} h} \ensuremath{\leq} {M  h \over 2}.
\]
\ensuremath{\QED}

Unlike the rectangular rule, the computational cost of computing the divided difference is independent of $h$! We only need to evaluate a function $f$ twice and do a single division. Here we are assuming that the computational cost of evaluating $f$ is independent of the point of evaluation. Later we will investigate the details of how computers work with numbers via floating point,  and confirm that this is a sensible assumption.

So why not just set $h$ ridiculously small? In the lab we explore this question and observe that there are significant errors introduced in the numerical realisation of this algorithm. We will return to the question of understanding these errors after learning floating point numbers. 

There are alternative versions of divided differences. Left-side divided differences evaluates to the left of the point where we wish to know the derivative:
\[
f'(x) \ensuremath{\approx} {f(x) - f(x-h) \over h}
\]
and central differences:
\[
f'(x) \ensuremath{\approx} {f(x + h) - f(x - h) \over 2h}
\]
We can further arrive at an approximation to the second derivative by composing a left- and right-sided finite difference:
\[
f''(x) \ensuremath{\approx} {f'(x+h) - f'(x) \over h} \ensuremath{\approx} {{f(x+h) - f(x) \over h} - {f(x) - f(x-h) \over h} \over h}
= {f(x+h) - 2f(x)  + f(x-h) \over h^2}
\]
In the lab we investigate the convergence rate of these approximations (in particular, that  central differences is more accurate than standard divided differences) and observe that they too suffer from unexplained (for now) loss of accuracy as $h \ensuremath{\rightarrow} 0$. In the problem sheet we prove the theoretical convergence rate, which is never realised because of these errors.



