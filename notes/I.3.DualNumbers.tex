
\section{Dual Numbers}
In this section we introduce a mathematically beautiful  alternative to divided differences for computing derivatives: \emph{dual numbers}. These are a commutative ring that \emph{exactly} compute derivatives, which when implemented on a computer gives very high-accuracy approximations to derivatives. They underpin forward-mode \href{https://en.wikipedia.org/wiki/Automatic_differentiation}{automatic differentation}. Automatic differentiation  is a basic tool in Machine Learning for computing gradients necessary for training neural networks.

\begin{definition}[Dual numbers] Dual numbers $\ensuremath{\bbD}$ are a commutative ring (over $\ensuremath{\bbR}$) generated by $1$ and $\ensuremath{\epsilon}$ such that $\ensuremath{\epsilon}^2 = 0$, that is,
\[
\ensuremath{\bbD} := \{a + b\ensuremath{\epsilon} \quad :\quad
 a,b \in \ensuremath{\bbR}, \quad \ensuremath{\epsilon}^2 =0 \}.
\]
\end{definition}

This is very much analoguous to complex numbers, which are a field generated by $1$ and $\I$ such that $\I^2 = -1$, that is,
\[
\ensuremath{\bbC} := \{a + b\I \quad :\quad
 a,b \in \ensuremath{\bbR}, \quad \I^2 =-1 \}.
\]
Compare multiplication of each number type which falls out of the rules of the generators:
\meeq{
(a + b \I) (c + d \I) = ac + (bc + ad) \I + bd \I^2 = ac -bd + (bc + ad) \I, \ccr
(a + b \ensuremath{\epsilon}) (c + d \ensuremath{\epsilon}) = ac + (bc + ad) \ensuremath{\epsilon} + bd \ensuremath{\epsilon}^2 = ac  + (bc + ad) \ensuremath{\epsilon}.
}
And just as we view $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbC}$ by equating $a \ensuremath{\in} \ensuremath{\bbR}$ with $a + 0\I \ensuremath{\in} \ensuremath{\bbC}$, we can view $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbD}$ by equating $a \ensuremath{\in} \ensuremath{\bbR}$ with $a + 0{\rm \ensuremath{\epsilon}} \ensuremath{\in} \ensuremath{\bbD}$.

Conceptually, dual numbers can be thought of as introducing an infinitesimally small $\ensuremath{\epsilon}$, where $\ensuremath{\epsilon}^2$ is so small it is treated as zero. This is the intuitive reason they allow for differentiation of functions. But we do not need to appeal to this calculus-like interpretation, instead, their construction and relationship to differentiation can be accomplished using purely algebraic reasoning.

\subsection{Differentiating polynomials}
Polynomials evaluated on dual numbers are well-defined as they depend only on the operations $+$ and $*$. From the formula for multiplication of dual numbers we deduce that evaluating a polynomial at a dual number $a + b \ensuremath{\epsilon}$ tells us the derivative of the polynomial at $a$:

\begin{theorem}[polynomials on dual numbers] Suppose $p$ is a polynomial. Then
\[
p(a + b \ensuremath{\epsilon}) = p(a) + b p'(a) \ensuremath{\epsilon}
\]
\end{theorem}
\textbf{Proof}

First consider $p(x) = x^n$ for $n \ensuremath{\geq} 0$.  The cases $n = 0$ and $n = 1$ are immediate. For $n > 1$ we have by induction:
\[
(a + b \ensuremath{\epsilon})^n = (a + b \ensuremath{\epsilon}) (a + b \ensuremath{\epsilon})^{n-1} = (a + b \ensuremath{\epsilon}) (a^{n-1} + (n-1) b a^{n-2} \ensuremath{\epsilon}) = a^n + b n a^{n-1} \ensuremath{\epsilon}.
\]
For a more general polynomial
\[
p(x) = \ensuremath{\sum}_{k=0}^n c_k x^k
\]
the result follows from linearity:
\[
p(a + b \ensuremath{\varepsilon}) = \ensuremath{\sum}_{k=0}^n c_k (a+b\ensuremath{\epsilon})^k = c_0 + \ensuremath{\sum}_{k=1}^n c_k (a^k +k b a^{k-1}\ensuremath{\epsilon})
= \ensuremath{\sum}_{k=0}^n c_k a^k + b \ensuremath{\sum}_{k=1}^n c_k k a^{k-1}\ensuremath{\epsilon} = p(a) + b p'(a) \ensuremath{\epsilon}.
\]
\ensuremath{\QED}

\begin{example}[differentiating polynomial] Consider computing $p'(2)$ where
\[
p(x) = (x-1)(x-2) + x^2.
\]
We can use dual numbers to differentiate, avoiding expanding in monomials or applying rules of differentiating:
\[
p(2+\ensuremath{\epsilon}) = (1+\ensuremath{\epsilon})\ensuremath{\epsilon} + (2+\ensuremath{\epsilon})^2 = \ensuremath{\epsilon} + 4 + 4\ensuremath{\epsilon} = 4 + \underbrace{5}_{p'(2)}\ensuremath{\epsilon}.
\]
\end{example}

\subsection{Differentiating other functions}
We can extend real-valued differentiable functions to dual numbers in a similar manner. First, consider a standard function with a Taylor series (e.g. ${\rm cos}$, ${\rm sin}$, ${\rm exp}$, etc.)
\[
f(x) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k x^k
\]
so that $a$ is inside the radius of convergence. This leads naturally to a definition on dual numbers:
\meeq{
f(a + b \ensuremath{\epsilon}) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k (a + b \ensuremath{\epsilon})^k = f_0 + \ensuremath{\sum}_{k=1}^\ensuremath{\infty} f_k (a^k + k a^{k-1} b \ensuremath{\epsilon}) = \ensuremath{\sum}_{k=0}^\ensuremath{\infty} f_k a^k +  \ensuremath{\sum}_{k=1}^\ensuremath{\infty} f_k k a^{k-1} b \ensuremath{\epsilon}  \ccr
  = f(a) + b f'(a) \ensuremath{\epsilon}.
}
More generally, given a differentiable function (which may not have a Taylor series) we can extend it to dual numbers:

\begin{definition}[dual extension] Suppose a real-valued function $f : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\bbR}$ is differentiable in $\ensuremath{\Omega} \ensuremath{\subset} \ensuremath{\bbR}$.  We can construct the \emph{dual extension} $\underline{f} : \ensuremath{\Omega} + \ensuremath{\epsilon}\ensuremath{\bbR} \ensuremath{\rightarrow} \ensuremath{\bbD}$ by defining
\[
\underline{f}(a + b \ensuremath{\epsilon}) := f(a) + b f'(a) \ensuremath{\epsilon}.
\]
By viewing $\ensuremath{\bbR} \ensuremath{\subset} \ensuremath{\bbD}$, it is natural to reuse the notation $f$ for the dual extension, hence when there's no chance of confusion we will identify $f(a + b \ensuremath{\epsilon}) \ensuremath{\equiv} \underline{f}(a+b \ensuremath{\epsilon})$. \end{definition}

Thus, for basic functions we have natural extensions:
\begin{align*}
\exp(a + b \ensuremath{\epsilon}) &:= \exp(a) + b \exp(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\sin(a + b \ensuremath{\epsilon}) &:= \sin(a) + b \cos(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\cos(a + b \ensuremath{\epsilon}) &:= \cos(a) - b \sin(a) \ensuremath{\epsilon} & (a,b \ensuremath{\in} \ensuremath{\bbR}) \\
\log(a + b \ensuremath{\epsilon}) &:= \log(a) + {b \over a} \ensuremath{\epsilon} & (a \ensuremath{\in} (0,\ensuremath{\infty}), b \ensuremath{\in} \ensuremath{\bbR}) \\
\sqrt{a+b \ensuremath{\epsilon}} &:= \sqrt{a} + {b \over 2 \sqrt{a}} \ensuremath{\epsilon} & (a \ensuremath{\in} (0,\ensuremath{\infty}), b \ensuremath{\in} \ensuremath{\bbR}) \\
|a + b \ensuremath{\epsilon}| &:= |a| + b\, {\rm sign} a\, \ensuremath{\epsilon} & (a \ensuremath{\in} \ensuremath{\bbR} \backslash \{0\} , b \ensuremath{\in} \ensuremath{\bbR})
\end{align*}
provided the function is differentiable at $a$. Note the last example does not have a convergent Taylor series (at $0$) but we can still extend it where it is differentiable.

Going further, we can add, multiply, and compose such dual-extensions. And the beauty is these automatically satisfy the right properties to be dual-extensions themselves, thus allowing for differentiation of  complicated functions built from basic differentiable building blocks.

The following lemma shows that addition and multiplication in some sense \ensuremath{\ldq}commute" with the dual-extension, hence we can recover the product rule from dual number multiplication:

\begin{lemma}[addition/multiplication] Suppose $f,g : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\bbR}$ are differentiable for $\ensuremath{\Omega} \ensuremath{\subset} \ensuremath{\bbR}$ and $c \ensuremath{\in} \ensuremath{\bbR}$. Then for $a \ensuremath{\in} \ensuremath{\Omega}$ and $b \ensuremath{\in} \ensuremath{\bbR}$ we have
\meeq{
\underline{f+g}(a+b\ensuremath{\epsilon}) = \underline{f}(a+b\ensuremath{\epsilon}) + \underline{g}(a+b\ensuremath{\epsilon}) \ccr 
\underline{c f}(a+b\ensuremath{\epsilon}) = c \underline{f}(a+b \ensuremath{\epsilon}) \ccr
\underline{f g}(a+b\ensuremath{\epsilon}) = \underline{f}(a+b \ensuremath{\epsilon}) \underline{g}(a+b \ensuremath{\epsilon})
}
\end{lemma}
\textbf{Proof} The first two are immediate due to linearity:
\meeq{
\underline{(f+g)}(a+b\ensuremath{\epsilon}) = (f+g)(a) + b(f + g)'(a) \ensuremath{\epsilon} \ccr
 = (f(a)+bf'(a)\ensuremath{\epsilon}) + (g(a)+bg'(a)\ensuremath{\epsilon}) = \underline{f}(a+b\ensuremath{\epsilon}) + \underline{g}(a+b\ensuremath{\epsilon}), \ccr
\underline{cf}(a+b\ensuremath{\epsilon}) = (cf)(a) + b(cf)'(a) \ensuremath{\epsilon} = c(f(a)+bf'(a)\ensuremath{\epsilon}) = c\underline{f}(a+b\ensuremath{\epsilon}).
}
The last property essentially captures the product rule of differentiation:
\meeq{
\underline{f g}(a+b\ensuremath{\epsilon}) = f(a)g(a) + b (f(a)g'(a)+f'(a) g'(a))\ensuremath{\epsilon}  \ccr
= (f(a)+bf'(a)\ensuremath{\epsilon})(g(a)+b g'(a)\ensuremath{\epsilon}) = \underline{f}(a+b \ensuremath{\epsilon}) \underline{g}(a+b \ensuremath{\epsilon}).
}
\ensuremath{\QED}

Furthermore composition recovers the chain rule:

\begin{lemma}[composition]  Suppose $f : \ensuremath{\Gamma} \ensuremath{\rightarrow} \ensuremath{\bbR}$ and $g : \ensuremath{\Omega} \ensuremath{\rightarrow} \ensuremath{\Gamma}$ are differentiable in $\ensuremath{\Omega},\ensuremath{\Gamma} \ensuremath{\subset} \ensuremath{\bbR}$. Then
\[
\underline{(f \ensuremath{\circ} g)}(a+b \ensuremath{\epsilon}) = \underline{f}(\underline{g}(a+b\ensuremath{\epsilon}))
\]
\end{lemma}
\textbf{Proof} Again it falls out of the properties of dual numbers:
\[
\underline{(f \ensuremath{\circ} g)}(a+b \ensuremath{\epsilon}) = f(g(a)) + bg'(a) f'(g(a)) \ensuremath{\epsilon} = \underline{f}(g(a)+bg'(a)\ensuremath{\epsilon}) = \underline{f}(\underline{g}(a+b\ensuremath{\epsilon}))
\]
\ensuremath{\QED}

A simple corollary is that any function defined in terms of addition, multiplication, composition, etc. of basic functions with dual-extensions will be differentiable via dual numbers. In this following example we see a practical realisation of this, where we differentiate a function by just evaluating it on dual numbers, implicitly, using the dual-extension for the basic build blocks:

\begin{example}[differentiating non-polynomial]

Consider differentiating $f(x) =  \exp(x^2 + \cos x)$ at the point $a = 1$, where  we automatically use the dual-extension of $\exp$ and $\cos$. We can differentiate $f$ by simply evaluating on the duals:
\[
f(1 + \ensuremath{\epsilon}) = \exp(1 + 2\ensuremath{\epsilon} + \cos 1 - \sin 1 \ensuremath{\epsilon}) =  \exp(1 + \cos 1) + \exp(1 + \cos 1) (2 - \sin 1) \ensuremath{\epsilon}.
\]
Therefore we deduce that
\[
f'(1) = \exp(1 + \cos 1) (2 - \sin 1).
\]
\end{example}

\subsection{Lab and problem sheet}
In the lab we explore how one can turn this mathematical idea into a practical  implementation on a computer, giving a basic version of \emph{forward-mode automatic differentiation}. This is a concept that underpins machine learning, which uses \emph{reverse-mode automatic differentiation} to compute gradients when performing stochastic gradient descent.  In order to implement dual numbers, we will introduce the concept of a \emph{type}: a data structure with fields.  For example, we will implement a type \texttt{Rat} for representing rationals $p/q$, where the type has two fields (\texttt{p} and \texttt{q}). Basic arithmetic operations like \texttt{+} and \texttt{*} can be implemented to correctly do rational arithmetic.  We will then create a new type that can  represent a dual number $a + b \ensuremath{\epsilon}$, where the the type has two fields (\texttt{a} and \texttt{b}). By implementing basic arithmetic operations as well as more complicated functions like \texttt{exp} we can efficiently, and extremely accurately, compute derivatives of quite general functions.

In the problem sheet, we explore how dual numbers can also be used for pen-and-paper calculations of derivatives. This gives an alternative to traditional differentiation rules like chain and product rule, that while it is mathematically equivalent feels very different in practice. (I prefer it because it is much more algorithmic!) Make sure when doing the problem sheet to only use dual numbers and not fall back to the more traditional rules.  We also see that one can extend the concept to a 2D-analogue of dual numbers, which allows for computation of gradients.



